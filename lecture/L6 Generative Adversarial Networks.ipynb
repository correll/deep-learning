{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "We have so far exclusively treated neural networks as classifiers and regressors, but neglected the high dimensional information that the network is creating. Convolution reduces spatial information (2D images or 1D time series) to components by means of filters. The filters have usually been trained based on a large number of examples, describing features that these examples have in common. The result of the convolutional layers is then a representation that describes how much of each feature has been in the original image. This process could also be used the other way round to generate images using \"deconvolution\". Instead of reducing high-dimensional information such as an image to low-dimensional one, such as a distribution over classes, such networks could be generate high-dimensional images from a few numbers that define the desired content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling and \"Deconvolving\"\n",
    "The label \"deconvolution\" is unfortunately confusing here, however, as it has established <a href=\"https://en.wikipedia.org/wiki/Deconvolution\">mathematical meaning</a>, which is usually not implemented in a neural network context. Rather, we can achieve the desired effect by first upsampling the input and performing a convolution then. In this example, a simple (2,2) image will be turned into a (4,4) image. What exactly happens is random, as the initial weights are random, and running the model below a couple of times will give the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[ 0.2291134   0.0492599   0.4582268   0.8179338 ]\n",
      " [ 0.4211383   0.24128473  0.6502515   2.0911193 ]\n",
      " [ 0.68734026  0.5074867   0.9164536   1.6358676 ]\n",
      " [ 0.39930302  0.76002985  0.53240395 -0.9105036 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2bbe89b70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code inspired by https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import UpSampling2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define input data\n",
    "X = asarray([[1, 2],\n",
    "            [3, 4]])\n",
    "# show input data for context\n",
    "print(X)\n",
    "# reshape input data into one sample a sample with a channel\n",
    "X = X.reshape((1, 2, 2, 1))\n",
    "model = Sequential()\n",
    "model.add(UpSampling2D(input_shape=(2, 2, 1)))\n",
    "model.add(Conv2D(1, (2,2), padding='same'))\n",
    "#model.summary()\n",
    "yhat = model.predict(X)\n",
    "# reshape output to remove channel to make printing easier\n",
    "yhat = yhat.reshape((4, 4))\n",
    "# summarize output\n",
    "print(yhat)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X.reshape((2,2)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a function to combine convolution and upsampling <code>Conv2DTranspose</code> that pads new rows and columns during upsampling with zeros. This can be seen when using (1,1) as a convolution kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[ 0.26431435 -0.2823633   0.5286287  -0.5647266 ]\n",
      " [-0.83294845  0.2662112  -1.6658969   0.5324224 ]\n",
      " [ 0.79294306 -0.8470899   1.0572574  -1.1294532 ]\n",
      " [-2.4988453   0.79863364 -3.3317938   1.0648448 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2884af358>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code inspired by https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/\n",
    "\n",
    "# example of using the transpose convolutional layer\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2DTranspose\n",
    "# define input data\n",
    "X = asarray([[1, 2],\n",
    "            [3, 4]])\n",
    "# show input data for context\n",
    "print(X)\n",
    "# reshape input data into one sample a sample with a channel\n",
    "X = X.reshape((1, 2, 2, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "# Use different kernels (1,1), (2,2) etc to see how Conv2DTranspose operates\n",
    "model.add(Conv2DTranspose(1, (2,2), strides=(2,2), input_shape=(2, 2, 1)))\n",
    "#model.summary()\n",
    "# make a prediction with the model\n",
    "yhat = model.predict(X)\n",
    "# reshape output to remove channel to make printing easier\n",
    "yhat = yhat.reshape((4, 4))\n",
    "# summarize output\n",
    "print(yhat)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X.reshape((2,2)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the right kernels to get meaningful output\n",
    "\n",
    "Lets assume a training set consists of single pixel images, which color is drawn from a 3D normal distribution centered around the color orange. The \"generator\" G() does not know this distribution, but turns uniform random input drawn from a distribution z into other random colors. The \"discriminator\" has learned the original distribution and can say \"hot\" (yes) or \"cold\" (no) and anything in between. The network can now use this feedback to improve the generator to better match the true distribution. Once gradient information is available, the generator can change its weights to get closer and closer to the desired distribution. This is known as backpropagation. \n",
    "\n",
    "At the same time, we can use knowledge of the fact that generated images are fake to improve the discriminator. Instead of training the discriminator with samples of shades of orange and totally random colors, we are training it with whatever the generator currently produces as examples of \"fake\" data. This will let the discriminator become more subtle over time.\n",
    "\n",
    "Training both generator and discriminator sounds like a chicken-egg problem, but is usually bootstrapped by implementing a generating network that is believed to have a sufficiently complex structure to create a variety of output that we are interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "A generator is using upsampling and convolution operations to turn a seed of noise into an image. Initially, the generated images are just noise, but will eventually show similar distributions as the training set. The generator itself is never trained as a stand-alone network, which is indicated by all parameters colored in light gray. Once this network is trained, the generator gets better and better, here showing the output after zero and 2000 training iterations, which will be explained further below. \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_generator.svg\" width=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "We use a discriminator to decide whether an image is following the desired distribution, i.e. like the training set, or a generated image. We can train this discriminator by using two batches of equal size, one of which contains training images, the other generated images. The generated images are labeled by a zero, the training images by a one.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_discriminator.svg\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "As the generator gets better, the discriminator will get presented with better and better generated images, becoming more and more sophisticated. Here, the training set at 0 and after 2000 iterations of training the generator are shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Generator and Discriminator into an Adversial Network\n",
    "A GAN consists of a generator and a discriminator network that are connected in series. When training the GAN, the generator pretends that images it generates are real images, which leads to a loss if the discriminator detects the fake image. In order to prevent this loss from backpropagating into the discriminator, all discriminator parameters are locked during training, which is indicated by all parameters of the discriminator shown in light gray.  \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_adversarial.svg\" width=\"75%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GAN\n",
    "\n",
    "The implementation below has been inspired by <a href=\"https://github.com/roatienza/Deep-Learning-Experiments\">Rowel Atienza's code</a>, and adapted to Keras 2.0. The architecture below follows the guidelines from <a href=\"https://arxiv.org/pdf/1511.06434.pdf%C3%AF%C2%BC%E2%80%B0\">UNSUPERVISED REPRESENTATION LEARNING\n",
    "WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</a>\n",
    "\n",
    "- Architecture guidelines for stable Deep Convolutional GANs\n",
    "- Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
    "- Use batchnorm in both the generator and the discriminator.\n",
    "- Remove fully connected hidden layers for deeper architectures.\n",
    "- Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "- Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential()\n",
    "depth = 64\n",
    "dropout = 0.4\n",
    "# In: 28 x 28 x 1, depth = 1\n",
    "# Out: 14 x 14 x 1, depth=64\n",
    "input_shape = (28, 28, 1)\n",
    "discriminator.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "# Out: 1-dim probability\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1))\n",
    "discriminator.add(Activation('sigmoid'))\n",
    "discriminator.summary()\n",
    "\n",
    "optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DT (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Sequential()\n",
    "dropout = 0.4\n",
    "depth = 64+64+64+64\n",
    "dim = 7\n",
    "# In: 100\n",
    "# Out: dim x dim x depth\n",
    "generator.add(Dense(dim*dim*depth, input_dim=100))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(Reshape((dim, dim, depth)))\n",
    "generator.add(Dropout(dropout))\n",
    "\n",
    "# In: dim x dim x depth\n",
    "# Out: 2*dim x 2*dim x depth/2\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "generator.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "# Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "generator.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "generator.add(Activation('sigmoid'))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_23 (Sequential)   (None, 28, 28, 1)         2394241   \n",
      "_________________________________________________________________\n",
      "sequential_22 (Sequential)   (None, 1)                 4311553   \n",
      "=================================================================\n",
      "Total params: 6,705,794\n",
      "Trainable params: 6,680,258\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "adversarial = Sequential()\n",
    "adversarial.add(generator)\n",
    "adversarial.add(discriminator)\n",
    "adversarial.summary()\n",
    "adversarial.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, Y_train), (x_test, Y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(10000, 28,28,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalize\n",
    "#\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(Y_train, 10)\n",
    "y_test = np_utils.to_categorical(Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.692285, acc: 0.519531]  [A loss: 1.033194, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [D loss: 0.652236, acc: 0.740234]  [A loss: 0.964225, acc: 0.000000]\n",
      "2: [D loss: 0.546154, acc: 1.000000]  [A loss: 1.026749, acc: 0.000000]\n",
      "3: [D loss: 0.373406, acc: 0.964844]  [A loss: 1.564447, acc: 0.000000]\n",
      "4: [D loss: 0.320550, acc: 0.970703]  [A loss: 0.292703, acc: 1.000000]\n",
      "5: [D loss: 0.169539, acc: 0.998047]  [A loss: 0.598088, acc: 0.695312]\n",
      "6: [D loss: 0.091678, acc: 0.990234]  [A loss: 0.020747, acc: 1.000000]\n",
      "7: [D loss: 0.063919, acc: 0.992188]  [A loss: 0.016115, acc: 1.000000]\n",
      "8: [D loss: 0.051287, acc: 0.998047]  [A loss: 0.009257, acc: 1.000000]\n",
      "9: [D loss: 0.051266, acc: 0.996094]  [A loss: 0.002556, acc: 1.000000]\n",
      "10: [D loss: 0.041456, acc: 1.000000]  [A loss: 0.005009, acc: 1.000000]\n",
      "11: [D loss: 0.036355, acc: 0.998047]  [A loss: 0.001485, acc: 1.000000]\n",
      "12: [D loss: 0.033876, acc: 0.996094]  [A loss: 0.000881, acc: 1.000000]\n",
      "13: [D loss: 0.026251, acc: 1.000000]  [A loss: 0.002695, acc: 1.000000]\n",
      "14: [D loss: 0.024623, acc: 0.998047]  [A loss: 0.000220, acc: 1.000000]\n",
      "15: [D loss: 0.021981, acc: 0.998047]  [A loss: 0.000326, acc: 1.000000]\n",
      "16: [D loss: 0.019518, acc: 0.998047]  [A loss: 0.000535, acc: 1.000000]\n",
      "17: [D loss: 0.015368, acc: 1.000000]  [A loss: 0.000164, acc: 1.000000]\n",
      "18: [D loss: 0.012368, acc: 1.000000]  [A loss: 0.000945, acc: 1.000000]\n",
      "19: [D loss: 0.011312, acc: 1.000000]  [A loss: 0.000181, acc: 1.000000]\n",
      "20: [D loss: 0.010505, acc: 0.998047]  [A loss: 0.000035, acc: 1.000000]\n",
      "21: [D loss: 0.008647, acc: 1.000000]  [A loss: 0.000054, acc: 1.000000]\n",
      "22: [D loss: 0.006679, acc: 1.000000]  [A loss: 0.000056, acc: 1.000000]\n",
      "23: [D loss: 0.005474, acc: 1.000000]  [A loss: 0.000053, acc: 1.000000]\n",
      "24: [D loss: 0.004419, acc: 1.000000]  [A loss: 0.000064, acc: 1.000000]\n",
      "25: [D loss: 0.005835, acc: 0.998047]  [A loss: 0.000007, acc: 1.000000]\n",
      "26: [D loss: 0.004204, acc: 1.000000]  [A loss: 0.000015, acc: 1.000000]\n",
      "27: [D loss: 0.002962, acc: 1.000000]  [A loss: 0.000113, acc: 1.000000]\n",
      "28: [D loss: 0.002519, acc: 1.000000]  [A loss: 0.000050, acc: 1.000000]\n",
      "29: [D loss: 0.007037, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "30: [D loss: 0.004482, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "31: [D loss: 0.002261, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "32: [D loss: 0.002189, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "33: [D loss: 0.001484, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "34: [D loss: 0.001166, acc: 1.000000]  [A loss: 0.000004, acc: 1.000000]\n",
      "35: [D loss: 0.001342, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "36: [D loss: 0.001051, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "37: [D loss: 0.000721, acc: 1.000000]  [A loss: 0.000009, acc: 1.000000]\n",
      "38: [D loss: 0.000671, acc: 1.000000]  [A loss: 0.000019, acc: 1.000000]\n",
      "39: [D loss: 0.000578, acc: 1.000000]  [A loss: 0.000066, acc: 1.000000]\n",
      "40: [D loss: 0.000725, acc: 1.000000]  [A loss: 0.000038, acc: 1.000000]\n",
      "41: [D loss: 0.001709, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "42: [D loss: 0.002751, acc: 1.000000]  [A loss: 0.007189, acc: 0.996094]\n",
      "43: [D loss: 0.286875, acc: 0.898438]  [A loss: 34.041245, acc: 0.000000]\n",
      "44: [D loss: 0.258747, acc: 0.892578]  [A loss: 0.000000, acc: 1.000000]\n",
      "45: [D loss: 0.198436, acc: 0.925781]  [A loss: 0.000000, acc: 1.000000]\n",
      "46: [D loss: 0.048344, acc: 0.988281]  [A loss: 0.000001, acc: 1.000000]\n",
      "47: [D loss: 0.060529, acc: 0.990234]  [A loss: 0.000183, acc: 1.000000]\n",
      "48: [D loss: 0.032169, acc: 0.994141]  [A loss: 0.005387, acc: 1.000000]\n",
      "49: [D loss: 0.032278, acc: 0.992188]  [A loss: 0.140350, acc: 0.957031]\n",
      "50: [D loss: 1.473373, acc: 0.597656]  [A loss: 27.944176, acc: 0.000000]\n",
      "51: [D loss: 3.002396, acc: 0.500000]  [A loss: 0.005622, acc: 1.000000]\n",
      "52: [D loss: 0.202096, acc: 0.916016]  [A loss: 0.057489, acc: 0.996094]\n",
      "53: [D loss: 0.278024, acc: 0.876953]  [A loss: 0.764013, acc: 0.562500]\n",
      "54: [D loss: 2.328951, acc: 0.501953]  [A loss: 9.846570, acc: 0.000000]\n",
      "55: [D loss: 0.550066, acc: 0.695312]  [A loss: 0.495619, acc: 0.773438]\n",
      "56: [D loss: 1.552072, acc: 0.505859]  [A loss: 7.786232, acc: 0.000000]\n",
      "57: [D loss: 0.374989, acc: 0.851562]  [A loss: 1.298084, acc: 0.152344]\n",
      "58: [D loss: 1.676516, acc: 0.500000]  [A loss: 7.291535, acc: 0.000000]\n",
      "59: [D loss: 0.427463, acc: 0.789062]  [A loss: 1.421377, acc: 0.078125]\n",
      "60: [D loss: 1.371139, acc: 0.498047]  [A loss: 6.146884, acc: 0.000000]\n",
      "61: [D loss: 0.314108, acc: 0.912109]  [A loss: 2.239631, acc: 0.003906]\n",
      "62: [D loss: 0.961155, acc: 0.505859]  [A loss: 5.543516, acc: 0.000000]\n",
      "63: [D loss: 0.304089, acc: 0.941406]  [A loss: 2.415445, acc: 0.000000]\n",
      "64: [D loss: 0.851490, acc: 0.509766]  [A loss: 5.361975, acc: 0.000000]\n",
      "65: [D loss: 0.296398, acc: 0.958984]  [A loss: 2.186640, acc: 0.003906]\n",
      "66: [D loss: 0.775191, acc: 0.519531]  [A loss: 5.084704, acc: 0.000000]\n",
      "67: [D loss: 0.300774, acc: 0.953125]  [A loss: 2.375137, acc: 0.003906]\n",
      "68: [D loss: 0.690308, acc: 0.533203]  [A loss: 4.891675, acc: 0.000000]\n",
      "69: [D loss: 0.286855, acc: 0.951172]  [A loss: 2.303407, acc: 0.007812]\n",
      "70: [D loss: 0.704353, acc: 0.537109]  [A loss: 4.889716, acc: 0.000000]\n",
      "71: [D loss: 0.293502, acc: 0.943359]  [A loss: 1.623678, acc: 0.035156]\n",
      "72: [D loss: 0.879881, acc: 0.509766]  [A loss: 5.573138, acc: 0.000000]\n",
      "73: [D loss: 0.345979, acc: 0.900391]  [A loss: 1.283135, acc: 0.113281]\n",
      "74: [D loss: 0.909741, acc: 0.505859]  [A loss: 5.018579, acc: 0.000000]\n",
      "75: [D loss: 0.315444, acc: 0.937500]  [A loss: 1.677310, acc: 0.023438]\n",
      "76: [D loss: 0.793548, acc: 0.539062]  [A loss: 4.443756, acc: 0.000000]\n",
      "77: [D loss: 0.365685, acc: 0.882812]  [A loss: 1.809083, acc: 0.027344]\n",
      "78: [D loss: 0.728777, acc: 0.566406]  [A loss: 4.332487, acc: 0.000000]\n",
      "79: [D loss: 0.390228, acc: 0.871094]  [A loss: 1.644358, acc: 0.054688]\n",
      "80: [D loss: 0.778280, acc: 0.556641]  [A loss: 4.272803, acc: 0.000000]\n",
      "81: [D loss: 0.408158, acc: 0.857422]  [A loss: 1.378936, acc: 0.128906]\n",
      "82: [D loss: 0.829414, acc: 0.539062]  [A loss: 4.170197, acc: 0.000000]\n",
      "83: [D loss: 0.442434, acc: 0.835938]  [A loss: 1.374954, acc: 0.117188]\n",
      "84: [D loss: 0.808478, acc: 0.544922]  [A loss: 4.110500, acc: 0.000000]\n",
      "85: [D loss: 0.441869, acc: 0.832031]  [A loss: 1.238412, acc: 0.164062]\n",
      "86: [D loss: 0.725876, acc: 0.568359]  [A loss: 3.405864, acc: 0.000000]\n",
      "87: [D loss: 0.392072, acc: 0.890625]  [A loss: 1.445523, acc: 0.058594]\n",
      "88: [D loss: 0.628702, acc: 0.615234]  [A loss: 3.419528, acc: 0.000000]\n",
      "89: [D loss: 0.368054, acc: 0.906250]  [A loss: 1.520204, acc: 0.035156]\n",
      "90: [D loss: 0.600951, acc: 0.619141]  [A loss: 3.453739, acc: 0.000000]\n",
      "91: [D loss: 0.376503, acc: 0.886719]  [A loss: 1.377377, acc: 0.070312]\n",
      "92: [D loss: 0.620621, acc: 0.591797]  [A loss: 3.635832, acc: 0.000000]\n",
      "93: [D loss: 0.407223, acc: 0.882812]  [A loss: 1.128434, acc: 0.156250]\n",
      "94: [D loss: 0.691966, acc: 0.542969]  [A loss: 3.717432, acc: 0.000000]\n",
      "95: [D loss: 0.434494, acc: 0.843750]  [A loss: 0.942930, acc: 0.308594]\n",
      "96: [D loss: 0.735889, acc: 0.533203]  [A loss: 3.321729, acc: 0.000000]\n",
      "97: [D loss: 0.414938, acc: 0.843750]  [A loss: 1.047090, acc: 0.183594]\n",
      "98: [D loss: 0.670312, acc: 0.552734]  [A loss: 2.861496, acc: 0.000000]\n",
      "99: [D loss: 0.374499, acc: 0.898438]  [A loss: 1.367723, acc: 0.066406]\n",
      "100: [D loss: 0.572288, acc: 0.628906]  [A loss: 2.754029, acc: 0.000000]\n",
      "101: [D loss: 0.419276, acc: 0.886719]  [A loss: 1.246917, acc: 0.089844]\n",
      "102: [D loss: 0.618799, acc: 0.593750]  [A loss: 2.847155, acc: 0.000000]\n",
      "103: [D loss: 0.409161, acc: 0.869141]  [A loss: 1.079811, acc: 0.171875]\n",
      "104: [D loss: 0.635734, acc: 0.572266]  [A loss: 2.841215, acc: 0.000000]\n",
      "105: [D loss: 0.424078, acc: 0.867188]  [A loss: 0.976344, acc: 0.238281]\n",
      "106: [D loss: 0.682833, acc: 0.554688]  [A loss: 2.984424, acc: 0.000000]\n",
      "107: [D loss: 0.415491, acc: 0.890625]  [A loss: 0.945668, acc: 0.261719]\n",
      "108: [D loss: 0.679640, acc: 0.537109]  [A loss: 2.943406, acc: 0.000000]\n",
      "109: [D loss: 0.429481, acc: 0.867188]  [A loss: 0.872094, acc: 0.312500]\n",
      "110: [D loss: 0.721001, acc: 0.521484]  [A loss: 2.890386, acc: 0.000000]\n",
      "111: [D loss: 0.425283, acc: 0.876953]  [A loss: 0.889297, acc: 0.312500]\n",
      "112: [D loss: 0.651298, acc: 0.564453]  [A loss: 2.495637, acc: 0.000000]\n",
      "113: [D loss: 0.417910, acc: 0.898438]  [A loss: 1.126105, acc: 0.105469]\n",
      "114: [D loss: 0.630907, acc: 0.550781]  [A loss: 2.990981, acc: 0.000000]\n",
      "115: [D loss: 0.439107, acc: 0.849609]  [A loss: 0.890844, acc: 0.269531]\n",
      "116: [D loss: 0.735162, acc: 0.511719]  [A loss: 3.085519, acc: 0.000000]\n",
      "117: [D loss: 0.471316, acc: 0.826172]  [A loss: 0.729144, acc: 0.488281]\n",
      "118: [D loss: 0.742724, acc: 0.511719]  [A loss: 2.423587, acc: 0.000000]\n",
      "119: [D loss: 0.457709, acc: 0.873047]  [A loss: 1.028200, acc: 0.136719]\n",
      "120: [D loss: 0.646077, acc: 0.542969]  [A loss: 2.327694, acc: 0.000000]\n",
      "121: [D loss: 0.473589, acc: 0.869141]  [A loss: 1.136882, acc: 0.109375]\n",
      "122: [D loss: 0.660537, acc: 0.546875]  [A loss: 2.658703, acc: 0.000000]\n",
      "123: [D loss: 0.491498, acc: 0.832031]  [A loss: 0.875678, acc: 0.289062]\n",
      "124: [D loss: 0.737741, acc: 0.517578]  [A loss: 2.459587, acc: 0.000000]\n",
      "125: [D loss: 0.525097, acc: 0.798828]  [A loss: 0.767994, acc: 0.425781]\n",
      "126: [D loss: 0.759321, acc: 0.515625]  [A loss: 2.269467, acc: 0.000000]\n",
      "127: [D loss: 0.530136, acc: 0.798828]  [A loss: 0.794067, acc: 0.390625]\n",
      "128: [D loss: 0.716158, acc: 0.517578]  [A loss: 2.105679, acc: 0.000000]\n",
      "129: [D loss: 0.528336, acc: 0.822266]  [A loss: 0.902183, acc: 0.234375]\n",
      "130: [D loss: 0.687618, acc: 0.531250]  [A loss: 1.971478, acc: 0.000000]\n",
      "131: [D loss: 0.540700, acc: 0.796875]  [A loss: 1.080057, acc: 0.125000]\n",
      "132: [D loss: 0.632718, acc: 0.560547]  [A loss: 2.018913, acc: 0.000000]\n",
      "133: [D loss: 0.528904, acc: 0.785156]  [A loss: 0.988655, acc: 0.152344]\n",
      "134: [D loss: 0.692239, acc: 0.539062]  [A loss: 2.406214, acc: 0.000000]\n",
      "135: [D loss: 0.554791, acc: 0.761719]  [A loss: 0.674028, acc: 0.558594]\n",
      "136: [D loss: 0.818806, acc: 0.503906]  [A loss: 2.190651, acc: 0.000000]\n",
      "137: [D loss: 0.576764, acc: 0.755859]  [A loss: 0.713233, acc: 0.433594]\n",
      "138: [D loss: 0.735093, acc: 0.507812]  [A loss: 1.712743, acc: 0.000000]\n",
      "139: [D loss: 0.568516, acc: 0.757812]  [A loss: 0.908332, acc: 0.214844]\n",
      "140: [D loss: 0.658250, acc: 0.546875]  [A loss: 1.648020, acc: 0.000000]\n",
      "141: [D loss: 0.574578, acc: 0.750000]  [A loss: 1.014770, acc: 0.089844]\n",
      "142: [D loss: 0.659982, acc: 0.554688]  [A loss: 1.910441, acc: 0.000000]\n",
      "143: [D loss: 0.585944, acc: 0.740234]  [A loss: 0.846002, acc: 0.312500]\n",
      "144: [D loss: 0.708843, acc: 0.531250]  [A loss: 1.967489, acc: 0.000000]\n",
      "145: [D loss: 0.576908, acc: 0.730469]  [A loss: 0.709223, acc: 0.511719]\n",
      "146: [D loss: 0.735613, acc: 0.513672]  [A loss: 1.895807, acc: 0.000000]\n",
      "147: [D loss: 0.585159, acc: 0.716797]  [A loss: 0.759532, acc: 0.425781]\n",
      "148: [D loss: 0.735872, acc: 0.511719]  [A loss: 1.657109, acc: 0.000000]\n",
      "149: [D loss: 0.576387, acc: 0.740234]  [A loss: 0.809896, acc: 0.335938]\n",
      "150: [D loss: 0.685304, acc: 0.529297]  [A loss: 1.693398, acc: 0.000000]\n",
      "151: [D loss: 0.561740, acc: 0.769531]  [A loss: 0.927094, acc: 0.175781]\n",
      "152: [D loss: 0.683003, acc: 0.511719]  [A loss: 2.017113, acc: 0.000000]\n",
      "153: [D loss: 0.597029, acc: 0.722656]  [A loss: 0.681957, acc: 0.546875]\n",
      "154: [D loss: 0.754895, acc: 0.511719]  [A loss: 1.976441, acc: 0.000000]\n",
      "155: [D loss: 0.581639, acc: 0.742188]  [A loss: 0.722582, acc: 0.437500]\n",
      "156: [D loss: 0.714642, acc: 0.498047]  [A loss: 1.567975, acc: 0.000000]\n",
      "157: [D loss: 0.602416, acc: 0.720703]  [A loss: 0.888932, acc: 0.183594]\n",
      "158: [D loss: 0.678995, acc: 0.500000]  [A loss: 1.672721, acc: 0.000000]\n",
      "159: [D loss: 0.581728, acc: 0.759766]  [A loss: 0.849374, acc: 0.269531]\n",
      "160: [D loss: 0.682790, acc: 0.509766]  [A loss: 1.716636, acc: 0.000000]\n",
      "161: [D loss: 0.593808, acc: 0.730469]  [A loss: 0.742342, acc: 0.472656]\n",
      "162: [D loss: 0.707595, acc: 0.513672]  [A loss: 1.758860, acc: 0.000000]\n",
      "163: [D loss: 0.592415, acc: 0.728516]  [A loss: 0.749452, acc: 0.429688]\n",
      "164: [D loss: 0.681970, acc: 0.509766]  [A loss: 1.634056, acc: 0.000000]\n",
      "165: [D loss: 0.594609, acc: 0.751953]  [A loss: 0.769728, acc: 0.343750]\n",
      "166: [D loss: 0.703593, acc: 0.496094]  [A loss: 1.605543, acc: 0.000000]\n",
      "167: [D loss: 0.587801, acc: 0.779297]  [A loss: 0.751572, acc: 0.425781]\n",
      "168: [D loss: 0.668253, acc: 0.515625]  [A loss: 1.559902, acc: 0.000000]\n",
      "169: [D loss: 0.600764, acc: 0.728516]  [A loss: 0.759572, acc: 0.398438]\n",
      "170: [D loss: 0.691339, acc: 0.515625]  [A loss: 1.544162, acc: 0.000000]\n",
      "171: [D loss: 0.584622, acc: 0.781250]  [A loss: 0.766893, acc: 0.367188]\n",
      "172: [D loss: 0.664676, acc: 0.503906]  [A loss: 1.497077, acc: 0.000000]\n",
      "173: [D loss: 0.582155, acc: 0.765625]  [A loss: 0.847062, acc: 0.250000]\n",
      "174: [D loss: 0.690578, acc: 0.533203]  [A loss: 1.714492, acc: 0.000000]\n",
      "175: [D loss: 0.620488, acc: 0.664062]  [A loss: 0.675509, acc: 0.562500]\n",
      "176: [D loss: 0.712351, acc: 0.505859]  [A loss: 1.526292, acc: 0.000000]\n",
      "177: [D loss: 0.618525, acc: 0.679688]  [A loss: 0.733372, acc: 0.425781]\n",
      "178: [D loss: 0.689167, acc: 0.500000]  [A loss: 1.392069, acc: 0.000000]\n",
      "179: [D loss: 0.619157, acc: 0.675781]  [A loss: 0.858145, acc: 0.210938]\n",
      "180: [D loss: 0.658421, acc: 0.550781]  [A loss: 1.358710, acc: 0.000000]\n",
      "181: [D loss: 0.591432, acc: 0.738281]  [A loss: 0.865803, acc: 0.175781]\n",
      "182: [D loss: 0.668222, acc: 0.523438]  [A loss: 1.548416, acc: 0.000000]\n",
      "183: [D loss: 0.615864, acc: 0.722656]  [A loss: 0.716129, acc: 0.480469]\n",
      "184: [D loss: 0.710113, acc: 0.511719]  [A loss: 1.606205, acc: 0.000000]\n",
      "185: [D loss: 0.631754, acc: 0.644531]  [A loss: 0.652838, acc: 0.675781]\n",
      "186: [D loss: 0.701347, acc: 0.503906]  [A loss: 1.255383, acc: 0.000000]\n",
      "187: [D loss: 0.617203, acc: 0.750000]  [A loss: 0.812164, acc: 0.250000]\n",
      "188: [D loss: 0.655365, acc: 0.546875]  [A loss: 1.193696, acc: 0.000000]\n",
      "189: [D loss: 0.593449, acc: 0.746094]  [A loss: 0.941785, acc: 0.097656]\n",
      "190: [D loss: 0.633054, acc: 0.580078]  [A loss: 1.380920, acc: 0.000000]\n",
      "191: [D loss: 0.596682, acc: 0.750000]  [A loss: 0.842719, acc: 0.218750]\n",
      "192: [D loss: 0.659825, acc: 0.544922]  [A loss: 1.547825, acc: 0.000000]\n",
      "193: [D loss: 0.606413, acc: 0.708984]  [A loss: 0.667416, acc: 0.554688]\n",
      "194: [D loss: 0.737795, acc: 0.519531]  [A loss: 1.505654, acc: 0.000000]\n",
      "195: [D loss: 0.626120, acc: 0.650391]  [A loss: 0.677391, acc: 0.542969]\n",
      "196: [D loss: 0.694085, acc: 0.513672]  [A loss: 1.164295, acc: 0.003906]\n",
      "197: [D loss: 0.598342, acc: 0.769531]  [A loss: 0.878454, acc: 0.132812]\n",
      "198: [D loss: 0.629512, acc: 0.583984]  [A loss: 1.177629, acc: 0.011719]\n",
      "199: [D loss: 0.587811, acc: 0.767578]  [A loss: 0.887324, acc: 0.167969]\n",
      "200: [D loss: 0.617271, acc: 0.566406]  [A loss: 1.286811, acc: 0.000000]\n",
      "201: [D loss: 0.582230, acc: 0.767578]  [A loss: 0.860992, acc: 0.199219]\n",
      "202: [D loss: 0.640296, acc: 0.576172]  [A loss: 1.502093, acc: 0.000000]\n",
      "203: [D loss: 0.605253, acc: 0.679688]  [A loss: 0.664661, acc: 0.578125]\n",
      "204: [D loss: 0.723550, acc: 0.513672]  [A loss: 1.578007, acc: 0.000000]\n",
      "205: [D loss: 0.613262, acc: 0.652344]  [A loss: 0.642740, acc: 0.621094]\n",
      "206: [D loss: 0.674143, acc: 0.527344]  [A loss: 1.162853, acc: 0.011719]\n",
      "207: [D loss: 0.593654, acc: 0.751953]  [A loss: 0.782474, acc: 0.304688]\n",
      "208: [D loss: 0.640745, acc: 0.576172]  [A loss: 1.192982, acc: 0.011719]\n",
      "209: [D loss: 0.611285, acc: 0.693359]  [A loss: 0.837164, acc: 0.253906]\n",
      "210: [D loss: 0.638746, acc: 0.566406]  [A loss: 1.481668, acc: 0.003906]\n",
      "211: [D loss: 0.590885, acc: 0.744141]  [A loss: 0.635358, acc: 0.644531]\n",
      "212: [D loss: 0.745436, acc: 0.511719]  [A loss: 1.744646, acc: 0.000000]\n",
      "213: [D loss: 0.637586, acc: 0.615234]  [A loss: 0.602907, acc: 0.722656]\n",
      "214: [D loss: 0.711569, acc: 0.515625]  [A loss: 1.178539, acc: 0.007812]\n",
      "215: [D loss: 0.592019, acc: 0.738281]  [A loss: 0.828583, acc: 0.281250]\n",
      "216: [D loss: 0.636542, acc: 0.568359]  [A loss: 1.334256, acc: 0.000000]\n",
      "217: [D loss: 0.577402, acc: 0.744141]  [A loss: 0.881635, acc: 0.195312]\n",
      "218: [D loss: 0.639270, acc: 0.580078]  [A loss: 1.509876, acc: 0.007812]\n",
      "219: [D loss: 0.592488, acc: 0.728516]  [A loss: 0.856430, acc: 0.246094]\n",
      "220: [D loss: 0.665673, acc: 0.539062]  [A loss: 1.802667, acc: 0.000000]\n",
      "221: [D loss: 0.582296, acc: 0.712891]  [A loss: 0.649240, acc: 0.597656]\n",
      "222: [D loss: 0.734581, acc: 0.529297]  [A loss: 1.530627, acc: 0.000000]\n",
      "223: [D loss: 0.616846, acc: 0.675781]  [A loss: 0.753615, acc: 0.394531]\n",
      "224: [D loss: 0.683936, acc: 0.533203]  [A loss: 1.323624, acc: 0.000000]\n",
      "225: [D loss: 0.603503, acc: 0.714844]  [A loss: 0.940779, acc: 0.140625]\n",
      "226: [D loss: 0.626816, acc: 0.611328]  [A loss: 1.348748, acc: 0.000000]\n",
      "227: [D loss: 0.583356, acc: 0.757812]  [A loss: 0.793488, acc: 0.324219]\n",
      "228: [D loss: 0.673454, acc: 0.556641]  [A loss: 1.537154, acc: 0.000000]\n",
      "229: [D loss: 0.615395, acc: 0.691406]  [A loss: 0.715226, acc: 0.480469]\n",
      "230: [D loss: 0.707812, acc: 0.519531]  [A loss: 1.490539, acc: 0.000000]\n",
      "231: [D loss: 0.599653, acc: 0.705078]  [A loss: 0.744376, acc: 0.402344]\n",
      "232: [D loss: 0.691341, acc: 0.541016]  [A loss: 1.338007, acc: 0.000000]\n",
      "233: [D loss: 0.608918, acc: 0.718750]  [A loss: 0.766644, acc: 0.359375]\n",
      "234: [D loss: 0.663883, acc: 0.544922]  [A loss: 1.190886, acc: 0.000000]\n",
      "235: [D loss: 0.608930, acc: 0.712891]  [A loss: 0.908338, acc: 0.144531]\n",
      "236: [D loss: 0.643996, acc: 0.585938]  [A loss: 1.224428, acc: 0.000000]\n",
      "237: [D loss: 0.614152, acc: 0.695312]  [A loss: 0.894790, acc: 0.160156]\n",
      "238: [D loss: 0.639489, acc: 0.585938]  [A loss: 1.184756, acc: 0.015625]\n",
      "239: [D loss: 0.611513, acc: 0.708984]  [A loss: 0.928174, acc: 0.097656]\n",
      "240: [D loss: 0.654596, acc: 0.539062]  [A loss: 1.454656, acc: 0.000000]\n",
      "241: [D loss: 0.612046, acc: 0.695312]  [A loss: 0.672552, acc: 0.558594]\n",
      "242: [D loss: 0.733130, acc: 0.509766]  [A loss: 1.615854, acc: 0.000000]\n",
      "243: [D loss: 0.628866, acc: 0.626953]  [A loss: 0.661628, acc: 0.597656]\n",
      "244: [D loss: 0.705151, acc: 0.517578]  [A loss: 1.127779, acc: 0.007812]\n",
      "245: [D loss: 0.604254, acc: 0.720703]  [A loss: 0.799216, acc: 0.257812]\n",
      "246: [D loss: 0.667253, acc: 0.558594]  [A loss: 1.144507, acc: 0.019531]\n",
      "247: [D loss: 0.614547, acc: 0.701172]  [A loss: 0.907978, acc: 0.140625]\n",
      "248: [D loss: 0.632938, acc: 0.605469]  [A loss: 1.126971, acc: 0.007812]\n",
      "249: [D loss: 0.606364, acc: 0.697266]  [A loss: 0.943916, acc: 0.125000]\n",
      "250: [D loss: 0.647043, acc: 0.576172]  [A loss: 1.383172, acc: 0.003906]\n",
      "251: [D loss: 0.611344, acc: 0.703125]  [A loss: 0.721807, acc: 0.480469]\n",
      "252: [D loss: 0.706603, acc: 0.525391]  [A loss: 1.615759, acc: 0.000000]\n",
      "253: [D loss: 0.642769, acc: 0.609375]  [A loss: 0.637752, acc: 0.648438]\n",
      "254: [D loss: 0.802080, acc: 0.496094]  [A loss: 1.456323, acc: 0.000000]\n",
      "255: [D loss: 0.621430, acc: 0.683594]  [A loss: 0.731258, acc: 0.441406]\n",
      "256: [D loss: 0.678061, acc: 0.542969]  [A loss: 1.200206, acc: 0.003906]\n",
      "257: [D loss: 0.615596, acc: 0.714844]  [A loss: 0.769152, acc: 0.292969]\n",
      "258: [D loss: 0.668609, acc: 0.546875]  [A loss: 1.108988, acc: 0.007812]\n",
      "259: [D loss: 0.618621, acc: 0.716797]  [A loss: 0.870368, acc: 0.164062]\n",
      "260: [D loss: 0.662524, acc: 0.537109]  [A loss: 1.241231, acc: 0.000000]\n",
      "261: [D loss: 0.618140, acc: 0.718750]  [A loss: 0.826529, acc: 0.199219]\n",
      "262: [D loss: 0.693388, acc: 0.517578]  [A loss: 1.376633, acc: 0.000000]\n",
      "263: [D loss: 0.635733, acc: 0.679688]  [A loss: 0.728875, acc: 0.441406]\n",
      "264: [D loss: 0.697192, acc: 0.509766]  [A loss: 1.344455, acc: 0.000000]\n",
      "265: [D loss: 0.617492, acc: 0.695312]  [A loss: 0.727491, acc: 0.433594]\n",
      "266: [D loss: 0.698096, acc: 0.519531]  [A loss: 1.210279, acc: 0.003906]\n",
      "267: [D loss: 0.624801, acc: 0.695312]  [A loss: 0.804966, acc: 0.269531]\n",
      "268: [D loss: 0.684276, acc: 0.537109]  [A loss: 1.223784, acc: 0.007812]\n",
      "269: [D loss: 0.635319, acc: 0.660156]  [A loss: 0.747336, acc: 0.394531]\n",
      "270: [D loss: 0.681562, acc: 0.523438]  [A loss: 1.320875, acc: 0.000000]\n",
      "271: [D loss: 0.632417, acc: 0.707031]  [A loss: 0.760095, acc: 0.339844]\n",
      "272: [D loss: 0.687402, acc: 0.523438]  [A loss: 1.235587, acc: 0.003906]\n",
      "273: [D loss: 0.636690, acc: 0.679688]  [A loss: 0.765211, acc: 0.375000]\n",
      "274: [D loss: 0.682115, acc: 0.537109]  [A loss: 1.345592, acc: 0.000000]\n",
      "275: [D loss: 0.633838, acc: 0.675781]  [A loss: 0.741926, acc: 0.398438]\n",
      "276: [D loss: 0.696508, acc: 0.521484]  [A loss: 1.233746, acc: 0.003906]\n",
      "277: [D loss: 0.627524, acc: 0.697266]  [A loss: 0.797825, acc: 0.246094]\n",
      "278: [D loss: 0.686836, acc: 0.525391]  [A loss: 1.241560, acc: 0.007812]\n",
      "279: [D loss: 0.626688, acc: 0.695312]  [A loss: 0.800829, acc: 0.285156]\n",
      "280: [D loss: 0.683368, acc: 0.544922]  [A loss: 1.316953, acc: 0.003906]\n",
      "281: [D loss: 0.644803, acc: 0.634766]  [A loss: 0.730790, acc: 0.425781]\n",
      "282: [D loss: 0.686771, acc: 0.521484]  [A loss: 1.227823, acc: 0.000000]\n",
      "283: [D loss: 0.625974, acc: 0.701172]  [A loss: 0.719654, acc: 0.453125]\n",
      "284: [D loss: 0.696339, acc: 0.496094]  [A loss: 1.244935, acc: 0.000000]\n",
      "285: [D loss: 0.634495, acc: 0.646484]  [A loss: 0.670709, acc: 0.582031]\n",
      "286: [D loss: 0.726196, acc: 0.505859]  [A loss: 1.277990, acc: 0.000000]\n",
      "287: [D loss: 0.642527, acc: 0.650391]  [A loss: 0.720923, acc: 0.414062]\n",
      "288: [D loss: 0.676570, acc: 0.527344]  [A loss: 1.108976, acc: 0.003906]\n",
      "289: [D loss: 0.640624, acc: 0.689453]  [A loss: 0.750196, acc: 0.394531]\n",
      "290: [D loss: 0.696394, acc: 0.531250]  [A loss: 1.136165, acc: 0.015625]\n",
      "291: [D loss: 0.639147, acc: 0.671875]  [A loss: 0.802945, acc: 0.269531]\n",
      "292: [D loss: 0.672675, acc: 0.539062]  [A loss: 1.143791, acc: 0.003906]\n",
      "293: [D loss: 0.637216, acc: 0.666016]  [A loss: 0.745367, acc: 0.390625]\n",
      "294: [D loss: 0.697116, acc: 0.529297]  [A loss: 1.272940, acc: 0.003906]\n",
      "295: [D loss: 0.640330, acc: 0.646484]  [A loss: 0.651199, acc: 0.628906]\n",
      "296: [D loss: 0.699899, acc: 0.517578]  [A loss: 1.159456, acc: 0.000000]\n",
      "297: [D loss: 0.650756, acc: 0.648438]  [A loss: 0.736873, acc: 0.406250]\n",
      "298: [D loss: 0.727258, acc: 0.501953]  [A loss: 1.191201, acc: 0.007812]\n",
      "299: [D loss: 0.646464, acc: 0.636719]  [A loss: 0.790069, acc: 0.292969]\n",
      "300: [D loss: 0.663004, acc: 0.546875]  [A loss: 1.078174, acc: 0.019531]\n",
      "301: [D loss: 0.641859, acc: 0.671875]  [A loss: 0.797282, acc: 0.269531]\n",
      "302: [D loss: 0.683098, acc: 0.523438]  [A loss: 1.188141, acc: 0.011719]\n",
      "303: [D loss: 0.650554, acc: 0.628906]  [A loss: 0.750884, acc: 0.363281]\n",
      "304: [D loss: 0.703354, acc: 0.517578]  [A loss: 1.228747, acc: 0.000000]\n",
      "305: [D loss: 0.634432, acc: 0.691406]  [A loss: 0.726018, acc: 0.457031]\n",
      "306: [D loss: 0.687651, acc: 0.519531]  [A loss: 1.315822, acc: 0.000000]\n",
      "307: [D loss: 0.652811, acc: 0.615234]  [A loss: 0.672742, acc: 0.535156]\n",
      "308: [D loss: 0.684871, acc: 0.519531]  [A loss: 1.152808, acc: 0.000000]\n",
      "309: [D loss: 0.645346, acc: 0.656250]  [A loss: 0.715188, acc: 0.453125]\n",
      "310: [D loss: 0.693035, acc: 0.515625]  [A loss: 1.153316, acc: 0.003906]\n",
      "311: [D loss: 0.641370, acc: 0.656250]  [A loss: 0.777715, acc: 0.316406]\n",
      "312: [D loss: 0.667436, acc: 0.562500]  [A loss: 1.164840, acc: 0.000000]\n",
      "313: [D loss: 0.628115, acc: 0.691406]  [A loss: 0.761689, acc: 0.351562]\n",
      "314: [D loss: 0.691056, acc: 0.529297]  [A loss: 1.285078, acc: 0.000000]\n",
      "315: [D loss: 0.634695, acc: 0.656250]  [A loss: 0.689085, acc: 0.550781]\n",
      "316: [D loss: 0.712770, acc: 0.509766]  [A loss: 1.376081, acc: 0.000000]\n",
      "317: [D loss: 0.649444, acc: 0.630859]  [A loss: 0.648187, acc: 0.656250]\n",
      "318: [D loss: 0.697503, acc: 0.521484]  [A loss: 1.148872, acc: 0.000000]\n",
      "319: [D loss: 0.639575, acc: 0.691406]  [A loss: 0.761544, acc: 0.332031]\n",
      "320: [D loss: 0.666639, acc: 0.531250]  [A loss: 1.043865, acc: 0.019531]\n",
      "321: [D loss: 0.647348, acc: 0.626953]  [A loss: 0.865647, acc: 0.175781]\n",
      "322: [D loss: 0.655645, acc: 0.570312]  [A loss: 1.129292, acc: 0.011719]\n",
      "323: [D loss: 0.629586, acc: 0.697266]  [A loss: 0.795505, acc: 0.273438]\n",
      "324: [D loss: 0.678978, acc: 0.539062]  [A loss: 1.208841, acc: 0.000000]\n",
      "325: [D loss: 0.637689, acc: 0.687500]  [A loss: 0.747992, acc: 0.371094]\n",
      "326: [D loss: 0.687290, acc: 0.554688]  [A loss: 1.282175, acc: 0.000000]\n",
      "327: [D loss: 0.659373, acc: 0.607422]  [A loss: 0.660954, acc: 0.609375]\n",
      "328: [D loss: 0.724895, acc: 0.513672]  [A loss: 1.239338, acc: 0.000000]\n",
      "329: [D loss: 0.635216, acc: 0.654297]  [A loss: 0.738450, acc: 0.386719]\n",
      "330: [D loss: 0.699156, acc: 0.527344]  [A loss: 1.198448, acc: 0.003906]\n",
      "331: [D loss: 0.634994, acc: 0.658203]  [A loss: 0.645096, acc: 0.609375]\n",
      "332: [D loss: 0.699473, acc: 0.509766]  [A loss: 1.068771, acc: 0.015625]\n",
      "333: [D loss: 0.651409, acc: 0.628906]  [A loss: 0.806919, acc: 0.218750]\n",
      "334: [D loss: 0.686705, acc: 0.533203]  [A loss: 1.069648, acc: 0.039062]\n",
      "335: [D loss: 0.628427, acc: 0.679688]  [A loss: 0.810195, acc: 0.230469]\n",
      "336: [D loss: 0.676122, acc: 0.566406]  [A loss: 1.073833, acc: 0.011719]\n",
      "337: [D loss: 0.640984, acc: 0.687500]  [A loss: 0.806373, acc: 0.300781]\n",
      "338: [D loss: 0.657440, acc: 0.580078]  [A loss: 1.095266, acc: 0.023438]\n",
      "339: [D loss: 0.644851, acc: 0.650391]  [A loss: 0.931989, acc: 0.132812]\n",
      "340: [D loss: 0.665741, acc: 0.582031]  [A loss: 1.168228, acc: 0.007812]\n",
      "341: [D loss: 0.626871, acc: 0.693359]  [A loss: 0.740206, acc: 0.402344]\n",
      "342: [D loss: 0.693834, acc: 0.552734]  [A loss: 1.398809, acc: 0.000000]\n",
      "343: [D loss: 0.656649, acc: 0.617188]  [A loss: 0.585344, acc: 0.746094]\n",
      "344: [D loss: 0.733860, acc: 0.505859]  [A loss: 1.212108, acc: 0.000000]\n",
      "345: [D loss: 0.656960, acc: 0.593750]  [A loss: 0.669021, acc: 0.585938]\n",
      "346: [D loss: 0.728788, acc: 0.515625]  [A loss: 1.080714, acc: 0.007812]\n",
      "347: [D loss: 0.644390, acc: 0.636719]  [A loss: 0.796394, acc: 0.253906]\n",
      "348: [D loss: 0.676158, acc: 0.558594]  [A loss: 0.916908, acc: 0.089844]\n",
      "349: [D loss: 0.638630, acc: 0.656250]  [A loss: 0.862102, acc: 0.203125]\n",
      "350: [D loss: 0.661698, acc: 0.589844]  [A loss: 0.946724, acc: 0.082031]\n",
      "351: [D loss: 0.649835, acc: 0.613281]  [A loss: 0.942006, acc: 0.074219]\n",
      "352: [D loss: 0.653463, acc: 0.587891]  [A loss: 0.976255, acc: 0.062500]\n",
      "353: [D loss: 0.655849, acc: 0.603516]  [A loss: 0.929923, acc: 0.125000]\n",
      "354: [D loss: 0.658910, acc: 0.568359]  [A loss: 0.998704, acc: 0.062500]\n",
      "355: [D loss: 0.657091, acc: 0.611328]  [A loss: 1.026075, acc: 0.042969]\n",
      "356: [D loss: 0.656174, acc: 0.621094]  [A loss: 1.003470, acc: 0.062500]\n",
      "357: [D loss: 0.667507, acc: 0.585938]  [A loss: 1.087156, acc: 0.023438]\n",
      "358: [D loss: 0.651717, acc: 0.636719]  [A loss: 0.918439, acc: 0.105469]\n",
      "359: [D loss: 0.667993, acc: 0.601562]  [A loss: 1.230486, acc: 0.007812]\n",
      "360: [D loss: 0.631226, acc: 0.679688]  [A loss: 0.732502, acc: 0.441406]\n",
      "361: [D loss: 0.716232, acc: 0.519531]  [A loss: 1.559951, acc: 0.000000]\n",
      "362: [D loss: 0.683813, acc: 0.541016]  [A loss: 0.534926, acc: 0.855469]\n",
      "363: [D loss: 0.799077, acc: 0.505859]  [A loss: 1.136180, acc: 0.007812]\n",
      "364: [D loss: 0.642869, acc: 0.660156]  [A loss: 0.726576, acc: 0.460938]\n",
      "365: [D loss: 0.683544, acc: 0.529297]  [A loss: 0.979677, acc: 0.042969]\n",
      "366: [D loss: 0.653397, acc: 0.630859]  [A loss: 0.774844, acc: 0.332031]\n",
      "367: [D loss: 0.694511, acc: 0.546875]  [A loss: 1.011706, acc: 0.042969]\n",
      "368: [D loss: 0.663552, acc: 0.591797]  [A loss: 0.789060, acc: 0.312500]\n",
      "369: [D loss: 0.685975, acc: 0.548828]  [A loss: 0.981016, acc: 0.046875]\n",
      "370: [D loss: 0.648682, acc: 0.664062]  [A loss: 0.842143, acc: 0.207031]\n",
      "371: [D loss: 0.670064, acc: 0.587891]  [A loss: 0.971769, acc: 0.066406]\n",
      "372: [D loss: 0.656859, acc: 0.619141]  [A loss: 0.893405, acc: 0.144531]\n",
      "373: [D loss: 0.660113, acc: 0.605469]  [A loss: 0.962409, acc: 0.078125]\n",
      "374: [D loss: 0.635667, acc: 0.648438]  [A loss: 0.921832, acc: 0.121094]\n",
      "375: [D loss: 0.672126, acc: 0.576172]  [A loss: 1.134339, acc: 0.019531]\n",
      "376: [D loss: 0.649627, acc: 0.613281]  [A loss: 0.765697, acc: 0.363281]\n",
      "377: [D loss: 0.704738, acc: 0.523438]  [A loss: 1.327147, acc: 0.000000]\n",
      "378: [D loss: 0.661379, acc: 0.603516]  [A loss: 0.595507, acc: 0.761719]\n",
      "379: [D loss: 0.763870, acc: 0.511719]  [A loss: 1.236798, acc: 0.007812]\n",
      "380: [D loss: 0.670505, acc: 0.587891]  [A loss: 0.684855, acc: 0.523438]\n",
      "381: [D loss: 0.692518, acc: 0.533203]  [A loss: 1.043147, acc: 0.027344]\n",
      "382: [D loss: 0.666502, acc: 0.601562]  [A loss: 0.800287, acc: 0.257812]\n",
      "383: [D loss: 0.676145, acc: 0.564453]  [A loss: 0.964578, acc: 0.074219]\n",
      "384: [D loss: 0.657917, acc: 0.609375]  [A loss: 0.912122, acc: 0.105469]\n",
      "385: [D loss: 0.666123, acc: 0.576172]  [A loss: 0.946298, acc: 0.078125]\n",
      "386: [D loss: 0.661165, acc: 0.615234]  [A loss: 0.872728, acc: 0.152344]\n",
      "387: [D loss: 0.654669, acc: 0.615234]  [A loss: 1.035196, acc: 0.042969]\n",
      "388: [D loss: 0.656167, acc: 0.628906]  [A loss: 0.859853, acc: 0.195312]\n",
      "389: [D loss: 0.676220, acc: 0.574219]  [A loss: 1.098141, acc: 0.023438]\n",
      "390: [D loss: 0.651704, acc: 0.630859]  [A loss: 0.821022, acc: 0.257812]\n",
      "391: [D loss: 0.679232, acc: 0.564453]  [A loss: 1.222180, acc: 0.011719]\n",
      "392: [D loss: 0.651139, acc: 0.619141]  [A loss: 0.719802, acc: 0.488281]\n",
      "393: [D loss: 0.719647, acc: 0.523438]  [A loss: 1.276097, acc: 0.007812]\n",
      "394: [D loss: 0.665700, acc: 0.609375]  [A loss: 0.652867, acc: 0.621094]\n",
      "395: [D loss: 0.705375, acc: 0.525391]  [A loss: 1.100378, acc: 0.007812]\n",
      "396: [D loss: 0.666324, acc: 0.587891]  [A loss: 0.760755, acc: 0.367188]\n",
      "397: [D loss: 0.686782, acc: 0.541016]  [A loss: 1.068319, acc: 0.015625]\n",
      "398: [D loss: 0.653648, acc: 0.648438]  [A loss: 0.774881, acc: 0.335938]\n",
      "399: [D loss: 0.675459, acc: 0.583984]  [A loss: 0.985952, acc: 0.058594]\n",
      "400: [D loss: 0.669752, acc: 0.585938]  [A loss: 0.865234, acc: 0.167969]\n",
      "401: [D loss: 0.684626, acc: 0.542969]  [A loss: 0.915864, acc: 0.101562]\n",
      "402: [D loss: 0.670476, acc: 0.591797]  [A loss: 0.878058, acc: 0.144531]\n",
      "403: [D loss: 0.673174, acc: 0.558594]  [A loss: 0.986298, acc: 0.050781]\n",
      "404: [D loss: 0.650760, acc: 0.650391]  [A loss: 0.814371, acc: 0.273438]\n",
      "405: [D loss: 0.674463, acc: 0.554688]  [A loss: 1.113598, acc: 0.019531]\n",
      "406: [D loss: 0.643784, acc: 0.636719]  [A loss: 0.703740, acc: 0.515625]\n",
      "407: [D loss: 0.713672, acc: 0.513672]  [A loss: 1.257188, acc: 0.000000]\n",
      "408: [D loss: 0.667751, acc: 0.593750]  [A loss: 0.643942, acc: 0.613281]\n",
      "409: [D loss: 0.730834, acc: 0.501953]  [A loss: 1.163410, acc: 0.007812]\n",
      "410: [D loss: 0.662302, acc: 0.599609]  [A loss: 0.691097, acc: 0.539062]\n",
      "411: [D loss: 0.702683, acc: 0.515625]  [A loss: 0.981200, acc: 0.027344]\n",
      "412: [D loss: 0.654048, acc: 0.617188]  [A loss: 0.852290, acc: 0.195312]\n",
      "413: [D loss: 0.674874, acc: 0.589844]  [A loss: 0.938834, acc: 0.097656]\n",
      "414: [D loss: 0.658642, acc: 0.615234]  [A loss: 0.830635, acc: 0.230469]\n",
      "415: [D loss: 0.669401, acc: 0.591797]  [A loss: 0.964111, acc: 0.062500]\n",
      "416: [D loss: 0.657869, acc: 0.603516]  [A loss: 0.887649, acc: 0.125000]\n",
      "417: [D loss: 0.678021, acc: 0.580078]  [A loss: 0.967195, acc: 0.062500]\n",
      "418: [D loss: 0.656643, acc: 0.609375]  [A loss: 0.846928, acc: 0.207031]\n",
      "419: [D loss: 0.685157, acc: 0.560547]  [A loss: 1.076915, acc: 0.019531]\n",
      "420: [D loss: 0.665730, acc: 0.601562]  [A loss: 0.818384, acc: 0.226562]\n",
      "421: [D loss: 0.675004, acc: 0.580078]  [A loss: 1.046052, acc: 0.023438]\n",
      "422: [D loss: 0.661481, acc: 0.601562]  [A loss: 0.800973, acc: 0.312500]\n",
      "423: [D loss: 0.698460, acc: 0.552734]  [A loss: 1.089158, acc: 0.023438]\n",
      "424: [D loss: 0.666053, acc: 0.621094]  [A loss: 0.765766, acc: 0.339844]\n",
      "425: [D loss: 0.708768, acc: 0.535156]  [A loss: 1.145355, acc: 0.003906]\n",
      "426: [D loss: 0.669100, acc: 0.585938]  [A loss: 0.662438, acc: 0.570312]\n",
      "427: [D loss: 0.697922, acc: 0.523438]  [A loss: 1.016559, acc: 0.039062]\n",
      "428: [D loss: 0.663944, acc: 0.601562]  [A loss: 0.743964, acc: 0.394531]\n",
      "429: [D loss: 0.698804, acc: 0.517578]  [A loss: 1.082850, acc: 0.023438]\n",
      "430: [D loss: 0.661191, acc: 0.626953]  [A loss: 0.691924, acc: 0.539062]\n",
      "431: [D loss: 0.695366, acc: 0.509766]  [A loss: 1.110424, acc: 0.011719]\n",
      "432: [D loss: 0.656395, acc: 0.638672]  [A loss: 0.732697, acc: 0.406250]\n",
      "433: [D loss: 0.684892, acc: 0.541016]  [A loss: 1.065054, acc: 0.023438]\n",
      "434: [D loss: 0.658363, acc: 0.619141]  [A loss: 0.728603, acc: 0.421875]\n",
      "435: [D loss: 0.687831, acc: 0.533203]  [A loss: 1.043360, acc: 0.015625]\n",
      "436: [D loss: 0.663606, acc: 0.621094]  [A loss: 0.763060, acc: 0.324219]\n",
      "437: [D loss: 0.695946, acc: 0.537109]  [A loss: 1.047927, acc: 0.027344]\n",
      "438: [D loss: 0.659210, acc: 0.630859]  [A loss: 0.763631, acc: 0.378906]\n",
      "439: [D loss: 0.697740, acc: 0.525391]  [A loss: 1.075420, acc: 0.015625]\n",
      "440: [D loss: 0.666007, acc: 0.593750]  [A loss: 0.747501, acc: 0.398438]\n",
      "441: [D loss: 0.692228, acc: 0.542969]  [A loss: 1.043017, acc: 0.019531]\n",
      "442: [D loss: 0.651055, acc: 0.675781]  [A loss: 0.732380, acc: 0.406250]\n",
      "443: [D loss: 0.671274, acc: 0.576172]  [A loss: 0.968173, acc: 0.046875]\n",
      "444: [D loss: 0.643445, acc: 0.658203]  [A loss: 0.838166, acc: 0.203125]\n",
      "445: [D loss: 0.665470, acc: 0.570312]  [A loss: 1.055066, acc: 0.023438]\n",
      "446: [D loss: 0.647326, acc: 0.636719]  [A loss: 0.802523, acc: 0.257812]\n",
      "447: [D loss: 0.694882, acc: 0.574219]  [A loss: 1.105687, acc: 0.015625]\n",
      "448: [D loss: 0.665263, acc: 0.597656]  [A loss: 0.687019, acc: 0.535156]\n",
      "449: [D loss: 0.698095, acc: 0.539062]  [A loss: 1.131533, acc: 0.007812]\n",
      "450: [D loss: 0.654081, acc: 0.626953]  [A loss: 0.738909, acc: 0.386719]\n",
      "451: [D loss: 0.683777, acc: 0.542969]  [A loss: 1.077562, acc: 0.027344]\n",
      "452: [D loss: 0.649294, acc: 0.630859]  [A loss: 0.755264, acc: 0.328125]\n",
      "453: [D loss: 0.690324, acc: 0.544922]  [A loss: 1.070213, acc: 0.015625]\n",
      "454: [D loss: 0.661313, acc: 0.625000]  [A loss: 0.779015, acc: 0.242188]\n",
      "455: [D loss: 0.670161, acc: 0.552734]  [A loss: 1.031293, acc: 0.027344]\n",
      "456: [D loss: 0.662548, acc: 0.585938]  [A loss: 0.804246, acc: 0.277344]\n",
      "457: [D loss: 0.675598, acc: 0.572266]  [A loss: 0.973162, acc: 0.046875]\n",
      "458: [D loss: 0.662761, acc: 0.607422]  [A loss: 0.851549, acc: 0.199219]\n",
      "459: [D loss: 0.659987, acc: 0.582031]  [A loss: 1.007445, acc: 0.058594]\n",
      "460: [D loss: 0.666582, acc: 0.580078]  [A loss: 0.817386, acc: 0.269531]\n",
      "461: [D loss: 0.691185, acc: 0.552734]  [A loss: 1.111425, acc: 0.019531]\n",
      "462: [D loss: 0.651200, acc: 0.628906]  [A loss: 0.806979, acc: 0.292969]\n",
      "463: [D loss: 0.679093, acc: 0.558594]  [A loss: 1.090363, acc: 0.039062]\n",
      "464: [D loss: 0.659618, acc: 0.607422]  [A loss: 0.746745, acc: 0.417969]\n",
      "465: [D loss: 0.675226, acc: 0.550781]  [A loss: 1.065451, acc: 0.031250]\n",
      "466: [D loss: 0.661345, acc: 0.587891]  [A loss: 0.764461, acc: 0.355469]\n",
      "467: [D loss: 0.673564, acc: 0.564453]  [A loss: 1.062904, acc: 0.035156]\n",
      "468: [D loss: 0.637105, acc: 0.650391]  [A loss: 0.763582, acc: 0.375000]\n",
      "469: [D loss: 0.710959, acc: 0.533203]  [A loss: 1.134903, acc: 0.015625]\n",
      "470: [D loss: 0.657726, acc: 0.613281]  [A loss: 0.735447, acc: 0.406250]\n",
      "471: [D loss: 0.690787, acc: 0.548828]  [A loss: 1.048770, acc: 0.039062]\n",
      "472: [D loss: 0.656774, acc: 0.626953]  [A loss: 0.719467, acc: 0.472656]\n",
      "473: [D loss: 0.681223, acc: 0.568359]  [A loss: 1.031863, acc: 0.050781]\n",
      "474: [D loss: 0.656199, acc: 0.625000]  [A loss: 0.769297, acc: 0.332031]\n",
      "475: [D loss: 0.694259, acc: 0.562500]  [A loss: 1.099022, acc: 0.007812]\n",
      "476: [D loss: 0.653159, acc: 0.648438]  [A loss: 0.751101, acc: 0.390625]\n",
      "477: [D loss: 0.697046, acc: 0.544922]  [A loss: 1.104829, acc: 0.007812]\n",
      "478: [D loss: 0.653770, acc: 0.621094]  [A loss: 0.703779, acc: 0.492188]\n",
      "479: [D loss: 0.699398, acc: 0.541016]  [A loss: 1.091094, acc: 0.031250]\n",
      "480: [D loss: 0.643627, acc: 0.630859]  [A loss: 0.750132, acc: 0.386719]\n",
      "481: [D loss: 0.683969, acc: 0.570312]  [A loss: 0.990822, acc: 0.074219]\n",
      "482: [D loss: 0.657554, acc: 0.628906]  [A loss: 0.819016, acc: 0.234375]\n",
      "483: [D loss: 0.652885, acc: 0.599609]  [A loss: 0.993213, acc: 0.046875]\n",
      "484: [D loss: 0.682699, acc: 0.558594]  [A loss: 0.927023, acc: 0.140625]\n",
      "485: [D loss: 0.673062, acc: 0.593750]  [A loss: 0.938735, acc: 0.117188]\n",
      "486: [D loss: 0.658500, acc: 0.607422]  [A loss: 0.804271, acc: 0.265625]\n",
      "487: [D loss: 0.669798, acc: 0.556641]  [A loss: 1.075958, acc: 0.031250]\n",
      "488: [D loss: 0.647990, acc: 0.634766]  [A loss: 0.752428, acc: 0.406250]\n",
      "489: [D loss: 0.671811, acc: 0.566406]  [A loss: 1.138546, acc: 0.011719]\n",
      "490: [D loss: 0.655838, acc: 0.611328]  [A loss: 0.719714, acc: 0.476562]\n",
      "491: [D loss: 0.704133, acc: 0.554688]  [A loss: 1.139520, acc: 0.011719]\n",
      "492: [D loss: 0.644214, acc: 0.650391]  [A loss: 0.739966, acc: 0.406250]\n",
      "493: [D loss: 0.693243, acc: 0.539062]  [A loss: 1.090569, acc: 0.023438]\n",
      "494: [D loss: 0.646387, acc: 0.621094]  [A loss: 0.759733, acc: 0.371094]\n",
      "495: [D loss: 0.686822, acc: 0.568359]  [A loss: 1.035983, acc: 0.031250]\n",
      "496: [D loss: 0.656790, acc: 0.630859]  [A loss: 0.781338, acc: 0.316406]\n",
      "497: [D loss: 0.669713, acc: 0.583984]  [A loss: 0.990976, acc: 0.058594]\n",
      "498: [D loss: 0.649442, acc: 0.623047]  [A loss: 0.802689, acc: 0.292969]\n",
      "499: [D loss: 0.671246, acc: 0.583984]  [A loss: 1.088018, acc: 0.050781]\n",
      "500: [D loss: 0.653215, acc: 0.611328]  [A loss: 0.750297, acc: 0.394531]\n",
      "501: [D loss: 0.695429, acc: 0.548828]  [A loss: 1.106840, acc: 0.039062]\n",
      "502: [D loss: 0.657248, acc: 0.611328]  [A loss: 0.769372, acc: 0.351562]\n",
      "503: [D loss: 0.700380, acc: 0.533203]  [A loss: 1.065189, acc: 0.019531]\n",
      "504: [D loss: 0.661473, acc: 0.615234]  [A loss: 0.755675, acc: 0.390625]\n",
      "505: [D loss: 0.706806, acc: 0.541016]  [A loss: 1.065110, acc: 0.031250]\n",
      "506: [D loss: 0.650169, acc: 0.634766]  [A loss: 0.811068, acc: 0.269531]\n",
      "507: [D loss: 0.666740, acc: 0.564453]  [A loss: 1.047824, acc: 0.035156]\n",
      "508: [D loss: 0.660502, acc: 0.601562]  [A loss: 0.816486, acc: 0.250000]\n",
      "509: [D loss: 0.684776, acc: 0.552734]  [A loss: 0.974968, acc: 0.050781]\n",
      "510: [D loss: 0.659058, acc: 0.597656]  [A loss: 0.839151, acc: 0.230469]\n",
      "511: [D loss: 0.695844, acc: 0.546875]  [A loss: 1.078093, acc: 0.019531]\n",
      "512: [D loss: 0.643348, acc: 0.642578]  [A loss: 0.794434, acc: 0.308594]\n",
      "513: [D loss: 0.660975, acc: 0.597656]  [A loss: 0.999802, acc: 0.078125]\n",
      "514: [D loss: 0.664269, acc: 0.603516]  [A loss: 0.771177, acc: 0.332031]\n",
      "515: [D loss: 0.692145, acc: 0.544922]  [A loss: 1.139437, acc: 0.019531]\n",
      "516: [D loss: 0.660184, acc: 0.615234]  [A loss: 0.739077, acc: 0.417969]\n",
      "517: [D loss: 0.707475, acc: 0.525391]  [A loss: 1.070988, acc: 0.019531]\n",
      "518: [D loss: 0.666998, acc: 0.615234]  [A loss: 0.736835, acc: 0.398438]\n",
      "519: [D loss: 0.683068, acc: 0.572266]  [A loss: 1.068380, acc: 0.015625]\n",
      "520: [D loss: 0.655348, acc: 0.662109]  [A loss: 0.782735, acc: 0.332031]\n",
      "521: [D loss: 0.682345, acc: 0.535156]  [A loss: 1.085103, acc: 0.019531]\n",
      "522: [D loss: 0.653888, acc: 0.615234]  [A loss: 0.782558, acc: 0.277344]\n",
      "523: [D loss: 0.696967, acc: 0.521484]  [A loss: 1.086894, acc: 0.039062]\n",
      "524: [D loss: 0.648830, acc: 0.634766]  [A loss: 0.779919, acc: 0.300781]\n",
      "525: [D loss: 0.688349, acc: 0.542969]  [A loss: 0.987046, acc: 0.062500]\n",
      "526: [D loss: 0.661278, acc: 0.585938]  [A loss: 0.799474, acc: 0.277344]\n",
      "527: [D loss: 0.665862, acc: 0.599609]  [A loss: 0.949758, acc: 0.058594]\n",
      "528: [D loss: 0.656661, acc: 0.611328]  [A loss: 0.835836, acc: 0.250000]\n",
      "529: [D loss: 0.672358, acc: 0.601562]  [A loss: 0.977720, acc: 0.058594]\n",
      "530: [D loss: 0.661440, acc: 0.613281]  [A loss: 0.860458, acc: 0.187500]\n",
      "531: [D loss: 0.681295, acc: 0.566406]  [A loss: 1.060701, acc: 0.050781]\n",
      "532: [D loss: 0.661040, acc: 0.615234]  [A loss: 0.840871, acc: 0.199219]\n",
      "533: [D loss: 0.668150, acc: 0.562500]  [A loss: 0.984161, acc: 0.085938]\n",
      "534: [D loss: 0.667521, acc: 0.599609]  [A loss: 0.856128, acc: 0.207031]\n",
      "535: [D loss: 0.678957, acc: 0.583984]  [A loss: 1.029521, acc: 0.062500]\n",
      "536: [D loss: 0.634086, acc: 0.664062]  [A loss: 0.793559, acc: 0.312500]\n",
      "537: [D loss: 0.690087, acc: 0.531250]  [A loss: 1.106074, acc: 0.031250]\n",
      "538: [D loss: 0.668485, acc: 0.583984]  [A loss: 0.655726, acc: 0.613281]\n",
      "539: [D loss: 0.712131, acc: 0.511719]  [A loss: 1.111950, acc: 0.035156]\n",
      "540: [D loss: 0.661183, acc: 0.603516]  [A loss: 0.737119, acc: 0.433594]\n",
      "541: [D loss: 0.691792, acc: 0.560547]  [A loss: 0.964953, acc: 0.089844]\n",
      "542: [D loss: 0.645294, acc: 0.640625]  [A loss: 0.804500, acc: 0.269531]\n",
      "543: [D loss: 0.677080, acc: 0.564453]  [A loss: 0.998955, acc: 0.066406]\n",
      "544: [D loss: 0.654118, acc: 0.630859]  [A loss: 0.803687, acc: 0.304688]\n",
      "545: [D loss: 0.676170, acc: 0.566406]  [A loss: 0.999140, acc: 0.050781]\n",
      "546: [D loss: 0.662728, acc: 0.587891]  [A loss: 0.828578, acc: 0.246094]\n",
      "547: [D loss: 0.686827, acc: 0.548828]  [A loss: 0.980947, acc: 0.085938]\n",
      "548: [D loss: 0.669204, acc: 0.597656]  [A loss: 0.760554, acc: 0.378906]\n",
      "549: [D loss: 0.686205, acc: 0.552734]  [A loss: 1.049236, acc: 0.042969]\n",
      "550: [D loss: 0.676563, acc: 0.564453]  [A loss: 0.876528, acc: 0.171875]\n",
      "551: [D loss: 0.673797, acc: 0.583984]  [A loss: 0.981714, acc: 0.074219]\n",
      "552: [D loss: 0.644008, acc: 0.650391]  [A loss: 0.900286, acc: 0.183594]\n",
      "553: [D loss: 0.652683, acc: 0.617188]  [A loss: 1.026708, acc: 0.042969]\n",
      "554: [D loss: 0.666858, acc: 0.578125]  [A loss: 0.873226, acc: 0.203125]\n",
      "555: [D loss: 0.675300, acc: 0.583984]  [A loss: 1.058642, acc: 0.042969]\n",
      "556: [D loss: 0.660862, acc: 0.605469]  [A loss: 0.776620, acc: 0.328125]\n",
      "557: [D loss: 0.677285, acc: 0.560547]  [A loss: 1.117873, acc: 0.007812]\n",
      "558: [D loss: 0.669600, acc: 0.580078]  [A loss: 0.806352, acc: 0.277344]\n",
      "559: [D loss: 0.673367, acc: 0.576172]  [A loss: 1.047305, acc: 0.058594]\n",
      "560: [D loss: 0.666446, acc: 0.582031]  [A loss: 0.761160, acc: 0.398438]\n",
      "561: [D loss: 0.681864, acc: 0.552734]  [A loss: 1.072866, acc: 0.039062]\n",
      "562: [D loss: 0.650158, acc: 0.632812]  [A loss: 0.764334, acc: 0.375000]\n",
      "563: [D loss: 0.698164, acc: 0.541016]  [A loss: 1.169466, acc: 0.019531]\n",
      "564: [D loss: 0.673118, acc: 0.556641]  [A loss: 0.743222, acc: 0.390625]\n",
      "565: [D loss: 0.693154, acc: 0.546875]  [A loss: 1.016105, acc: 0.042969]\n",
      "566: [D loss: 0.663859, acc: 0.601562]  [A loss: 0.810679, acc: 0.300781]\n",
      "567: [D loss: 0.699748, acc: 0.537109]  [A loss: 1.012434, acc: 0.082031]\n",
      "568: [D loss: 0.670079, acc: 0.572266]  [A loss: 0.811774, acc: 0.273438]\n",
      "569: [D loss: 0.682262, acc: 0.546875]  [A loss: 1.011919, acc: 0.074219]\n",
      "570: [D loss: 0.655306, acc: 0.619141]  [A loss: 0.780088, acc: 0.363281]\n",
      "571: [D loss: 0.678197, acc: 0.576172]  [A loss: 0.983257, acc: 0.093750]\n",
      "572: [D loss: 0.665132, acc: 0.595703]  [A loss: 0.813521, acc: 0.289062]\n",
      "573: [D loss: 0.674174, acc: 0.587891]  [A loss: 1.027873, acc: 0.027344]\n",
      "574: [D loss: 0.653913, acc: 0.650391]  [A loss: 0.815551, acc: 0.281250]\n",
      "575: [D loss: 0.674153, acc: 0.568359]  [A loss: 1.015461, acc: 0.046875]\n",
      "576: [D loss: 0.662552, acc: 0.582031]  [A loss: 0.854618, acc: 0.152344]\n",
      "577: [D loss: 0.677302, acc: 0.583984]  [A loss: 0.993767, acc: 0.062500]\n",
      "578: [D loss: 0.657666, acc: 0.621094]  [A loss: 0.865819, acc: 0.203125]\n",
      "579: [D loss: 0.672044, acc: 0.580078]  [A loss: 0.979603, acc: 0.097656]\n",
      "580: [D loss: 0.682197, acc: 0.548828]  [A loss: 1.062772, acc: 0.058594]\n",
      "581: [D loss: 0.662400, acc: 0.601562]  [A loss: 0.803412, acc: 0.316406]\n",
      "582: [D loss: 0.683196, acc: 0.568359]  [A loss: 1.052565, acc: 0.035156]\n",
      "583: [D loss: 0.648455, acc: 0.648438]  [A loss: 0.821554, acc: 0.273438]\n",
      "584: [D loss: 0.675044, acc: 0.558594]  [A loss: 1.127543, acc: 0.039062]\n",
      "585: [D loss: 0.653370, acc: 0.603516]  [A loss: 0.717138, acc: 0.445312]\n",
      "586: [D loss: 0.721994, acc: 0.529297]  [A loss: 1.118238, acc: 0.027344]\n",
      "587: [D loss: 0.663451, acc: 0.576172]  [A loss: 0.684649, acc: 0.578125]\n",
      "588: [D loss: 0.697128, acc: 0.558594]  [A loss: 1.052671, acc: 0.054688]\n",
      "589: [D loss: 0.666185, acc: 0.597656]  [A loss: 0.770510, acc: 0.343750]\n",
      "590: [D loss: 0.694471, acc: 0.568359]  [A loss: 1.037218, acc: 0.042969]\n",
      "591: [D loss: 0.653584, acc: 0.642578]  [A loss: 0.771334, acc: 0.324219]\n",
      "592: [D loss: 0.678094, acc: 0.574219]  [A loss: 1.002438, acc: 0.066406]\n",
      "593: [D loss: 0.660714, acc: 0.623047]  [A loss: 0.830860, acc: 0.226562]\n",
      "594: [D loss: 0.667946, acc: 0.593750]  [A loss: 1.025139, acc: 0.050781]\n",
      "595: [D loss: 0.670051, acc: 0.585938]  [A loss: 0.811303, acc: 0.250000]\n",
      "596: [D loss: 0.665592, acc: 0.607422]  [A loss: 0.961061, acc: 0.078125]\n",
      "597: [D loss: 0.662413, acc: 0.613281]  [A loss: 0.797305, acc: 0.324219]\n",
      "598: [D loss: 0.689722, acc: 0.544922]  [A loss: 1.045767, acc: 0.031250]\n",
      "599: [D loss: 0.647835, acc: 0.644531]  [A loss: 0.854667, acc: 0.234375]\n",
      "600: [D loss: 0.678160, acc: 0.539062]  [A loss: 1.063389, acc: 0.035156]\n",
      "601: [D loss: 0.658937, acc: 0.599609]  [A loss: 0.851653, acc: 0.250000]\n",
      "602: [D loss: 0.684375, acc: 0.554688]  [A loss: 1.030001, acc: 0.054688]\n",
      "603: [D loss: 0.668375, acc: 0.589844]  [A loss: 0.790923, acc: 0.300781]\n",
      "604: [D loss: 0.715445, acc: 0.535156]  [A loss: 1.174664, acc: 0.027344]\n",
      "605: [D loss: 0.668879, acc: 0.589844]  [A loss: 0.756455, acc: 0.363281]\n",
      "606: [D loss: 0.682468, acc: 0.554688]  [A loss: 1.003325, acc: 0.058594]\n",
      "607: [D loss: 0.660794, acc: 0.621094]  [A loss: 0.814160, acc: 0.269531]\n",
      "608: [D loss: 0.685162, acc: 0.535156]  [A loss: 1.032055, acc: 0.066406]\n",
      "609: [D loss: 0.659947, acc: 0.609375]  [A loss: 0.832488, acc: 0.261719]\n",
      "610: [D loss: 0.671335, acc: 0.595703]  [A loss: 0.963611, acc: 0.093750]\n",
      "611: [D loss: 0.656912, acc: 0.625000]  [A loss: 0.973009, acc: 0.093750]\n",
      "612: [D loss: 0.663662, acc: 0.615234]  [A loss: 0.841662, acc: 0.242188]\n",
      "613: [D loss: 0.676761, acc: 0.578125]  [A loss: 1.126544, acc: 0.023438]\n",
      "614: [D loss: 0.642013, acc: 0.632812]  [A loss: 0.790805, acc: 0.289062]\n",
      "615: [D loss: 0.716800, acc: 0.525391]  [A loss: 1.132863, acc: 0.031250]\n",
      "616: [D loss: 0.661255, acc: 0.609375]  [A loss: 0.750480, acc: 0.417969]\n",
      "617: [D loss: 0.678305, acc: 0.554688]  [A loss: 1.097367, acc: 0.050781]\n",
      "618: [D loss: 0.662591, acc: 0.623047]  [A loss: 0.724135, acc: 0.457031]\n",
      "619: [D loss: 0.700217, acc: 0.546875]  [A loss: 1.081690, acc: 0.042969]\n",
      "620: [D loss: 0.665722, acc: 0.603516]  [A loss: 0.791318, acc: 0.304688]\n",
      "621: [D loss: 0.707783, acc: 0.533203]  [A loss: 1.034652, acc: 0.046875]\n",
      "622: [D loss: 0.685545, acc: 0.574219]  [A loss: 0.809737, acc: 0.289062]\n",
      "623: [D loss: 0.683951, acc: 0.556641]  [A loss: 1.038040, acc: 0.062500]\n",
      "624: [D loss: 0.646108, acc: 0.623047]  [A loss: 0.846320, acc: 0.218750]\n",
      "625: [D loss: 0.683283, acc: 0.568359]  [A loss: 1.087404, acc: 0.046875]\n",
      "626: [D loss: 0.666775, acc: 0.587891]  [A loss: 0.794332, acc: 0.304688]\n",
      "627: [D loss: 0.689554, acc: 0.564453]  [A loss: 1.011401, acc: 0.066406]\n",
      "628: [D loss: 0.660755, acc: 0.625000]  [A loss: 0.797835, acc: 0.312500]\n",
      "629: [D loss: 0.671939, acc: 0.554688]  [A loss: 0.974067, acc: 0.097656]\n",
      "630: [D loss: 0.656271, acc: 0.630859]  [A loss: 0.880046, acc: 0.187500]\n",
      "631: [D loss: 0.700372, acc: 0.541016]  [A loss: 1.022962, acc: 0.050781]\n",
      "632: [D loss: 0.659714, acc: 0.621094]  [A loss: 0.832426, acc: 0.250000]\n",
      "633: [D loss: 0.679602, acc: 0.578125]  [A loss: 1.037330, acc: 0.039062]\n",
      "634: [D loss: 0.653371, acc: 0.605469]  [A loss: 0.785448, acc: 0.312500]\n",
      "635: [D loss: 0.708052, acc: 0.531250]  [A loss: 1.149909, acc: 0.015625]\n",
      "636: [D loss: 0.664454, acc: 0.595703]  [A loss: 0.728977, acc: 0.457031]\n",
      "637: [D loss: 0.683525, acc: 0.550781]  [A loss: 0.993031, acc: 0.066406]\n",
      "638: [D loss: 0.657138, acc: 0.619141]  [A loss: 0.865841, acc: 0.218750]\n",
      "639: [D loss: 0.676577, acc: 0.574219]  [A loss: 0.985659, acc: 0.078125]\n",
      "640: [D loss: 0.658882, acc: 0.599609]  [A loss: 0.822434, acc: 0.281250]\n",
      "641: [D loss: 0.656930, acc: 0.605469]  [A loss: 0.975810, acc: 0.132812]\n",
      "642: [D loss: 0.657417, acc: 0.619141]  [A loss: 0.864953, acc: 0.167969]\n",
      "643: [D loss: 0.691891, acc: 0.558594]  [A loss: 1.086412, acc: 0.027344]\n",
      "644: [D loss: 0.660628, acc: 0.599609]  [A loss: 0.779943, acc: 0.308594]\n",
      "645: [D loss: 0.692975, acc: 0.568359]  [A loss: 1.067073, acc: 0.074219]\n",
      "646: [D loss: 0.672558, acc: 0.603516]  [A loss: 0.865200, acc: 0.207031]\n",
      "647: [D loss: 0.694632, acc: 0.542969]  [A loss: 1.019958, acc: 0.046875]\n",
      "648: [D loss: 0.662153, acc: 0.597656]  [A loss: 0.881586, acc: 0.167969]\n",
      "649: [D loss: 0.656669, acc: 0.585938]  [A loss: 1.045763, acc: 0.058594]\n",
      "650: [D loss: 0.651582, acc: 0.623047]  [A loss: 0.938548, acc: 0.136719]\n",
      "651: [D loss: 0.667615, acc: 0.587891]  [A loss: 0.992808, acc: 0.046875]\n",
      "652: [D loss: 0.670785, acc: 0.587891]  [A loss: 0.906950, acc: 0.152344]\n",
      "653: [D loss: 0.671809, acc: 0.550781]  [A loss: 0.948700, acc: 0.093750]\n",
      "654: [D loss: 0.666664, acc: 0.589844]  [A loss: 0.931582, acc: 0.164062]\n",
      "655: [D loss: 0.661761, acc: 0.605469]  [A loss: 0.952177, acc: 0.121094]\n",
      "656: [D loss: 0.654765, acc: 0.615234]  [A loss: 0.967625, acc: 0.101562]\n",
      "657: [D loss: 0.665716, acc: 0.619141]  [A loss: 0.999186, acc: 0.089844]\n",
      "658: [D loss: 0.650218, acc: 0.632812]  [A loss: 0.892347, acc: 0.164062]\n",
      "659: [D loss: 0.662465, acc: 0.589844]  [A loss: 1.066557, acc: 0.050781]\n",
      "660: [D loss: 0.669651, acc: 0.585938]  [A loss: 0.818310, acc: 0.304688]\n",
      "661: [D loss: 0.680650, acc: 0.562500]  [A loss: 1.201450, acc: 0.031250]\n",
      "662: [D loss: 0.666777, acc: 0.619141]  [A loss: 0.783925, acc: 0.375000]\n",
      "663: [D loss: 0.687649, acc: 0.552734]  [A loss: 1.194397, acc: 0.031250]\n",
      "664: [D loss: 0.656726, acc: 0.595703]  [A loss: 0.692037, acc: 0.550781]\n",
      "665: [D loss: 0.705341, acc: 0.537109]  [A loss: 1.150506, acc: 0.007812]\n",
      "666: [D loss: 0.660346, acc: 0.591797]  [A loss: 0.765807, acc: 0.394531]\n",
      "667: [D loss: 0.708695, acc: 0.519531]  [A loss: 1.071477, acc: 0.054688]\n",
      "668: [D loss: 0.656847, acc: 0.607422]  [A loss: 0.772572, acc: 0.386719]\n",
      "669: [D loss: 0.709098, acc: 0.539062]  [A loss: 1.034193, acc: 0.062500]\n",
      "670: [D loss: 0.656263, acc: 0.621094]  [A loss: 0.845355, acc: 0.222656]\n",
      "671: [D loss: 0.672602, acc: 0.587891]  [A loss: 0.981963, acc: 0.101562]\n",
      "672: [D loss: 0.645296, acc: 0.644531]  [A loss: 0.850927, acc: 0.246094]\n",
      "673: [D loss: 0.664147, acc: 0.603516]  [A loss: 0.933952, acc: 0.148438]\n",
      "674: [D loss: 0.660683, acc: 0.613281]  [A loss: 0.814730, acc: 0.277344]\n",
      "675: [D loss: 0.685830, acc: 0.552734]  [A loss: 1.032003, acc: 0.109375]\n",
      "676: [D loss: 0.668436, acc: 0.580078]  [A loss: 0.868243, acc: 0.218750]\n",
      "677: [D loss: 0.660356, acc: 0.564453]  [A loss: 0.932850, acc: 0.140625]\n",
      "678: [D loss: 0.646014, acc: 0.617188]  [A loss: 0.942168, acc: 0.121094]\n",
      "679: [D loss: 0.676906, acc: 0.589844]  [A loss: 1.028248, acc: 0.066406]\n",
      "680: [D loss: 0.651595, acc: 0.623047]  [A loss: 0.897527, acc: 0.187500]\n",
      "681: [D loss: 0.683381, acc: 0.585938]  [A loss: 1.047430, acc: 0.066406]\n",
      "682: [D loss: 0.667002, acc: 0.601562]  [A loss: 0.817466, acc: 0.300781]\n",
      "683: [D loss: 0.692357, acc: 0.564453]  [A loss: 1.111608, acc: 0.042969]\n",
      "684: [D loss: 0.663060, acc: 0.603516]  [A loss: 0.747425, acc: 0.414062]\n",
      "685: [D loss: 0.708022, acc: 0.523438]  [A loss: 1.164884, acc: 0.035156]\n",
      "686: [D loss: 0.672152, acc: 0.597656]  [A loss: 0.716281, acc: 0.460938]\n",
      "687: [D loss: 0.711883, acc: 0.542969]  [A loss: 1.143211, acc: 0.015625]\n",
      "688: [D loss: 0.669246, acc: 0.574219]  [A loss: 0.734858, acc: 0.433594]\n",
      "689: [D loss: 0.684924, acc: 0.570312]  [A loss: 1.052715, acc: 0.046875]\n",
      "690: [D loss: 0.674935, acc: 0.574219]  [A loss: 0.840683, acc: 0.296875]\n",
      "691: [D loss: 0.678276, acc: 0.583984]  [A loss: 1.012121, acc: 0.074219]\n",
      "692: [D loss: 0.678616, acc: 0.556641]  [A loss: 0.859941, acc: 0.226562]\n",
      "693: [D loss: 0.683412, acc: 0.595703]  [A loss: 1.084280, acc: 0.078125]\n",
      "694: [D loss: 0.652342, acc: 0.625000]  [A loss: 0.814694, acc: 0.300781]\n",
      "695: [D loss: 0.674771, acc: 0.583984]  [A loss: 1.046627, acc: 0.074219]\n",
      "696: [D loss: 0.657647, acc: 0.597656]  [A loss: 0.798022, acc: 0.273438]\n",
      "697: [D loss: 0.681126, acc: 0.576172]  [A loss: 1.096649, acc: 0.042969]\n",
      "698: [D loss: 0.652104, acc: 0.628906]  [A loss: 0.787081, acc: 0.355469]\n",
      "699: [D loss: 0.674217, acc: 0.560547]  [A loss: 1.106231, acc: 0.066406]\n",
      "700: [D loss: 0.677611, acc: 0.599609]  [A loss: 0.856121, acc: 0.230469]\n",
      "701: [D loss: 0.698065, acc: 0.548828]  [A loss: 1.038605, acc: 0.082031]\n",
      "702: [D loss: 0.677350, acc: 0.562500]  [A loss: 0.768826, acc: 0.375000]\n",
      "703: [D loss: 0.681699, acc: 0.568359]  [A loss: 1.065057, acc: 0.050781]\n",
      "704: [D loss: 0.651013, acc: 0.617188]  [A loss: 0.812115, acc: 0.285156]\n",
      "705: [D loss: 0.687478, acc: 0.554688]  [A loss: 1.072985, acc: 0.054688]\n",
      "706: [D loss: 0.659006, acc: 0.583984]  [A loss: 0.736260, acc: 0.433594]\n",
      "707: [D loss: 0.692695, acc: 0.558594]  [A loss: 1.115867, acc: 0.027344]\n",
      "708: [D loss: 0.669494, acc: 0.595703]  [A loss: 0.731657, acc: 0.433594]\n",
      "709: [D loss: 0.708312, acc: 0.531250]  [A loss: 1.102191, acc: 0.039062]\n",
      "710: [D loss: 0.677144, acc: 0.550781]  [A loss: 0.835299, acc: 0.242188]\n",
      "711: [D loss: 0.674883, acc: 0.574219]  [A loss: 1.005154, acc: 0.082031]\n",
      "712: [D loss: 0.664008, acc: 0.605469]  [A loss: 0.939091, acc: 0.179688]\n",
      "713: [D loss: 0.657818, acc: 0.611328]  [A loss: 0.886015, acc: 0.175781]\n",
      "714: [D loss: 0.667538, acc: 0.593750]  [A loss: 1.007605, acc: 0.074219]\n",
      "715: [D loss: 0.665846, acc: 0.601562]  [A loss: 0.902453, acc: 0.117188]\n",
      "716: [D loss: 0.667808, acc: 0.578125]  [A loss: 1.072987, acc: 0.050781]\n",
      "717: [D loss: 0.664121, acc: 0.625000]  [A loss: 0.776890, acc: 0.324219]\n",
      "718: [D loss: 0.690975, acc: 0.564453]  [A loss: 1.112113, acc: 0.023438]\n",
      "719: [D loss: 0.670033, acc: 0.552734]  [A loss: 0.717258, acc: 0.500000]\n",
      "720: [D loss: 0.681896, acc: 0.580078]  [A loss: 1.058241, acc: 0.050781]\n",
      "721: [D loss: 0.638164, acc: 0.650391]  [A loss: 0.814506, acc: 0.304688]\n",
      "722: [D loss: 0.685208, acc: 0.556641]  [A loss: 1.073964, acc: 0.070312]\n",
      "723: [D loss: 0.658833, acc: 0.587891]  [A loss: 0.796501, acc: 0.332031]\n",
      "724: [D loss: 0.686293, acc: 0.537109]  [A loss: 1.108357, acc: 0.054688]\n",
      "725: [D loss: 0.669820, acc: 0.601562]  [A loss: 0.783368, acc: 0.339844]\n",
      "726: [D loss: 0.664424, acc: 0.599609]  [A loss: 1.055425, acc: 0.066406]\n",
      "727: [D loss: 0.659705, acc: 0.603516]  [A loss: 0.807795, acc: 0.277344]\n",
      "728: [D loss: 0.669130, acc: 0.587891]  [A loss: 0.994246, acc: 0.113281]\n",
      "729: [D loss: 0.652308, acc: 0.587891]  [A loss: 0.864796, acc: 0.218750]\n",
      "730: [D loss: 0.678822, acc: 0.564453]  [A loss: 1.100825, acc: 0.019531]\n",
      "731: [D loss: 0.672802, acc: 0.583984]  [A loss: 0.855936, acc: 0.218750]\n",
      "732: [D loss: 0.676120, acc: 0.574219]  [A loss: 0.997289, acc: 0.078125]\n",
      "733: [D loss: 0.657641, acc: 0.619141]  [A loss: 0.931383, acc: 0.132812]\n",
      "734: [D loss: 0.665597, acc: 0.603516]  [A loss: 0.992717, acc: 0.085938]\n",
      "735: [D loss: 0.667489, acc: 0.558594]  [A loss: 0.882106, acc: 0.222656]\n",
      "736: [D loss: 0.669202, acc: 0.587891]  [A loss: 0.967108, acc: 0.132812]\n",
      "737: [D loss: 0.673022, acc: 0.591797]  [A loss: 0.910692, acc: 0.148438]\n",
      "738: [D loss: 0.669402, acc: 0.552734]  [A loss: 0.995246, acc: 0.113281]\n",
      "739: [D loss: 0.685991, acc: 0.566406]  [A loss: 0.937230, acc: 0.105469]\n",
      "740: [D loss: 0.675168, acc: 0.583984]  [A loss: 1.059037, acc: 0.082031]\n",
      "741: [D loss: 0.663323, acc: 0.593750]  [A loss: 0.884419, acc: 0.199219]\n",
      "742: [D loss: 0.666818, acc: 0.583984]  [A loss: 0.992687, acc: 0.097656]\n",
      "743: [D loss: 0.669253, acc: 0.591797]  [A loss: 0.900767, acc: 0.187500]\n",
      "744: [D loss: 0.679590, acc: 0.582031]  [A loss: 1.206197, acc: 0.027344]\n",
      "745: [D loss: 0.665007, acc: 0.570312]  [A loss: 0.742739, acc: 0.437500]\n",
      "746: [D loss: 0.711629, acc: 0.529297]  [A loss: 1.209816, acc: 0.019531]\n",
      "747: [D loss: 0.671852, acc: 0.595703]  [A loss: 0.679713, acc: 0.585938]\n",
      "748: [D loss: 0.729768, acc: 0.515625]  [A loss: 1.161929, acc: 0.019531]\n",
      "749: [D loss: 0.668254, acc: 0.591797]  [A loss: 0.756165, acc: 0.425781]\n",
      "750: [D loss: 0.702288, acc: 0.546875]  [A loss: 1.059899, acc: 0.062500]\n",
      "751: [D loss: 0.656061, acc: 0.621094]  [A loss: 0.804460, acc: 0.324219]\n",
      "752: [D loss: 0.689958, acc: 0.562500]  [A loss: 1.022919, acc: 0.093750]\n",
      "753: [D loss: 0.675011, acc: 0.587891]  [A loss: 0.865248, acc: 0.214844]\n",
      "754: [D loss: 0.677084, acc: 0.582031]  [A loss: 0.974753, acc: 0.093750]\n",
      "755: [D loss: 0.650736, acc: 0.609375]  [A loss: 0.892565, acc: 0.207031]\n",
      "756: [D loss: 0.666850, acc: 0.582031]  [A loss: 1.084454, acc: 0.066406]\n",
      "757: [D loss: 0.653652, acc: 0.601562]  [A loss: 0.814121, acc: 0.308594]\n",
      "758: [D loss: 0.698756, acc: 0.525391]  [A loss: 1.153224, acc: 0.046875]\n",
      "759: [D loss: 0.654225, acc: 0.611328]  [A loss: 0.771212, acc: 0.394531]\n",
      "760: [D loss: 0.701567, acc: 0.542969]  [A loss: 1.120668, acc: 0.031250]\n",
      "761: [D loss: 0.680343, acc: 0.568359]  [A loss: 0.766719, acc: 0.386719]\n",
      "762: [D loss: 0.710536, acc: 0.542969]  [A loss: 1.097673, acc: 0.054688]\n",
      "763: [D loss: 0.661993, acc: 0.601562]  [A loss: 0.779826, acc: 0.351562]\n",
      "764: [D loss: 0.676510, acc: 0.558594]  [A loss: 1.103602, acc: 0.062500]\n",
      "765: [D loss: 0.685322, acc: 0.546875]  [A loss: 0.844686, acc: 0.273438]\n",
      "766: [D loss: 0.673645, acc: 0.576172]  [A loss: 0.995490, acc: 0.097656]\n",
      "767: [D loss: 0.657045, acc: 0.609375]  [A loss: 0.848230, acc: 0.238281]\n",
      "768: [D loss: 0.686851, acc: 0.560547]  [A loss: 1.050826, acc: 0.082031]\n",
      "769: [D loss: 0.665907, acc: 0.580078]  [A loss: 0.805746, acc: 0.296875]\n",
      "770: [D loss: 0.687580, acc: 0.558594]  [A loss: 1.063419, acc: 0.046875]\n",
      "771: [D loss: 0.668400, acc: 0.589844]  [A loss: 0.790694, acc: 0.363281]\n",
      "772: [D loss: 0.698620, acc: 0.541016]  [A loss: 0.991101, acc: 0.097656]\n",
      "773: [D loss: 0.662478, acc: 0.601562]  [A loss: 0.828863, acc: 0.257812]\n",
      "774: [D loss: 0.698652, acc: 0.554688]  [A loss: 1.095039, acc: 0.066406]\n",
      "775: [D loss: 0.659165, acc: 0.609375]  [A loss: 0.779065, acc: 0.359375]\n",
      "776: [D loss: 0.700199, acc: 0.539062]  [A loss: 1.140874, acc: 0.042969]\n",
      "777: [D loss: 0.670958, acc: 0.582031]  [A loss: 0.742302, acc: 0.449219]\n",
      "778: [D loss: 0.705055, acc: 0.554688]  [A loss: 1.130808, acc: 0.078125]\n",
      "779: [D loss: 0.660377, acc: 0.597656]  [A loss: 0.801339, acc: 0.343750]\n",
      "780: [D loss: 0.705903, acc: 0.550781]  [A loss: 1.013803, acc: 0.078125]\n",
      "781: [D loss: 0.671266, acc: 0.595703]  [A loss: 0.800135, acc: 0.332031]\n",
      "782: [D loss: 0.678944, acc: 0.574219]  [A loss: 1.011423, acc: 0.101562]\n",
      "783: [D loss: 0.670934, acc: 0.572266]  [A loss: 0.953620, acc: 0.136719]\n",
      "784: [D loss: 0.685476, acc: 0.560547]  [A loss: 1.021623, acc: 0.085938]\n",
      "785: [D loss: 0.668264, acc: 0.613281]  [A loss: 0.894071, acc: 0.160156]\n",
      "786: [D loss: 0.656310, acc: 0.591797]  [A loss: 0.901165, acc: 0.210938]\n",
      "787: [D loss: 0.679414, acc: 0.564453]  [A loss: 0.993574, acc: 0.144531]\n",
      "788: [D loss: 0.673108, acc: 0.568359]  [A loss: 0.883051, acc: 0.203125]\n",
      "789: [D loss: 0.667973, acc: 0.582031]  [A loss: 1.105241, acc: 0.054688]\n",
      "790: [D loss: 0.665283, acc: 0.601562]  [A loss: 0.810845, acc: 0.339844]\n",
      "791: [D loss: 0.684521, acc: 0.578125]  [A loss: 1.137373, acc: 0.035156]\n",
      "792: [D loss: 0.675705, acc: 0.562500]  [A loss: 0.886090, acc: 0.187500]\n",
      "793: [D loss: 0.675144, acc: 0.583984]  [A loss: 1.023663, acc: 0.078125]\n",
      "794: [D loss: 0.667636, acc: 0.589844]  [A loss: 0.796711, acc: 0.347656]\n",
      "795: [D loss: 0.678514, acc: 0.583984]  [A loss: 1.142068, acc: 0.062500]\n",
      "796: [D loss: 0.662376, acc: 0.601562]  [A loss: 0.761680, acc: 0.406250]\n",
      "797: [D loss: 0.691664, acc: 0.556641]  [A loss: 1.171521, acc: 0.035156]\n",
      "798: [D loss: 0.665848, acc: 0.621094]  [A loss: 0.779151, acc: 0.390625]\n",
      "799: [D loss: 0.692741, acc: 0.566406]  [A loss: 1.103930, acc: 0.046875]\n",
      "800: [D loss: 0.657042, acc: 0.601562]  [A loss: 0.736583, acc: 0.472656]\n",
      "801: [D loss: 0.720158, acc: 0.521484]  [A loss: 1.219970, acc: 0.015625]\n",
      "802: [D loss: 0.677868, acc: 0.572266]  [A loss: 0.720159, acc: 0.492188]\n",
      "803: [D loss: 0.706829, acc: 0.541016]  [A loss: 1.130411, acc: 0.054688]\n",
      "804: [D loss: 0.662200, acc: 0.615234]  [A loss: 0.729307, acc: 0.457031]\n",
      "805: [D loss: 0.718853, acc: 0.527344]  [A loss: 1.091697, acc: 0.054688]\n",
      "806: [D loss: 0.672825, acc: 0.582031]  [A loss: 0.750953, acc: 0.421875]\n",
      "807: [D loss: 0.691024, acc: 0.542969]  [A loss: 1.000873, acc: 0.105469]\n",
      "808: [D loss: 0.689335, acc: 0.564453]  [A loss: 0.865804, acc: 0.207031]\n",
      "809: [D loss: 0.670053, acc: 0.597656]  [A loss: 0.946063, acc: 0.121094]\n",
      "810: [D loss: 0.675613, acc: 0.578125]  [A loss: 0.890796, acc: 0.203125]\n",
      "811: [D loss: 0.659306, acc: 0.591797]  [A loss: 0.894135, acc: 0.191406]\n",
      "812: [D loss: 0.670259, acc: 0.607422]  [A loss: 1.030607, acc: 0.062500]\n",
      "813: [D loss: 0.673419, acc: 0.552734]  [A loss: 0.814251, acc: 0.300781]\n",
      "814: [D loss: 0.690971, acc: 0.574219]  [A loss: 1.059120, acc: 0.078125]\n",
      "815: [D loss: 0.663829, acc: 0.587891]  [A loss: 0.811996, acc: 0.332031]\n",
      "816: [D loss: 0.679163, acc: 0.568359]  [A loss: 1.015524, acc: 0.082031]\n",
      "817: [D loss: 0.694190, acc: 0.558594]  [A loss: 0.935407, acc: 0.164062]\n",
      "818: [D loss: 0.654116, acc: 0.613281]  [A loss: 0.930652, acc: 0.167969]\n",
      "819: [D loss: 0.679272, acc: 0.541016]  [A loss: 0.983215, acc: 0.128906]\n",
      "820: [D loss: 0.676376, acc: 0.589844]  [A loss: 0.965091, acc: 0.140625]\n",
      "821: [D loss: 0.680009, acc: 0.578125]  [A loss: 0.915524, acc: 0.175781]\n",
      "822: [D loss: 0.681861, acc: 0.574219]  [A loss: 1.029603, acc: 0.085938]\n",
      "823: [D loss: 0.681202, acc: 0.552734]  [A loss: 0.910904, acc: 0.167969]\n",
      "824: [D loss: 0.688934, acc: 0.550781]  [A loss: 1.001278, acc: 0.089844]\n",
      "825: [D loss: 0.648938, acc: 0.617188]  [A loss: 0.850920, acc: 0.242188]\n",
      "826: [D loss: 0.687466, acc: 0.554688]  [A loss: 1.049054, acc: 0.082031]\n",
      "827: [D loss: 0.665365, acc: 0.587891]  [A loss: 0.810254, acc: 0.328125]\n",
      "828: [D loss: 0.682706, acc: 0.558594]  [A loss: 1.159910, acc: 0.015625]\n",
      "829: [D loss: 0.672370, acc: 0.552734]  [A loss: 0.724273, acc: 0.453125]\n",
      "830: [D loss: 0.728075, acc: 0.531250]  [A loss: 1.171996, acc: 0.035156]\n",
      "831: [D loss: 0.707857, acc: 0.515625]  [A loss: 0.739181, acc: 0.421875]\n",
      "832: [D loss: 0.718229, acc: 0.558594]  [A loss: 1.151438, acc: 0.019531]\n",
      "833: [D loss: 0.674452, acc: 0.578125]  [A loss: 0.738142, acc: 0.429688]\n",
      "834: [D loss: 0.725501, acc: 0.519531]  [A loss: 1.082120, acc: 0.039062]\n",
      "835: [D loss: 0.660253, acc: 0.605469]  [A loss: 0.771359, acc: 0.367188]\n",
      "836: [D loss: 0.709918, acc: 0.533203]  [A loss: 0.990071, acc: 0.117188]\n",
      "837: [D loss: 0.664475, acc: 0.603516]  [A loss: 0.882604, acc: 0.164062]\n",
      "838: [D loss: 0.690733, acc: 0.539062]  [A loss: 1.002169, acc: 0.050781]\n",
      "839: [D loss: 0.659075, acc: 0.599609]  [A loss: 0.921699, acc: 0.152344]\n",
      "840: [D loss: 0.664364, acc: 0.589844]  [A loss: 0.941515, acc: 0.132812]\n",
      "841: [D loss: 0.681864, acc: 0.542969]  [A loss: 0.908157, acc: 0.175781]\n",
      "842: [D loss: 0.672116, acc: 0.562500]  [A loss: 0.921090, acc: 0.152344]\n",
      "843: [D loss: 0.659700, acc: 0.607422]  [A loss: 0.945746, acc: 0.160156]\n",
      "844: [D loss: 0.692043, acc: 0.560547]  [A loss: 0.980481, acc: 0.105469]\n",
      "845: [D loss: 0.671275, acc: 0.583984]  [A loss: 0.811760, acc: 0.320312]\n",
      "846: [D loss: 0.687767, acc: 0.556641]  [A loss: 1.153033, acc: 0.027344]\n",
      "847: [D loss: 0.669356, acc: 0.601562]  [A loss: 0.737244, acc: 0.449219]\n",
      "848: [D loss: 0.717828, acc: 0.535156]  [A loss: 1.162466, acc: 0.035156]\n",
      "849: [D loss: 0.663233, acc: 0.580078]  [A loss: 0.748868, acc: 0.417969]\n",
      "850: [D loss: 0.703050, acc: 0.531250]  [A loss: 1.083877, acc: 0.050781]\n",
      "851: [D loss: 0.662287, acc: 0.609375]  [A loss: 0.776531, acc: 0.332031]\n",
      "852: [D loss: 0.673982, acc: 0.585938]  [A loss: 0.976140, acc: 0.125000]\n",
      "853: [D loss: 0.661599, acc: 0.601562]  [A loss: 0.854538, acc: 0.218750]\n",
      "854: [D loss: 0.671189, acc: 0.570312]  [A loss: 0.977543, acc: 0.101562]\n",
      "855: [D loss: 0.670848, acc: 0.599609]  [A loss: 0.889989, acc: 0.179688]\n",
      "856: [D loss: 0.658790, acc: 0.595703]  [A loss: 0.995776, acc: 0.085938]\n",
      "857: [D loss: 0.669497, acc: 0.574219]  [A loss: 0.943772, acc: 0.113281]\n",
      "858: [D loss: 0.677033, acc: 0.554688]  [A loss: 0.985061, acc: 0.109375]\n",
      "859: [D loss: 0.657252, acc: 0.611328]  [A loss: 0.859388, acc: 0.250000]\n",
      "860: [D loss: 0.671225, acc: 0.576172]  [A loss: 1.054146, acc: 0.066406]\n",
      "861: [D loss: 0.651904, acc: 0.630859]  [A loss: 0.743072, acc: 0.453125]\n",
      "862: [D loss: 0.691997, acc: 0.568359]  [A loss: 1.039287, acc: 0.097656]\n",
      "863: [D loss: 0.661644, acc: 0.595703]  [A loss: 0.791171, acc: 0.324219]\n",
      "864: [D loss: 0.686073, acc: 0.572266]  [A loss: 1.114571, acc: 0.054688]\n",
      "865: [D loss: 0.677168, acc: 0.574219]  [A loss: 0.786835, acc: 0.378906]\n",
      "866: [D loss: 0.684941, acc: 0.556641]  [A loss: 1.123977, acc: 0.062500]\n",
      "867: [D loss: 0.674453, acc: 0.580078]  [A loss: 0.757938, acc: 0.433594]\n",
      "868: [D loss: 0.742555, acc: 0.511719]  [A loss: 1.195357, acc: 0.011719]\n",
      "869: [D loss: 0.690920, acc: 0.554688]  [A loss: 0.792788, acc: 0.359375]\n",
      "870: [D loss: 0.681660, acc: 0.585938]  [A loss: 1.047792, acc: 0.074219]\n",
      "871: [D loss: 0.672814, acc: 0.564453]  [A loss: 0.829513, acc: 0.304688]\n",
      "872: [D loss: 0.693899, acc: 0.531250]  [A loss: 1.079570, acc: 0.066406]\n",
      "873: [D loss: 0.660513, acc: 0.599609]  [A loss: 0.786695, acc: 0.378906]\n",
      "874: [D loss: 0.698936, acc: 0.550781]  [A loss: 1.103276, acc: 0.066406]\n",
      "875: [D loss: 0.664659, acc: 0.605469]  [A loss: 0.707302, acc: 0.515625]\n",
      "876: [D loss: 0.707311, acc: 0.535156]  [A loss: 1.080797, acc: 0.078125]\n",
      "877: [D loss: 0.660617, acc: 0.585938]  [A loss: 0.787998, acc: 0.324219]\n",
      "878: [D loss: 0.702816, acc: 0.560547]  [A loss: 1.022330, acc: 0.101562]\n",
      "879: [D loss: 0.662335, acc: 0.591797]  [A loss: 0.775715, acc: 0.390625]\n",
      "880: [D loss: 0.687130, acc: 0.562500]  [A loss: 1.043054, acc: 0.089844]\n",
      "881: [D loss: 0.673800, acc: 0.576172]  [A loss: 0.787651, acc: 0.371094]\n",
      "882: [D loss: 0.690812, acc: 0.566406]  [A loss: 1.040003, acc: 0.078125]\n",
      "883: [D loss: 0.658945, acc: 0.607422]  [A loss: 0.827083, acc: 0.300781]\n",
      "884: [D loss: 0.662484, acc: 0.617188]  [A loss: 1.035881, acc: 0.101562]\n",
      "885: [D loss: 0.685820, acc: 0.560547]  [A loss: 0.963751, acc: 0.140625]\n",
      "886: [D loss: 0.684643, acc: 0.544922]  [A loss: 0.973589, acc: 0.128906]\n",
      "887: [D loss: 0.664881, acc: 0.585938]  [A loss: 0.908616, acc: 0.160156]\n",
      "888: [D loss: 0.673164, acc: 0.599609]  [A loss: 1.031321, acc: 0.085938]\n",
      "889: [D loss: 0.648002, acc: 0.636719]  [A loss: 0.900213, acc: 0.230469]\n",
      "890: [D loss: 0.697645, acc: 0.570312]  [A loss: 1.109279, acc: 0.066406]\n",
      "891: [D loss: 0.694812, acc: 0.541016]  [A loss: 0.801604, acc: 0.332031]\n",
      "892: [D loss: 0.696486, acc: 0.529297]  [A loss: 1.148311, acc: 0.054688]\n",
      "893: [D loss: 0.665085, acc: 0.570312]  [A loss: 0.803850, acc: 0.292969]\n",
      "894: [D loss: 0.688561, acc: 0.582031]  [A loss: 1.134678, acc: 0.046875]\n",
      "895: [D loss: 0.663629, acc: 0.560547]  [A loss: 0.768040, acc: 0.378906]\n",
      "896: [D loss: 0.672970, acc: 0.591797]  [A loss: 1.194654, acc: 0.042969]\n",
      "897: [D loss: 0.670553, acc: 0.578125]  [A loss: 0.717279, acc: 0.503906]\n",
      "898: [D loss: 0.718652, acc: 0.539062]  [A loss: 1.175327, acc: 0.035156]\n",
      "899: [D loss: 0.672084, acc: 0.568359]  [A loss: 0.726929, acc: 0.460938]\n",
      "900: [D loss: 0.718041, acc: 0.519531]  [A loss: 1.032690, acc: 0.066406]\n",
      "901: [D loss: 0.658079, acc: 0.619141]  [A loss: 0.815440, acc: 0.316406]\n",
      "902: [D loss: 0.690595, acc: 0.546875]  [A loss: 1.064711, acc: 0.082031]\n",
      "903: [D loss: 0.676082, acc: 0.585938]  [A loss: 0.761084, acc: 0.378906]\n",
      "904: [D loss: 0.689964, acc: 0.541016]  [A loss: 1.059432, acc: 0.070312]\n",
      "905: [D loss: 0.660892, acc: 0.609375]  [A loss: 0.834870, acc: 0.265625]\n",
      "906: [D loss: 0.676149, acc: 0.585938]  [A loss: 1.034852, acc: 0.074219]\n",
      "907: [D loss: 0.666813, acc: 0.582031]  [A loss: 0.827183, acc: 0.285156]\n",
      "908: [D loss: 0.726396, acc: 0.498047]  [A loss: 1.140350, acc: 0.066406]\n",
      "909: [D loss: 0.672901, acc: 0.593750]  [A loss: 0.778819, acc: 0.386719]\n",
      "910: [D loss: 0.702097, acc: 0.531250]  [A loss: 1.088934, acc: 0.035156]\n",
      "911: [D loss: 0.657996, acc: 0.599609]  [A loss: 0.766901, acc: 0.382812]\n",
      "912: [D loss: 0.700871, acc: 0.535156]  [A loss: 1.076752, acc: 0.046875]\n",
      "913: [D loss: 0.673182, acc: 0.578125]  [A loss: 0.832683, acc: 0.292969]\n",
      "914: [D loss: 0.672830, acc: 0.568359]  [A loss: 1.050213, acc: 0.082031]\n",
      "915: [D loss: 0.668348, acc: 0.595703]  [A loss: 0.815881, acc: 0.308594]\n",
      "916: [D loss: 0.669795, acc: 0.580078]  [A loss: 1.066489, acc: 0.070312]\n",
      "917: [D loss: 0.661769, acc: 0.587891]  [A loss: 0.812283, acc: 0.320312]\n",
      "918: [D loss: 0.700610, acc: 0.558594]  [A loss: 1.074224, acc: 0.050781]\n",
      "919: [D loss: 0.672972, acc: 0.562500]  [A loss: 0.849803, acc: 0.273438]\n",
      "920: [D loss: 0.679512, acc: 0.564453]  [A loss: 1.011586, acc: 0.085938]\n",
      "921: [D loss: 0.670407, acc: 0.570312]  [A loss: 0.838273, acc: 0.230469]\n",
      "922: [D loss: 0.697068, acc: 0.564453]  [A loss: 1.201462, acc: 0.039062]\n",
      "923: [D loss: 0.684661, acc: 0.541016]  [A loss: 0.704303, acc: 0.515625]\n",
      "924: [D loss: 0.686816, acc: 0.548828]  [A loss: 1.102772, acc: 0.050781]\n",
      "925: [D loss: 0.681931, acc: 0.544922]  [A loss: 0.794263, acc: 0.328125]\n",
      "926: [D loss: 0.688529, acc: 0.554688]  [A loss: 1.009432, acc: 0.089844]\n",
      "927: [D loss: 0.660550, acc: 0.587891]  [A loss: 0.803205, acc: 0.292969]\n",
      "928: [D loss: 0.683266, acc: 0.580078]  [A loss: 0.980154, acc: 0.136719]\n",
      "929: [D loss: 0.658481, acc: 0.611328]  [A loss: 0.820335, acc: 0.308594]\n",
      "930: [D loss: 0.676512, acc: 0.585938]  [A loss: 1.053255, acc: 0.070312]\n",
      "931: [D loss: 0.655798, acc: 0.593750]  [A loss: 0.837363, acc: 0.277344]\n",
      "932: [D loss: 0.682204, acc: 0.566406]  [A loss: 1.117726, acc: 0.070312]\n",
      "933: [D loss: 0.688287, acc: 0.548828]  [A loss: 0.820961, acc: 0.300781]\n",
      "934: [D loss: 0.663448, acc: 0.582031]  [A loss: 1.079464, acc: 0.089844]\n",
      "935: [D loss: 0.669066, acc: 0.597656]  [A loss: 0.778717, acc: 0.351562]\n",
      "936: [D loss: 0.703560, acc: 0.523438]  [A loss: 1.092254, acc: 0.058594]\n",
      "937: [D loss: 0.668600, acc: 0.576172]  [A loss: 0.765367, acc: 0.378906]\n",
      "938: [D loss: 0.680091, acc: 0.574219]  [A loss: 1.109987, acc: 0.054688]\n",
      "939: [D loss: 0.670928, acc: 0.585938]  [A loss: 0.790607, acc: 0.324219]\n",
      "940: [D loss: 0.701163, acc: 0.542969]  [A loss: 1.142120, acc: 0.035156]\n",
      "941: [D loss: 0.683224, acc: 0.578125]  [A loss: 0.682245, acc: 0.570312]\n",
      "942: [D loss: 0.712504, acc: 0.527344]  [A loss: 1.120712, acc: 0.046875]\n",
      "943: [D loss: 0.659021, acc: 0.621094]  [A loss: 0.819973, acc: 0.296875]\n",
      "944: [D loss: 0.676946, acc: 0.558594]  [A loss: 0.993363, acc: 0.074219]\n",
      "945: [D loss: 0.666642, acc: 0.609375]  [A loss: 0.885616, acc: 0.222656]\n",
      "946: [D loss: 0.679259, acc: 0.576172]  [A loss: 0.902121, acc: 0.167969]\n",
      "947: [D loss: 0.685255, acc: 0.546875]  [A loss: 0.920223, acc: 0.152344]\n",
      "948: [D loss: 0.675661, acc: 0.578125]  [A loss: 0.985172, acc: 0.140625]\n",
      "949: [D loss: 0.676664, acc: 0.562500]  [A loss: 0.943714, acc: 0.160156]\n",
      "950: [D loss: 0.657474, acc: 0.595703]  [A loss: 0.916698, acc: 0.175781]\n",
      "951: [D loss: 0.662182, acc: 0.601562]  [A loss: 0.922091, acc: 0.160156]\n",
      "952: [D loss: 0.662173, acc: 0.630859]  [A loss: 1.003272, acc: 0.097656]\n",
      "953: [D loss: 0.660020, acc: 0.611328]  [A loss: 0.849699, acc: 0.265625]\n",
      "954: [D loss: 0.697290, acc: 0.556641]  [A loss: 1.087182, acc: 0.027344]\n",
      "955: [D loss: 0.670921, acc: 0.597656]  [A loss: 0.802256, acc: 0.304688]\n",
      "956: [D loss: 0.706352, acc: 0.550781]  [A loss: 1.214424, acc: 0.054688]\n",
      "957: [D loss: 0.673775, acc: 0.544922]  [A loss: 0.707542, acc: 0.515625]\n",
      "958: [D loss: 0.740465, acc: 0.505859]  [A loss: 1.242247, acc: 0.023438]\n",
      "959: [D loss: 0.687431, acc: 0.554688]  [A loss: 0.703567, acc: 0.503906]\n",
      "960: [D loss: 0.704560, acc: 0.533203]  [A loss: 1.148789, acc: 0.035156]\n",
      "961: [D loss: 0.662885, acc: 0.578125]  [A loss: 0.788384, acc: 0.308594]\n",
      "962: [D loss: 0.681360, acc: 0.585938]  [A loss: 0.997868, acc: 0.113281]\n",
      "963: [D loss: 0.659090, acc: 0.591797]  [A loss: 0.913499, acc: 0.207031]\n",
      "964: [D loss: 0.658315, acc: 0.583984]  [A loss: 0.900051, acc: 0.195312]\n",
      "965: [D loss: 0.658938, acc: 0.585938]  [A loss: 0.911162, acc: 0.179688]\n",
      "966: [D loss: 0.702897, acc: 0.546875]  [A loss: 0.942380, acc: 0.156250]\n",
      "967: [D loss: 0.666848, acc: 0.587891]  [A loss: 0.971441, acc: 0.152344]\n",
      "968: [D loss: 0.663906, acc: 0.591797]  [A loss: 0.928640, acc: 0.195312]\n",
      "969: [D loss: 0.670927, acc: 0.595703]  [A loss: 1.074904, acc: 0.070312]\n",
      "970: [D loss: 0.683802, acc: 0.574219]  [A loss: 0.807787, acc: 0.332031]\n",
      "971: [D loss: 0.676691, acc: 0.550781]  [A loss: 1.095149, acc: 0.085938]\n",
      "972: [D loss: 0.681574, acc: 0.560547]  [A loss: 0.790223, acc: 0.375000]\n",
      "973: [D loss: 0.683905, acc: 0.560547]  [A loss: 1.212204, acc: 0.027344]\n",
      "974: [D loss: 0.680821, acc: 0.562500]  [A loss: 0.679789, acc: 0.601562]\n",
      "975: [D loss: 0.723236, acc: 0.541016]  [A loss: 1.223842, acc: 0.027344]\n",
      "976: [D loss: 0.667875, acc: 0.589844]  [A loss: 0.698743, acc: 0.515625]\n",
      "977: [D loss: 0.671793, acc: 0.568359]  [A loss: 1.042003, acc: 0.109375]\n",
      "978: [D loss: 0.660711, acc: 0.582031]  [A loss: 0.771055, acc: 0.421875]\n",
      "979: [D loss: 0.698721, acc: 0.552734]  [A loss: 1.005712, acc: 0.093750]\n",
      "980: [D loss: 0.666837, acc: 0.587891]  [A loss: 0.811322, acc: 0.339844]\n",
      "981: [D loss: 0.674906, acc: 0.585938]  [A loss: 1.054718, acc: 0.093750]\n",
      "982: [D loss: 0.663510, acc: 0.603516]  [A loss: 0.769244, acc: 0.410156]\n",
      "983: [D loss: 0.683381, acc: 0.566406]  [A loss: 0.972377, acc: 0.132812]\n",
      "984: [D loss: 0.680747, acc: 0.548828]  [A loss: 0.843787, acc: 0.277344]\n",
      "985: [D loss: 0.686755, acc: 0.556641]  [A loss: 1.106200, acc: 0.070312]\n",
      "986: [D loss: 0.676254, acc: 0.576172]  [A loss: 0.746421, acc: 0.425781]\n",
      "987: [D loss: 0.761045, acc: 0.513672]  [A loss: 1.151636, acc: 0.031250]\n",
      "988: [D loss: 0.679432, acc: 0.552734]  [A loss: 0.799270, acc: 0.324219]\n",
      "989: [D loss: 0.692277, acc: 0.539062]  [A loss: 1.051008, acc: 0.089844]\n",
      "990: [D loss: 0.691926, acc: 0.541016]  [A loss: 0.838795, acc: 0.289062]\n",
      "991: [D loss: 0.665493, acc: 0.585938]  [A loss: 0.971084, acc: 0.152344]\n",
      "992: [D loss: 0.666270, acc: 0.595703]  [A loss: 0.987659, acc: 0.121094]\n",
      "993: [D loss: 0.688511, acc: 0.562500]  [A loss: 0.884498, acc: 0.207031]\n",
      "994: [D loss: 0.665750, acc: 0.603516]  [A loss: 0.938727, acc: 0.171875]\n",
      "995: [D loss: 0.663920, acc: 0.591797]  [A loss: 0.933049, acc: 0.210938]\n",
      "996: [D loss: 0.660084, acc: 0.576172]  [A loss: 0.947861, acc: 0.148438]\n",
      "997: [D loss: 0.674252, acc: 0.578125]  [A loss: 0.968362, acc: 0.113281]\n",
      "998: [D loss: 0.672835, acc: 0.574219]  [A loss: 0.904124, acc: 0.140625]\n",
      "999: [D loss: 0.676508, acc: 0.599609]  [A loss: 1.026021, acc: 0.097656]\n"
     ]
    }
   ],
   "source": [
    "train_steps=1000\n",
    "batch_size=256\n",
    "save_interval=100\n",
    "\n",
    "def plot_images(save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "    filename = 'mnist.png'\n",
    "    if fake:\n",
    "        if noise is None:\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "        else:\n",
    "            filename = \"mnist_%d.png\" % step\n",
    "        images = generator.predict(noise)\n",
    "    else:\n",
    "        i = np.random.randint(0, x_train.shape[0], samples)\n",
    "        images = x_train[i, :, :, :]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [28,28])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save2file:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "noise_input = None\n",
    "if save_interval>0:\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "    \n",
    "a_loss = np.zeros((train_steps,2))\n",
    "d_loss = np.zeros((train_steps,2))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    images_train = x_train[np.random.randint(0,x_train.shape[0], size=batch_size), :, :, :]\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "    images_fake = generator.predict(noise)\n",
    "    x = np.concatenate((images_train, images_fake))\n",
    "    y = np.ones([2*batch_size, 1])\n",
    "    y[batch_size:, :] = 0\n",
    "    discriminator.trainable = True\n",
    "    #discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy']) # slows down, why?\n",
    "    d_loss[i] = discriminator.train_on_batch(x, y)\n",
    "    discriminator.trainable = False\n",
    "    #adversarial.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy']) # slows down, why?\n",
    "    y = np.ones([batch_size, 1])\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "    a_loss[i] = adversarial.train_on_batch(noise, y)\n",
    "    log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[i,0], d_loss[i,1])\n",
    "    log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[i,0], a_loss[i,1])\n",
    "    print(log_mesg)\n",
    "    if save_interval>0:\n",
    "        if (i+1)%save_interval==0:\n",
    "            plot_images(save2file=True, samples=noise_input.shape[0],noise=noise_input, step=(i+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x306dd5dd8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title('Discriminator network')\n",
    "plt.plot(d_loss[:,:])\n",
    "plt.legend(discriminator.metrics_names)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Adversarial network')\n",
    "plt.plot(a_loss[:,:])\n",
    "plt.legend(adversarial.metrics_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    # (WF+2P)/S+1\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "        # In: 28 x 28 x 1, depth = 1\n",
    "        # Out: 14 x 14 x 1, depth=64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.4\n",
    "        depth = 64+64+64+64\n",
    "        dim = 7\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        (x_train, Y_train), (x_test, Y_test) = mnist.load_data()\n",
    "        x_train = x_train.reshape(60000,28,28,1)\n",
    "        x_test = x_test.reshape(10000, 28,28,1)\n",
    "        self.x_train = x_train.astype('float32')\n",
    "        self.x_test = x_test.astype('float32')\n",
    "        # normalize\n",
    "        #\n",
    "        self.x_train /= 255\n",
    "        self.x_test /= 255\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = np_utils.to_categorical(Y_train, 10)\n",
    "        self.y_test = np_utils.to_categorical(Y_test, 10)\n",
    "        \n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "            self.discriminator.trainable = False\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mnist_dcgan = MNIST_DCGAN()\n",
    "    timer = ElapsedTimer()\n",
    "    mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=50)\n",
    "    timer.elapsed_time()\n",
    "    mnist_dcgan.plot_images(fake=True)\n",
    "    mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/GAN_mnist.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "- GAN architecture not set in stone. Key idea: use \"discriminator\" to generate self-supervised training examples and use backpropagation to train the \"generator\" to trick the discriminator. \n",
    "- Other example: derive generator structure to generate camouflage patterns. Use detector to detect object. Backpropagation will change generator to minimize detection.\n",
    "- Games: generator to generate move, discriminator to decide whether move was good\n",
    "- Controllers: generator to generate controller \"phenotype\", discriminator to decide whether controller is good\n",
    "- Fine balance between improving generator and discriminator is needed and the approach does not necessarily need to succeed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
