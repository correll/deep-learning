{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "We have so far exclusively treated neural networks as classifiers and regressors, but neglected the high dimensional information that the network is creating. In a nutshell, why just classifying images and not just creating some? Can we run a neural network the other way round to create images, speech, text or whatever else we normally just feed in? \n",
    "\n",
    "Convolution reduces spatial information (2D images or 1D time series) to components by means of filters. The filters have usually been trained based on a large number of examples, describing features that these examples have in common. The result of the convolutional layers is then a representation that describes how much of each feature has been in the original image. This process could also be used the other way round to generate images using \"deconvolution\". Instead of reducing high-dimensional information such as an image to low-dimensional one, such as a distribution over classes, such networks could be generate high-dimensional images from a few numbers that define the desired content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling and \"Deconvolving\"\n",
    "The label \"deconvolution\" is unfortunately confusing here, however, as it has established <a href=\"https://en.wikipedia.org/wiki/Deconvolution\">mathematical meaning</a>, which is usually not implemented in a neural network context. Rather, we can achieve the desired effect by first upsampling the input and performing a convolution then. In this example, a simple (2,2) image will be turned into a (4,4) image. What exactly happens is random, as the initial weights are random, and running the model below a couple of times will give the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1.1149725  2.103928   2.229945   0.25203347]\n",
      " [2.9510407  3.9399962  4.066013   0.43501008]\n",
      " [3.3449173  4.3338733  4.45989    0.50406694]\n",
      " [0.5908152  0.75322515 0.7877536  0.13811374]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15b67df98>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code inspired by https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import UpSampling2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define input data\n",
    "X = asarray([[1, 2],\n",
    "            [3, 4]])\n",
    "# show input data for context\n",
    "print(X)\n",
    "# reshape input data into one sample a sample with a channel\n",
    "X = X.reshape((1, 2, 2, 1))\n",
    "model = Sequential()\n",
    "model.add(UpSampling2D(input_shape=(2, 2, 1)))\n",
    "model.add(Conv2D(1, (2,2), padding='same'))\n",
    "#model.summary()\n",
    "yhat = model.predict(X)\n",
    "# reshape output to remove channel to make printing easier\n",
    "yhat = yhat.reshape((4, 4))\n",
    "# summarize output\n",
    "print(yhat)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X.reshape((2,2)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a function to combine convolution and upsampling <code>Conv2DTranspose</code> that pads new rows and columns during upsampling with zeros. This can be seen when using (1,1) as a convolution kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[ 0.35152036  0.6204863   0.7030407   1.2409726 ]\n",
      " [-0.35024494 -0.4251445  -0.7004899  -0.850289  ]\n",
      " [ 1.0545611   1.861459    1.4060814   2.4819453 ]\n",
      " [-1.0507348  -1.2754335  -1.4009798  -1.700578  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15f261390>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code inspired by https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/\n",
    "\n",
    "# example of using the transpose convolutional layer\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2DTranspose\n",
    "# define input data\n",
    "X = asarray([[1, 2],\n",
    "            [3, 4]])\n",
    "# show input data for context\n",
    "print(X)\n",
    "# reshape input data into one sample a sample with a channel\n",
    "X = X.reshape((1, 2, 2, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "# Use different kernels (1,1), (2,2) etc to see how Conv2DTranspose operates\n",
    "model.add(Conv2DTranspose(1, (2,2), strides=(2,2), input_shape=(2, 2, 1)))\n",
    "#model.summary()\n",
    "# make a prediction with the model\n",
    "yhat = model.predict(X)\n",
    "# reshape output to remove channel to make printing easier\n",
    "yhat = yhat.reshape((4, 4))\n",
    "# summarize output\n",
    "print(yhat)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X.reshape((2,2)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the right kernels to get meaningful output\n",
    "\n",
    "Lets assume a training set consists of single pixel images, which color is drawn from a 3D normal distribution centered around the color orange. The \"generator\" G() does not know this distribution, but turns uniform random input drawn from a distribution z into other random colors. The \"discriminator\" has learned the original distribution and can say \"hot\" (yes) or \"cold\" (no) and anything in between. The network can now use this feedback to improve the generator to better match the true distribution. Once gradient information is available, the generator can change its weights to get closer and closer to the desired distribution. This is known as backpropagation. \n",
    "\n",
    "At the same time, we can use knowledge of the fact that generated images are fake to improve the discriminator. Instead of training the discriminator with samples of shades of orange and totally random colors, we are training it with whatever the generator currently produces as examples of \"fake\" data. This will let the discriminator become more subtle over time.\n",
    "\n",
    "Training both generator and discriminator sounds like a chicken-egg problem, but is usually bootstrapped by implementing a generating network that is believed to have a sufficiently complex structure to create a variety of output that we are interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "A generator is using upsampling and convolution operations to turn a seed of noise into an image. Initially, the generated images are just noise, but will eventually show similar distributions as the training set. The generator itself is never trained as a stand-alone network, which is indicated by all parameters colored in light gray. Once this network is trained, the generator gets better and better, here showing the output after zero and 2000 training iterations, which will be explained further below. \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_generator.svg\" width=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "We use a discriminator to decide whether an image is following the desired distribution, i.e. like the training set, or a generated image. We can train this discriminator by using two batches of equal size, one of which contains training images, the other generated images. The generated images are labeled by a zero, the training images by a one.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_discriminator.svg\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "As the generator gets better, the discriminator will get presented with better and better generated images, becoming more and more sophisticated. Here, the training set at 0 and after 2000 iterations of training the generator are shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Generator and Discriminator into an Adversial Network\n",
    "A GAN consists of a generator and a discriminator network that are connected in series. When training the GAN, the generator pretends that images it generates are real images, which leads to a loss if the discriminator detects the fake image. In order to prevent this loss from backpropagating into the discriminator, all discriminator parameters are locked during training, which is indicated by all parameters of the discriminator shown in light gray.  \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_adversarial.svg\" width=\"75%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GAN\n",
    "\n",
    "The implementation below has been inspired by <a href=\"https://github.com/roatienza/Deep-Learning-Experiments\">Rowel Atienza's code</a>, and adapted to Keras 2.0. The architecture below follows the guidelines from <a href=\"https://arxiv.org/pdf/1511.06434.pdf%C3%AF%C2%BC%E2%80%B0\">UNSUPERVISED REPRESENTATION LEARNING\n",
    "WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</a>\n",
    "\n",
    "- Architecture guidelines for stable Deep Convolutional GANs\n",
    "- Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
    "- Use batchnorm in both the generator and the discriminator.\n",
    "- Remove fully connected hidden layers for deeper architectures.\n",
    "- Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "- Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100 # 100 gives good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential()\n",
    "depth = 64\n",
    "dropout = 0.4\n",
    "# In: 28 x 28 x 1, depth = 1\n",
    "# Out: 14 x 14 x 1, depth=64\n",
    "input_shape = (28, 28, 1)\n",
    "discriminator.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "# Out: 1-dim probability\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1))\n",
    "discriminator.add(Activation('sigmoid'))\n",
    "discriminator.summary()\n",
    "\n",
    "optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 12544)             25088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,152,385\n",
      "Trainable params: 1,126,849\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Sequential()\n",
    "dropout = 0.4\n",
    "depth = 64+64+64+64\n",
    "dim = 7\n",
    "# In: 100\n",
    "# Out: dim x dim x depth\n",
    "generator.add(Dense(dim*dim*depth, input_dim=noise_dim))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(Reshape((dim, dim, depth)))\n",
    "generator.add(Dropout(dropout))\n",
    "\n",
    "# In: dim x dim x depth\n",
    "# Out: 2*dim x 2*dim x depth/2\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "generator.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "# Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "generator.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "generator.add(Activation('sigmoid'))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_4 (Sequential)    (None, 28, 28, 1)         1152385   \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 1)                 4311553   \n",
      "=================================================================\n",
      "Total params: 5,463,938\n",
      "Trainable params: 5,438,402\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "adversarial = Sequential()\n",
    "adversarial.add(generator)\n",
    "adversarial.add(discriminator)\n",
    "adversarial.summary()\n",
    "adversarial.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, Y_train), (x_test, Y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(10000, 28,28,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalize\n",
    "#\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(Y_train, 10)\n",
    "y_test = np_utils.to_categorical(Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.030305, acc: 0.990234]  [A loss: 0.034259, acc: 0.984375]\n",
      "1: [D loss: 0.030716, acc: 0.992188]  [A loss: 0.022632, acc: 1.000000]\n",
      "2: [D loss: 0.015224, acc: 0.998047]  [A loss: 0.025762, acc: 0.996094]\n",
      "3: [D loss: 0.014097, acc: 0.998047]  [A loss: 0.021311, acc: 0.996094]\n",
      "4: [D loss: 0.016068, acc: 1.000000]  [A loss: 0.065851, acc: 0.980469]\n",
      "5: [D loss: 0.030780, acc: 1.000000]  [A loss: 4.214302, acc: 0.015625]\n",
      "6: [D loss: 0.054648, acc: 0.994141]  [A loss: 1.058146, acc: 0.492188]\n",
      "7: [D loss: 0.008915, acc: 1.000000]  [A loss: 2.120664, acc: 0.816406]\n",
      "8: [D loss: 0.119705, acc: 0.966797]  [A loss: 11.647167, acc: 0.000000]\n",
      "9: [D loss: 0.382366, acc: 0.835938]  [A loss: 0.057759, acc: 0.968750]\n",
      "10: [D loss: 0.340357, acc: 0.810547]  [A loss: 0.666597, acc: 0.699219]\n",
      "11: [D loss: 0.031309, acc: 0.996094]  [A loss: 0.267225, acc: 0.886719]\n",
      "12: [D loss: 0.036863, acc: 0.998047]  [A loss: 0.147826, acc: 0.929688]\n",
      "13: [D loss: 0.035295, acc: 1.000000]  [A loss: 0.192481, acc: 0.925781]\n",
      "14: [D loss: 0.045289, acc: 0.998047]  [A loss: 0.355685, acc: 0.886719]\n",
      "15: [D loss: 0.082155, acc: 0.994141]  [A loss: 1.803452, acc: 0.074219]\n",
      "16: [D loss: 0.163767, acc: 0.914062]  [A loss: 5.500586, acc: 0.000000]\n",
      "17: [D loss: 0.046247, acc: 0.996094]  [A loss: 0.579700, acc: 0.730469]\n",
      "18: [D loss: 0.106824, acc: 0.968750]  [A loss: 5.021767, acc: 0.000000]\n",
      "19: [D loss: 0.068574, acc: 0.996094]  [A loss: 3.036944, acc: 0.000000]\n",
      "20: [D loss: 0.239800, acc: 0.869141]  [A loss: 10.482661, acc: 0.000000]\n",
      "21: [D loss: 0.428990, acc: 0.794922]  [A loss: 0.052298, acc: 0.968750]\n",
      "22: [D loss: 0.804033, acc: 0.699219]  [A loss: 4.119586, acc: 0.000000]\n",
      "23: [D loss: 0.078443, acc: 0.998047]  [A loss: 1.344290, acc: 0.320312]\n",
      "24: [D loss: 0.232965, acc: 0.896484]  [A loss: 3.824170, acc: 0.000000]\n",
      "25: [D loss: 0.076485, acc: 0.996094]  [A loss: 1.899482, acc: 0.140625]\n",
      "26: [D loss: 0.296114, acc: 0.843750]  [A loss: 6.277008, acc: 0.000000]\n",
      "27: [D loss: 0.126424, acc: 0.970703]  [A loss: 0.961771, acc: 0.535156]\n",
      "28: [D loss: 0.396053, acc: 0.785156]  [A loss: 6.990832, acc: 0.000000]\n",
      "29: [D loss: 0.194714, acc: 0.941406]  [A loss: 1.242144, acc: 0.347656]\n",
      "30: [D loss: 0.402188, acc: 0.794922]  [A loss: 5.750227, acc: 0.000000]\n",
      "31: [D loss: 0.125289, acc: 0.980469]  [A loss: 2.127373, acc: 0.070312]\n",
      "32: [D loss: 0.312273, acc: 0.849609]  [A loss: 5.031425, acc: 0.000000]\n",
      "33: [D loss: 0.110440, acc: 0.992188]  [A loss: 2.885396, acc: 0.015625]\n",
      "34: [D loss: 0.224236, acc: 0.873047]  [A loss: 4.584815, acc: 0.000000]\n",
      "35: [D loss: 0.122355, acc: 0.978516]  [A loss: 2.823454, acc: 0.039062]\n",
      "36: [D loss: 0.205960, acc: 0.882812]  [A loss: 4.595187, acc: 0.000000]\n",
      "37: [D loss: 0.128957, acc: 0.972656]  [A loss: 2.497777, acc: 0.062500]\n",
      "38: [D loss: 0.296112, acc: 0.843750]  [A loss: 5.786422, acc: 0.000000]\n",
      "39: [D loss: 0.189087, acc: 0.945312]  [A loss: 1.787139, acc: 0.085938]\n",
      "40: [D loss: 0.325858, acc: 0.855469]  [A loss: 5.069756, acc: 0.000000]\n",
      "41: [D loss: 0.175386, acc: 0.953125]  [A loss: 2.074365, acc: 0.007812]\n",
      "42: [D loss: 0.339495, acc: 0.841797]  [A loss: 4.730138, acc: 0.000000]\n",
      "43: [D loss: 0.128720, acc: 0.970703]  [A loss: 2.341983, acc: 0.003906]\n",
      "44: [D loss: 0.211540, acc: 0.888672]  [A loss: 3.887271, acc: 0.000000]\n",
      "45: [D loss: 0.133291, acc: 0.972656]  [A loss: 2.173273, acc: 0.003906]\n",
      "46: [D loss: 0.305967, acc: 0.800781]  [A loss: 4.827969, acc: 0.000000]\n",
      "47: [D loss: 0.247364, acc: 0.925781]  [A loss: 1.474019, acc: 0.007812]\n",
      "48: [D loss: 0.513490, acc: 0.699219]  [A loss: 4.792368, acc: 0.000000]\n",
      "49: [D loss: 0.270862, acc: 0.892578]  [A loss: 1.587063, acc: 0.007812]\n",
      "50: [D loss: 0.358916, acc: 0.771484]  [A loss: 3.336997, acc: 0.000000]\n",
      "51: [D loss: 0.167697, acc: 0.964844]  [A loss: 1.960745, acc: 0.000000]\n",
      "52: [D loss: 0.261807, acc: 0.853516]  [A loss: 3.018357, acc: 0.000000]\n",
      "53: [D loss: 0.218590, acc: 0.968750]  [A loss: 2.284849, acc: 0.000000]\n",
      "54: [D loss: 0.305737, acc: 0.818359]  [A loss: 3.050604, acc: 0.000000]\n",
      "55: [D loss: 0.304696, acc: 0.912109]  [A loss: 1.875060, acc: 0.000000]\n",
      "56: [D loss: 0.494415, acc: 0.650391]  [A loss: 3.660257, acc: 0.000000]\n",
      "57: [D loss: 0.462934, acc: 0.812500]  [A loss: 0.977120, acc: 0.144531]\n",
      "58: [D loss: 0.726965, acc: 0.621094]  [A loss: 2.528446, acc: 0.000000]\n",
      "59: [D loss: 0.369750, acc: 0.898438]  [A loss: 1.366250, acc: 0.000000]\n",
      "60: [D loss: 0.455629, acc: 0.660156]  [A loss: 2.161611, acc: 0.000000]\n",
      "61: [D loss: 0.389884, acc: 0.888672]  [A loss: 1.684059, acc: 0.000000]\n",
      "62: [D loss: 0.422044, acc: 0.740234]  [A loss: 2.054939, acc: 0.000000]\n",
      "63: [D loss: 0.366777, acc: 0.871094]  [A loss: 1.769200, acc: 0.000000]\n",
      "64: [D loss: 0.409707, acc: 0.757812]  [A loss: 2.266137, acc: 0.000000]\n",
      "65: [D loss: 0.366204, acc: 0.904297]  [A loss: 1.746038, acc: 0.000000]\n",
      "66: [D loss: 0.387090, acc: 0.748047]  [A loss: 2.395160, acc: 0.000000]\n",
      "67: [D loss: 0.395725, acc: 0.896484]  [A loss: 1.464730, acc: 0.003906]\n",
      "68: [D loss: 0.512611, acc: 0.615234]  [A loss: 3.034239, acc: 0.000000]\n",
      "69: [D loss: 0.412513, acc: 0.853516]  [A loss: 0.880835, acc: 0.269531]\n",
      "70: [D loss: 0.659381, acc: 0.576172]  [A loss: 2.416529, acc: 0.000000]\n",
      "71: [D loss: 0.342235, acc: 0.939453]  [A loss: 1.244106, acc: 0.019531]\n",
      "72: [D loss: 0.466646, acc: 0.644531]  [A loss: 2.169414, acc: 0.000000]\n",
      "73: [D loss: 0.331259, acc: 0.917969]  [A loss: 1.570353, acc: 0.003906]\n",
      "74: [D loss: 0.456016, acc: 0.714844]  [A loss: 2.370103, acc: 0.000000]\n",
      "75: [D loss: 0.358889, acc: 0.921875]  [A loss: 1.398062, acc: 0.000000]\n",
      "76: [D loss: 0.512193, acc: 0.642578]  [A loss: 2.766205, acc: 0.000000]\n",
      "77: [D loss: 0.424918, acc: 0.849609]  [A loss: 0.814783, acc: 0.429688]\n",
      "78: [D loss: 0.731948, acc: 0.576172]  [A loss: 2.399173, acc: 0.000000]\n",
      "79: [D loss: 0.437697, acc: 0.886719]  [A loss: 0.948355, acc: 0.128906]\n",
      "80: [D loss: 0.583360, acc: 0.576172]  [A loss: 1.895805, acc: 0.000000]\n",
      "81: [D loss: 0.427057, acc: 0.906250]  [A loss: 1.170558, acc: 0.023438]\n",
      "82: [D loss: 0.530625, acc: 0.576172]  [A loss: 2.081662, acc: 0.000000]\n",
      "83: [D loss: 0.424124, acc: 0.896484]  [A loss: 1.269203, acc: 0.000000]\n",
      "84: [D loss: 0.557034, acc: 0.583984]  [A loss: 2.379068, acc: 0.000000]\n",
      "85: [D loss: 0.444275, acc: 0.904297]  [A loss: 1.085422, acc: 0.050781]\n",
      "86: [D loss: 0.633990, acc: 0.570312]  [A loss: 2.203909, acc: 0.000000]\n",
      "87: [D loss: 0.488540, acc: 0.890625]  [A loss: 0.983658, acc: 0.054688]\n",
      "88: [D loss: 0.633697, acc: 0.560547]  [A loss: 1.891776, acc: 0.000000]\n",
      "89: [D loss: 0.510012, acc: 0.787109]  [A loss: 1.111284, acc: 0.035156]\n",
      "90: [D loss: 0.600882, acc: 0.578125]  [A loss: 1.767822, acc: 0.000000]\n",
      "91: [D loss: 0.501143, acc: 0.794922]  [A loss: 1.071960, acc: 0.074219]\n",
      "92: [D loss: 0.625377, acc: 0.568359]  [A loss: 1.889272, acc: 0.000000]\n",
      "93: [D loss: 0.547423, acc: 0.751953]  [A loss: 0.834147, acc: 0.449219]\n",
      "94: [D loss: 0.665809, acc: 0.533203]  [A loss: 1.805616, acc: 0.000000]\n",
      "95: [D loss: 0.519876, acc: 0.796875]  [A loss: 0.865772, acc: 0.355469]\n",
      "96: [D loss: 0.627094, acc: 0.574219]  [A loss: 1.701111, acc: 0.000000]\n",
      "97: [D loss: 0.500741, acc: 0.833984]  [A loss: 0.979304, acc: 0.148438]\n",
      "98: [D loss: 0.602686, acc: 0.566406]  [A loss: 1.851061, acc: 0.000000]\n",
      "99: [D loss: 0.510167, acc: 0.810547]  [A loss: 1.018467, acc: 0.062500]\n",
      "100: [D loss: 0.606423, acc: 0.587891]  [A loss: 2.030688, acc: 0.000000]\n",
      "101: [D loss: 0.503641, acc: 0.876953]  [A loss: 0.816359, acc: 0.292969]\n",
      "102: [D loss: 0.718312, acc: 0.574219]  [A loss: 2.012773, acc: 0.000000]\n",
      "103: [D loss: 0.523160, acc: 0.824219]  [A loss: 0.732265, acc: 0.546875]\n",
      "104: [D loss: 0.661699, acc: 0.558594]  [A loss: 1.421837, acc: 0.000000]\n",
      "105: [D loss: 0.525614, acc: 0.761719]  [A loss: 0.961615, acc: 0.070312]\n",
      "106: [D loss: 0.584441, acc: 0.583984]  [A loss: 1.572457, acc: 0.000000]\n",
      "107: [D loss: 0.506678, acc: 0.802734]  [A loss: 0.991613, acc: 0.054688]\n",
      "108: [D loss: 0.622206, acc: 0.574219]  [A loss: 1.995737, acc: 0.000000]\n",
      "109: [D loss: 0.531373, acc: 0.828125]  [A loss: 0.687793, acc: 0.632812]\n",
      "110: [D loss: 0.694833, acc: 0.558594]  [A loss: 1.601491, acc: 0.000000]\n",
      "111: [D loss: 0.510589, acc: 0.896484]  [A loss: 0.799832, acc: 0.296875]\n",
      "112: [D loss: 0.579414, acc: 0.566406]  [A loss: 1.385732, acc: 0.000000]\n",
      "113: [D loss: 0.474101, acc: 0.835938]  [A loss: 1.005973, acc: 0.023438]\n",
      "114: [D loss: 0.564195, acc: 0.568359]  [A loss: 1.770094, acc: 0.000000]\n",
      "115: [D loss: 0.459903, acc: 0.933594]  [A loss: 0.852070, acc: 0.191406]\n",
      "116: [D loss: 0.680261, acc: 0.542969]  [A loss: 2.240152, acc: 0.000000]\n",
      "117: [D loss: 0.540434, acc: 0.759766]  [A loss: 0.672905, acc: 0.667969]\n",
      "118: [D loss: 0.633266, acc: 0.552734]  [A loss: 1.331100, acc: 0.000000]\n",
      "119: [D loss: 0.497383, acc: 0.787109]  [A loss: 1.082742, acc: 0.019531]\n",
      "120: [D loss: 0.560875, acc: 0.578125]  [A loss: 1.509160, acc: 0.000000]\n",
      "121: [D loss: 0.489215, acc: 0.777344]  [A loss: 1.229493, acc: 0.003906]\n",
      "122: [D loss: 0.526160, acc: 0.625000]  [A loss: 1.899471, acc: 0.000000]\n",
      "123: [D loss: 0.489235, acc: 0.876953]  [A loss: 0.951720, acc: 0.089844]\n",
      "124: [D loss: 0.703700, acc: 0.566406]  [A loss: 2.417902, acc: 0.000000]\n",
      "125: [D loss: 0.608506, acc: 0.609375]  [A loss: 0.580183, acc: 0.828125]\n",
      "126: [D loss: 0.723209, acc: 0.535156]  [A loss: 1.199332, acc: 0.000000]\n",
      "127: [D loss: 0.509306, acc: 0.773438]  [A loss: 0.999139, acc: 0.035156]\n",
      "128: [D loss: 0.557159, acc: 0.580078]  [A loss: 1.185996, acc: 0.003906]\n",
      "129: [D loss: 0.526742, acc: 0.636719]  [A loss: 1.230266, acc: 0.003906]\n",
      "130: [D loss: 0.515545, acc: 0.619141]  [A loss: 1.542350, acc: 0.000000]\n",
      "131: [D loss: 0.488709, acc: 0.783203]  [A loss: 1.249663, acc: 0.000000]\n",
      "132: [D loss: 0.563517, acc: 0.593750]  [A loss: 1.873624, acc: 0.000000]\n",
      "133: [D loss: 0.496496, acc: 0.859375]  [A loss: 0.838411, acc: 0.312500]\n",
      "134: [D loss: 0.786178, acc: 0.544922]  [A loss: 2.203855, acc: 0.000000]\n",
      "135: [D loss: 0.608709, acc: 0.625000]  [A loss: 0.714964, acc: 0.550781]\n",
      "136: [D loss: 0.649990, acc: 0.535156]  [A loss: 1.067263, acc: 0.000000]\n",
      "137: [D loss: 0.556694, acc: 0.640625]  [A loss: 1.004579, acc: 0.046875]\n",
      "138: [D loss: 0.584430, acc: 0.597656]  [A loss: 1.164654, acc: 0.000000]\n",
      "139: [D loss: 0.601163, acc: 0.593750]  [A loss: 1.212801, acc: 0.003906]\n",
      "140: [D loss: 0.592196, acc: 0.599609]  [A loss: 1.200486, acc: 0.003906]\n",
      "141: [D loss: 0.592926, acc: 0.605469]  [A loss: 1.231271, acc: 0.000000]\n",
      "142: [D loss: 0.590081, acc: 0.613281]  [A loss: 1.286690, acc: 0.003906]\n",
      "143: [D loss: 0.581503, acc: 0.638672]  [A loss: 1.373450, acc: 0.000000]\n",
      "144: [D loss: 0.554615, acc: 0.681641]  [A loss: 1.125074, acc: 0.003906]\n",
      "145: [D loss: 0.636498, acc: 0.568359]  [A loss: 1.936003, acc: 0.000000]\n",
      "146: [D loss: 0.582844, acc: 0.736328]  [A loss: 0.691323, acc: 0.671875]\n",
      "147: [D loss: 0.748051, acc: 0.523438]  [A loss: 1.677544, acc: 0.000000]\n",
      "148: [D loss: 0.585375, acc: 0.755859]  [A loss: 0.756098, acc: 0.425781]\n",
      "149: [D loss: 0.636581, acc: 0.544922]  [A loss: 1.197864, acc: 0.000000]\n",
      "150: [D loss: 0.547411, acc: 0.689453]  [A loss: 1.062029, acc: 0.023438]\n",
      "151: [D loss: 0.588076, acc: 0.574219]  [A loss: 1.535663, acc: 0.000000]\n",
      "152: [D loss: 0.532073, acc: 0.824219]  [A loss: 0.960898, acc: 0.082031]\n",
      "153: [D loss: 0.671194, acc: 0.552734]  [A loss: 2.227931, acc: 0.000000]\n",
      "154: [D loss: 0.584312, acc: 0.654297]  [A loss: 0.673595, acc: 0.667969]\n",
      "155: [D loss: 0.709446, acc: 0.525391]  [A loss: 1.379962, acc: 0.000000]\n",
      "156: [D loss: 0.549371, acc: 0.845703]  [A loss: 0.906583, acc: 0.097656]\n",
      "157: [D loss: 0.632093, acc: 0.542969]  [A loss: 1.530891, acc: 0.000000]\n",
      "158: [D loss: 0.531286, acc: 0.824219]  [A loss: 0.977431, acc: 0.039062]\n",
      "159: [D loss: 0.670470, acc: 0.562500]  [A loss: 1.924472, acc: 0.000000]\n",
      "160: [D loss: 0.555480, acc: 0.773438]  [A loss: 0.803209, acc: 0.289062]\n",
      "161: [D loss: 0.663318, acc: 0.542969]  [A loss: 1.639283, acc: 0.000000]\n",
      "162: [D loss: 0.528200, acc: 0.863281]  [A loss: 0.883419, acc: 0.113281]\n",
      "163: [D loss: 0.592748, acc: 0.541016]  [A loss: 1.567245, acc: 0.000000]\n",
      "164: [D loss: 0.492493, acc: 0.916016]  [A loss: 1.036629, acc: 0.015625]\n",
      "165: [D loss: 0.599353, acc: 0.576172]  [A loss: 1.862561, acc: 0.000000]\n",
      "166: [D loss: 0.510469, acc: 0.859375]  [A loss: 0.812131, acc: 0.281250]\n",
      "167: [D loss: 0.714642, acc: 0.535156]  [A loss: 1.929175, acc: 0.000000]\n",
      "168: [D loss: 0.551431, acc: 0.751953]  [A loss: 0.772117, acc: 0.378906]\n",
      "169: [D loss: 0.654747, acc: 0.523438]  [A loss: 1.475393, acc: 0.000000]\n",
      "170: [D loss: 0.537231, acc: 0.816406]  [A loss: 0.980796, acc: 0.062500]\n",
      "171: [D loss: 0.644737, acc: 0.562500]  [A loss: 1.601623, acc: 0.000000]\n",
      "172: [D loss: 0.554231, acc: 0.777344]  [A loss: 0.927971, acc: 0.093750]\n",
      "173: [D loss: 0.736683, acc: 0.546875]  [A loss: 1.757287, acc: 0.000000]\n",
      "174: [D loss: 0.618927, acc: 0.685547]  [A loss: 0.719990, acc: 0.484375]\n",
      "175: [D loss: 0.709266, acc: 0.529297]  [A loss: 1.337253, acc: 0.000000]\n",
      "176: [D loss: 0.579917, acc: 0.728516]  [A loss: 0.853752, acc: 0.210938]\n",
      "177: [D loss: 0.635662, acc: 0.570312]  [A loss: 1.302912, acc: 0.000000]\n",
      "178: [D loss: 0.584091, acc: 0.683594]  [A loss: 1.048112, acc: 0.019531]\n",
      "179: [D loss: 0.605014, acc: 0.583984]  [A loss: 1.421070, acc: 0.000000]\n",
      "180: [D loss: 0.548935, acc: 0.759766]  [A loss: 1.026283, acc: 0.027344]\n",
      "181: [D loss: 0.597871, acc: 0.593750]  [A loss: 1.680967, acc: 0.000000]\n",
      "182: [D loss: 0.514171, acc: 0.857422]  [A loss: 0.891915, acc: 0.144531]\n",
      "183: [D loss: 0.653726, acc: 0.546875]  [A loss: 1.907314, acc: 0.000000]\n",
      "184: [D loss: 0.533383, acc: 0.818359]  [A loss: 0.813164, acc: 0.269531]\n",
      "185: [D loss: 0.639837, acc: 0.525391]  [A loss: 1.667057, acc: 0.000000]\n",
      "186: [D loss: 0.537987, acc: 0.816406]  [A loss: 0.905386, acc: 0.191406]\n",
      "187: [D loss: 0.648466, acc: 0.558594]  [A loss: 1.658545, acc: 0.000000]\n",
      "188: [D loss: 0.563553, acc: 0.771484]  [A loss: 0.878414, acc: 0.144531]\n",
      "189: [D loss: 0.649334, acc: 0.535156]  [A loss: 1.621122, acc: 0.000000]\n",
      "190: [D loss: 0.567370, acc: 0.769531]  [A loss: 0.886012, acc: 0.179688]\n",
      "191: [D loss: 0.640530, acc: 0.539062]  [A loss: 1.589503, acc: 0.000000]\n",
      "192: [D loss: 0.591998, acc: 0.734375]  [A loss: 0.882516, acc: 0.199219]\n",
      "193: [D loss: 0.665274, acc: 0.529297]  [A loss: 1.756406, acc: 0.000000]\n",
      "194: [D loss: 0.578013, acc: 0.742188]  [A loss: 0.791808, acc: 0.343750]\n",
      "195: [D loss: 0.727945, acc: 0.531250]  [A loss: 1.865243, acc: 0.000000]\n",
      "196: [D loss: 0.618047, acc: 0.689453]  [A loss: 0.697718, acc: 0.535156]\n",
      "197: [D loss: 0.709914, acc: 0.500000]  [A loss: 1.590634, acc: 0.000000]\n",
      "198: [D loss: 0.568815, acc: 0.783203]  [A loss: 0.916531, acc: 0.101562]\n",
      "199: [D loss: 0.622751, acc: 0.546875]  [A loss: 1.748970, acc: 0.000000]\n",
      "200: [D loss: 0.546402, acc: 0.763672]  [A loss: 1.066715, acc: 0.039062]\n",
      "201: [D loss: 0.640589, acc: 0.537109]  [A loss: 2.024312, acc: 0.000000]\n",
      "202: [D loss: 0.519868, acc: 0.810547]  [A loss: 0.902781, acc: 0.148438]\n",
      "203: [D loss: 0.699683, acc: 0.531250]  [A loss: 2.090132, acc: 0.000000]\n",
      "204: [D loss: 0.552270, acc: 0.755859]  [A loss: 0.798223, acc: 0.371094]\n",
      "205: [D loss: 0.749266, acc: 0.515625]  [A loss: 1.766613, acc: 0.000000]\n",
      "206: [D loss: 0.569750, acc: 0.767578]  [A loss: 0.855782, acc: 0.187500]\n",
      "207: [D loss: 0.732118, acc: 0.548828]  [A loss: 1.483656, acc: 0.000000]\n",
      "208: [D loss: 0.610050, acc: 0.679688]  [A loss: 0.985703, acc: 0.074219]\n",
      "209: [D loss: 0.671724, acc: 0.595703]  [A loss: 1.353070, acc: 0.000000]\n",
      "210: [D loss: 0.641631, acc: 0.611328]  [A loss: 1.054176, acc: 0.027344]\n",
      "211: [D loss: 0.646014, acc: 0.587891]  [A loss: 1.310196, acc: 0.000000]\n",
      "212: [D loss: 0.608572, acc: 0.658203]  [A loss: 1.137519, acc: 0.007812]\n",
      "213: [D loss: 0.621963, acc: 0.621094]  [A loss: 1.453833, acc: 0.000000]\n",
      "214: [D loss: 0.591328, acc: 0.736328]  [A loss: 1.064865, acc: 0.042969]\n",
      "215: [D loss: 0.617170, acc: 0.574219]  [A loss: 1.731033, acc: 0.000000]\n",
      "216: [D loss: 0.582588, acc: 0.763672]  [A loss: 0.896466, acc: 0.152344]\n",
      "217: [D loss: 0.649297, acc: 0.531250]  [A loss: 1.987129, acc: 0.000000]\n",
      "218: [D loss: 0.591262, acc: 0.710938]  [A loss: 0.548798, acc: 0.863281]\n",
      "219: [D loss: 0.776683, acc: 0.500000]  [A loss: 1.903996, acc: 0.000000]\n",
      "220: [D loss: 0.598632, acc: 0.675781]  [A loss: 0.647721, acc: 0.648438]\n",
      "221: [D loss: 0.737474, acc: 0.500000]  [A loss: 1.627060, acc: 0.000000]\n",
      "222: [D loss: 0.588679, acc: 0.765625]  [A loss: 0.930585, acc: 0.117188]\n",
      "223: [D loss: 0.666082, acc: 0.527344]  [A loss: 1.655777, acc: 0.000000]\n",
      "224: [D loss: 0.610259, acc: 0.705078]  [A loss: 0.882236, acc: 0.164062]\n",
      "225: [D loss: 0.704532, acc: 0.525391]  [A loss: 1.692352, acc: 0.000000]\n",
      "226: [D loss: 0.598063, acc: 0.726562]  [A loss: 0.771445, acc: 0.406250]\n",
      "227: [D loss: 0.705829, acc: 0.533203]  [A loss: 1.673691, acc: 0.000000]\n",
      "228: [D loss: 0.595833, acc: 0.746094]  [A loss: 0.780134, acc: 0.351562]\n",
      "229: [D loss: 0.705627, acc: 0.507812]  [A loss: 1.554520, acc: 0.000000]\n",
      "230: [D loss: 0.601739, acc: 0.740234]  [A loss: 0.821022, acc: 0.281250]\n",
      "231: [D loss: 0.694989, acc: 0.521484]  [A loss: 1.698553, acc: 0.000000]\n",
      "232: [D loss: 0.611946, acc: 0.691406]  [A loss: 0.830273, acc: 0.253906]\n",
      "233: [D loss: 0.686367, acc: 0.554688]  [A loss: 1.629018, acc: 0.000000]\n",
      "234: [D loss: 0.607478, acc: 0.718750]  [A loss: 0.802211, acc: 0.296875]\n",
      "235: [D loss: 0.684260, acc: 0.544922]  [A loss: 1.724317, acc: 0.000000]\n",
      "236: [D loss: 0.593096, acc: 0.716797]  [A loss: 0.870443, acc: 0.242188]\n",
      "237: [D loss: 0.684780, acc: 0.535156]  [A loss: 1.734380, acc: 0.000000]\n",
      "238: [D loss: 0.596779, acc: 0.722656]  [A loss: 0.795174, acc: 0.347656]\n",
      "239: [D loss: 0.672716, acc: 0.535156]  [A loss: 1.648840, acc: 0.000000]\n",
      "240: [D loss: 0.579651, acc: 0.744141]  [A loss: 0.793884, acc: 0.332031]\n",
      "241: [D loss: 0.715985, acc: 0.542969]  [A loss: 1.579924, acc: 0.000000]\n",
      "242: [D loss: 0.600052, acc: 0.720703]  [A loss: 0.760640, acc: 0.402344]\n",
      "243: [D loss: 0.710048, acc: 0.539062]  [A loss: 1.529446, acc: 0.000000]\n",
      "244: [D loss: 0.623833, acc: 0.646484]  [A loss: 0.798527, acc: 0.328125]\n",
      "245: [D loss: 0.738612, acc: 0.548828]  [A loss: 1.576877, acc: 0.000000]\n",
      "246: [D loss: 0.639193, acc: 0.615234]  [A loss: 0.894740, acc: 0.164062]\n",
      "247: [D loss: 0.714706, acc: 0.548828]  [A loss: 1.387112, acc: 0.000000]\n",
      "248: [D loss: 0.657023, acc: 0.619141]  [A loss: 0.929409, acc: 0.132812]\n",
      "249: [D loss: 0.696600, acc: 0.541016]  [A loss: 1.401345, acc: 0.000000]\n",
      "250: [D loss: 0.645400, acc: 0.630859]  [A loss: 0.931041, acc: 0.101562]\n",
      "251: [D loss: 0.676060, acc: 0.546875]  [A loss: 1.499723, acc: 0.000000]\n",
      "252: [D loss: 0.630280, acc: 0.640625]  [A loss: 0.887044, acc: 0.195312]\n",
      "253: [D loss: 0.691208, acc: 0.541016]  [A loss: 1.719974, acc: 0.000000]\n",
      "254: [D loss: 0.618817, acc: 0.671875]  [A loss: 0.775834, acc: 0.414062]\n",
      "255: [D loss: 0.711835, acc: 0.527344]  [A loss: 1.745764, acc: 0.000000]\n",
      "256: [D loss: 0.608840, acc: 0.685547]  [A loss: 0.752316, acc: 0.425781]\n",
      "257: [D loss: 0.685180, acc: 0.525391]  [A loss: 1.517585, acc: 0.000000]\n",
      "258: [D loss: 0.606962, acc: 0.703125]  [A loss: 0.848908, acc: 0.230469]\n",
      "259: [D loss: 0.684606, acc: 0.539062]  [A loss: 1.509142, acc: 0.000000]\n",
      "260: [D loss: 0.602612, acc: 0.699219]  [A loss: 0.986405, acc: 0.093750]\n",
      "261: [D loss: 0.643281, acc: 0.572266]  [A loss: 1.522770, acc: 0.003906]\n",
      "262: [D loss: 0.608575, acc: 0.671875]  [A loss: 1.020679, acc: 0.109375]\n",
      "263: [D loss: 0.671395, acc: 0.556641]  [A loss: 1.590293, acc: 0.000000]\n",
      "264: [D loss: 0.601584, acc: 0.720703]  [A loss: 0.891881, acc: 0.167969]\n",
      "265: [D loss: 0.661134, acc: 0.546875]  [A loss: 1.627218, acc: 0.000000]\n",
      "266: [D loss: 0.593498, acc: 0.748047]  [A loss: 0.804957, acc: 0.304688]\n",
      "267: [D loss: 0.687764, acc: 0.535156]  [A loss: 1.707005, acc: 0.000000]\n",
      "268: [D loss: 0.600958, acc: 0.716797]  [A loss: 0.709877, acc: 0.523438]\n",
      "269: [D loss: 0.733003, acc: 0.511719]  [A loss: 1.641092, acc: 0.000000]\n",
      "270: [D loss: 0.625070, acc: 0.701172]  [A loss: 0.756552, acc: 0.382812]\n",
      "271: [D loss: 0.719820, acc: 0.521484]  [A loss: 1.469000, acc: 0.000000]\n",
      "272: [D loss: 0.619215, acc: 0.712891]  [A loss: 0.837586, acc: 0.257812]\n",
      "273: [D loss: 0.692520, acc: 0.531250]  [A loss: 1.380105, acc: 0.000000]\n",
      "274: [D loss: 0.604746, acc: 0.724609]  [A loss: 0.862409, acc: 0.156250]\n",
      "275: [D loss: 0.672014, acc: 0.527344]  [A loss: 1.418792, acc: 0.000000]\n",
      "276: [D loss: 0.629023, acc: 0.650391]  [A loss: 0.969800, acc: 0.085938]\n",
      "277: [D loss: 0.664810, acc: 0.558594]  [A loss: 1.360002, acc: 0.003906]\n",
      "278: [D loss: 0.633545, acc: 0.644531]  [A loss: 0.959187, acc: 0.078125]\n",
      "279: [D loss: 0.662105, acc: 0.562500]  [A loss: 1.392581, acc: 0.000000]\n",
      "280: [D loss: 0.632644, acc: 0.636719]  [A loss: 0.910265, acc: 0.136719]\n",
      "281: [D loss: 0.681033, acc: 0.546875]  [A loss: 1.550949, acc: 0.000000]\n",
      "282: [D loss: 0.629915, acc: 0.666016]  [A loss: 0.813518, acc: 0.265625]\n",
      "283: [D loss: 0.699222, acc: 0.544922]  [A loss: 1.572177, acc: 0.000000]\n",
      "284: [D loss: 0.625589, acc: 0.671875]  [A loss: 0.749799, acc: 0.414062]\n",
      "285: [D loss: 0.699561, acc: 0.529297]  [A loss: 1.521878, acc: 0.000000]\n",
      "286: [D loss: 0.650808, acc: 0.601562]  [A loss: 0.854274, acc: 0.296875]\n",
      "287: [D loss: 0.699443, acc: 0.580078]  [A loss: 1.510667, acc: 0.000000]\n",
      "288: [D loss: 0.633056, acc: 0.650391]  [A loss: 0.823746, acc: 0.332031]\n",
      "289: [D loss: 0.705230, acc: 0.546875]  [A loss: 1.560711, acc: 0.000000]\n",
      "290: [D loss: 0.617441, acc: 0.669922]  [A loss: 0.774093, acc: 0.394531]\n",
      "291: [D loss: 0.709322, acc: 0.554688]  [A loss: 1.406585, acc: 0.000000]\n",
      "292: [D loss: 0.608838, acc: 0.726562]  [A loss: 0.842797, acc: 0.207031]\n",
      "293: [D loss: 0.667677, acc: 0.541016]  [A loss: 1.295242, acc: 0.000000]\n",
      "294: [D loss: 0.627438, acc: 0.669922]  [A loss: 0.888516, acc: 0.117188]\n",
      "295: [D loss: 0.658842, acc: 0.541016]  [A loss: 1.291413, acc: 0.000000]\n",
      "296: [D loss: 0.623038, acc: 0.666016]  [A loss: 0.960364, acc: 0.070312]\n",
      "297: [D loss: 0.658140, acc: 0.544922]  [A loss: 1.347418, acc: 0.000000]\n",
      "298: [D loss: 0.623556, acc: 0.652344]  [A loss: 0.897121, acc: 0.117188]\n",
      "299: [D loss: 0.673974, acc: 0.533203]  [A loss: 1.443625, acc: 0.000000]\n",
      "300: [D loss: 0.640587, acc: 0.625000]  [A loss: 0.883189, acc: 0.152344]\n",
      "301: [D loss: 0.684622, acc: 0.523438]  [A loss: 1.473651, acc: 0.000000]\n",
      "302: [D loss: 0.623606, acc: 0.677734]  [A loss: 0.786024, acc: 0.316406]\n",
      "303: [D loss: 0.695480, acc: 0.521484]  [A loss: 1.435189, acc: 0.000000]\n",
      "304: [D loss: 0.631542, acc: 0.673828]  [A loss: 0.801514, acc: 0.246094]\n",
      "305: [D loss: 0.714365, acc: 0.523438]  [A loss: 1.645334, acc: 0.000000]\n",
      "306: [D loss: 0.619600, acc: 0.675781]  [A loss: 0.708941, acc: 0.523438]\n",
      "307: [D loss: 0.739482, acc: 0.519531]  [A loss: 1.611236, acc: 0.000000]\n",
      "308: [D loss: 0.615631, acc: 0.687500]  [A loss: 0.761031, acc: 0.355469]\n",
      "309: [D loss: 0.701063, acc: 0.517578]  [A loss: 1.543708, acc: 0.000000]\n",
      "310: [D loss: 0.605519, acc: 0.718750]  [A loss: 0.832998, acc: 0.210938]\n",
      "311: [D loss: 0.674675, acc: 0.519531]  [A loss: 1.456754, acc: 0.003906]\n",
      "312: [D loss: 0.620119, acc: 0.708984]  [A loss: 0.803933, acc: 0.207031]\n",
      "313: [D loss: 0.727894, acc: 0.511719]  [A loss: 1.313290, acc: 0.000000]\n",
      "314: [D loss: 0.646737, acc: 0.648438]  [A loss: 0.862717, acc: 0.117188]\n",
      "315: [D loss: 0.670436, acc: 0.548828]  [A loss: 1.147470, acc: 0.003906]\n",
      "316: [D loss: 0.654794, acc: 0.609375]  [A loss: 0.990052, acc: 0.035156]\n",
      "317: [D loss: 0.657161, acc: 0.585938]  [A loss: 1.034901, acc: 0.007812]\n",
      "318: [D loss: 0.621576, acc: 0.656250]  [A loss: 1.062206, acc: 0.015625]\n",
      "319: [D loss: 0.610128, acc: 0.669922]  [A loss: 1.123203, acc: 0.015625]\n",
      "320: [D loss: 0.599283, acc: 0.679688]  [A loss: 1.141777, acc: 0.007812]\n",
      "321: [D loss: 0.578758, acc: 0.691406]  [A loss: 1.195991, acc: 0.000000]\n",
      "322: [D loss: 0.607981, acc: 0.632812]  [A loss: 1.327118, acc: 0.000000]\n",
      "323: [D loss: 0.594338, acc: 0.691406]  [A loss: 1.141372, acc: 0.011719]\n",
      "324: [D loss: 0.654929, acc: 0.570312]  [A loss: 1.471014, acc: 0.000000]\n",
      "325: [D loss: 0.619477, acc: 0.693359]  [A loss: 0.883096, acc: 0.132812]\n",
      "326: [D loss: 0.674348, acc: 0.535156]  [A loss: 1.667548, acc: 0.000000]\n",
      "327: [D loss: 0.634003, acc: 0.652344]  [A loss: 0.734420, acc: 0.394531]\n",
      "328: [D loss: 0.736036, acc: 0.505859]  [A loss: 1.636602, acc: 0.000000]\n",
      "329: [D loss: 0.632280, acc: 0.669922]  [A loss: 0.681300, acc: 0.593750]\n",
      "330: [D loss: 0.756732, acc: 0.517578]  [A loss: 1.532391, acc: 0.000000]\n",
      "331: [D loss: 0.676576, acc: 0.593750]  [A loss: 0.729318, acc: 0.457031]\n",
      "332: [D loss: 0.770389, acc: 0.539062]  [A loss: 1.361858, acc: 0.000000]\n",
      "333: [D loss: 0.681262, acc: 0.544922]  [A loss: 0.813556, acc: 0.292969]\n",
      "334: [D loss: 0.702443, acc: 0.527344]  [A loss: 1.187251, acc: 0.011719]\n",
      "335: [D loss: 0.662889, acc: 0.548828]  [A loss: 0.950915, acc: 0.093750]\n",
      "336: [D loss: 0.679265, acc: 0.564453]  [A loss: 1.082172, acc: 0.007812]\n",
      "337: [D loss: 0.650474, acc: 0.603516]  [A loss: 0.983742, acc: 0.093750]\n",
      "338: [D loss: 0.659096, acc: 0.615234]  [A loss: 1.245834, acc: 0.000000]\n",
      "339: [D loss: 0.616757, acc: 0.648438]  [A loss: 1.033232, acc: 0.062500]\n",
      "340: [D loss: 0.641293, acc: 0.599609]  [A loss: 1.332378, acc: 0.000000]\n",
      "341: [D loss: 0.606744, acc: 0.710938]  [A loss: 1.039176, acc: 0.027344]\n",
      "342: [D loss: 0.618104, acc: 0.617188]  [A loss: 1.453737, acc: 0.000000]\n",
      "343: [D loss: 0.590693, acc: 0.740234]  [A loss: 0.884168, acc: 0.136719]\n",
      "344: [D loss: 0.642350, acc: 0.541016]  [A loss: 1.640232, acc: 0.000000]\n",
      "345: [D loss: 0.592944, acc: 0.728516]  [A loss: 0.700302, acc: 0.539062]\n",
      "346: [D loss: 0.714671, acc: 0.509766]  [A loss: 1.687603, acc: 0.000000]\n",
      "347: [D loss: 0.623014, acc: 0.677734]  [A loss: 0.751505, acc: 0.417969]\n",
      "348: [D loss: 0.719769, acc: 0.523438]  [A loss: 1.490303, acc: 0.000000]\n",
      "349: [D loss: 0.648019, acc: 0.625000]  [A loss: 0.859817, acc: 0.171875]\n",
      "350: [D loss: 0.703502, acc: 0.521484]  [A loss: 1.342068, acc: 0.000000]\n",
      "351: [D loss: 0.675702, acc: 0.582031]  [A loss: 0.831157, acc: 0.207031]\n",
      "352: [D loss: 0.694745, acc: 0.570312]  [A loss: 1.366325, acc: 0.000000]\n",
      "353: [D loss: 0.647322, acc: 0.654297]  [A loss: 0.839556, acc: 0.183594]\n",
      "354: [D loss: 0.682956, acc: 0.546875]  [A loss: 1.413731, acc: 0.000000]\n",
      "355: [D loss: 0.600890, acc: 0.722656]  [A loss: 0.879422, acc: 0.207031]\n",
      "356: [D loss: 0.649918, acc: 0.572266]  [A loss: 1.442400, acc: 0.000000]\n",
      "357: [D loss: 0.579189, acc: 0.765625]  [A loss: 0.954404, acc: 0.125000]\n",
      "358: [D loss: 0.606759, acc: 0.597656]  [A loss: 1.434717, acc: 0.000000]\n",
      "359: [D loss: 0.578863, acc: 0.755859]  [A loss: 1.024092, acc: 0.027344]\n",
      "360: [D loss: 0.641103, acc: 0.566406]  [A loss: 1.671255, acc: 0.000000]\n",
      "361: [D loss: 0.577084, acc: 0.738281]  [A loss: 0.859412, acc: 0.246094]\n",
      "362: [D loss: 0.724142, acc: 0.548828]  [A loss: 1.774238, acc: 0.000000]\n",
      "363: [D loss: 0.630536, acc: 0.626953]  [A loss: 0.805181, acc: 0.406250]\n",
      "364: [D loss: 0.699984, acc: 0.548828]  [A loss: 1.405716, acc: 0.000000]\n",
      "365: [D loss: 0.599302, acc: 0.691406]  [A loss: 0.905573, acc: 0.144531]\n",
      "366: [D loss: 0.671486, acc: 0.509766]  [A loss: 1.310950, acc: 0.000000]\n",
      "367: [D loss: 0.607476, acc: 0.689453]  [A loss: 1.017505, acc: 0.027344]\n",
      "368: [D loss: 0.619866, acc: 0.589844]  [A loss: 1.328787, acc: 0.000000]\n",
      "369: [D loss: 0.589461, acc: 0.730469]  [A loss: 1.096581, acc: 0.007812]\n",
      "370: [D loss: 0.615703, acc: 0.603516]  [A loss: 1.341146, acc: 0.003906]\n",
      "371: [D loss: 0.573188, acc: 0.736328]  [A loss: 0.921280, acc: 0.179688]\n",
      "372: [D loss: 0.650577, acc: 0.546875]  [A loss: 1.379576, acc: 0.003906]\n",
      "373: [D loss: 0.596900, acc: 0.750000]  [A loss: 0.739155, acc: 0.414062]\n",
      "374: [D loss: 0.658671, acc: 0.511719]  [A loss: 1.545827, acc: 0.000000]\n",
      "375: [D loss: 0.620772, acc: 0.673828]  [A loss: 0.850735, acc: 0.230469]\n",
      "376: [D loss: 0.633756, acc: 0.558594]  [A loss: 1.716516, acc: 0.000000]\n",
      "377: [D loss: 0.579699, acc: 0.751953]  [A loss: 0.832707, acc: 0.265625]\n",
      "378: [D loss: 0.652325, acc: 0.531250]  [A loss: 1.788930, acc: 0.000000]\n",
      "379: [D loss: 0.545780, acc: 0.781250]  [A loss: 0.969928, acc: 0.097656]\n",
      "380: [D loss: 0.716344, acc: 0.535156]  [A loss: 1.697532, acc: 0.000000]\n",
      "381: [D loss: 0.594060, acc: 0.693359]  [A loss: 0.842125, acc: 0.195312]\n",
      "382: [D loss: 0.670033, acc: 0.513672]  [A loss: 1.624229, acc: 0.000000]\n",
      "383: [D loss: 0.548790, acc: 0.769531]  [A loss: 1.082707, acc: 0.046875]\n",
      "384: [D loss: 0.644979, acc: 0.548828]  [A loss: 1.724014, acc: 0.000000]\n",
      "385: [D loss: 0.582963, acc: 0.716797]  [A loss: 1.041153, acc: 0.078125]\n",
      "386: [D loss: 0.722627, acc: 0.546875]  [A loss: 1.485353, acc: 0.000000]\n",
      "387: [D loss: 0.626827, acc: 0.664062]  [A loss: 1.010980, acc: 0.117188]\n",
      "388: [D loss: 0.671495, acc: 0.566406]  [A loss: 1.356990, acc: 0.000000]\n",
      "389: [D loss: 0.635404, acc: 0.652344]  [A loss: 1.076248, acc: 0.035156]\n",
      "390: [D loss: 0.705593, acc: 0.560547]  [A loss: 1.346861, acc: 0.007812]\n",
      "391: [D loss: 0.686109, acc: 0.556641]  [A loss: 1.106557, acc: 0.039062]\n",
      "392: [D loss: 0.680840, acc: 0.568359]  [A loss: 1.207287, acc: 0.019531]\n",
      "393: [D loss: 0.695506, acc: 0.556641]  [A loss: 1.035961, acc: 0.105469]\n",
      "394: [D loss: 0.740196, acc: 0.498047]  [A loss: 1.223729, acc: 0.023438]\n",
      "395: [D loss: 0.684562, acc: 0.556641]  [A loss: 0.912763, acc: 0.222656]\n",
      "396: [D loss: 0.693897, acc: 0.544922]  [A loss: 1.411356, acc: 0.000000]\n",
      "397: [D loss: 0.621531, acc: 0.687500]  [A loss: 0.922666, acc: 0.199219]\n",
      "398: [D loss: 0.709238, acc: 0.539062]  [A loss: 1.513622, acc: 0.003906]\n",
      "399: [D loss: 0.649398, acc: 0.599609]  [A loss: 0.778696, acc: 0.347656]\n",
      "400: [D loss: 0.715879, acc: 0.527344]  [A loss: 1.429131, acc: 0.007812]\n",
      "401: [D loss: 0.623280, acc: 0.695312]  [A loss: 0.761075, acc: 0.394531]\n",
      "402: [D loss: 0.709667, acc: 0.535156]  [A loss: 1.446572, acc: 0.000000]\n",
      "403: [D loss: 0.589796, acc: 0.736328]  [A loss: 0.905132, acc: 0.191406]\n",
      "404: [D loss: 0.693742, acc: 0.539062]  [A loss: 1.520805, acc: 0.000000]\n",
      "405: [D loss: 0.617526, acc: 0.667969]  [A loss: 0.869899, acc: 0.207031]\n",
      "406: [D loss: 0.671597, acc: 0.568359]  [A loss: 1.388805, acc: 0.000000]\n",
      "407: [D loss: 0.612400, acc: 0.666016]  [A loss: 0.926701, acc: 0.148438]\n",
      "408: [D loss: 0.657990, acc: 0.558594]  [A loss: 1.438296, acc: 0.000000]\n",
      "409: [D loss: 0.635397, acc: 0.632812]  [A loss: 0.950827, acc: 0.175781]\n",
      "410: [D loss: 0.663421, acc: 0.597656]  [A loss: 1.422604, acc: 0.003906]\n",
      "411: [D loss: 0.644186, acc: 0.595703]  [A loss: 0.920358, acc: 0.179688]\n",
      "412: [D loss: 0.707752, acc: 0.597656]  [A loss: 1.474512, acc: 0.003906]\n",
      "413: [D loss: 0.660029, acc: 0.568359]  [A loss: 0.825415, acc: 0.285156]\n",
      "414: [D loss: 0.722879, acc: 0.521484]  [A loss: 1.424984, acc: 0.000000]\n",
      "415: [D loss: 0.655103, acc: 0.599609]  [A loss: 0.821160, acc: 0.312500]\n",
      "416: [D loss: 0.688601, acc: 0.537109]  [A loss: 1.370208, acc: 0.000000]\n",
      "417: [D loss: 0.636289, acc: 0.652344]  [A loss: 0.951467, acc: 0.089844]\n",
      "418: [D loss: 0.662665, acc: 0.539062]  [A loss: 1.420748, acc: 0.000000]\n",
      "419: [D loss: 0.607920, acc: 0.720703]  [A loss: 0.891898, acc: 0.160156]\n",
      "420: [D loss: 0.682055, acc: 0.537109]  [A loss: 1.514130, acc: 0.000000]\n",
      "421: [D loss: 0.632811, acc: 0.648438]  [A loss: 0.888215, acc: 0.171875]\n",
      "422: [D loss: 0.712667, acc: 0.533203]  [A loss: 1.638854, acc: 0.000000]\n",
      "423: [D loss: 0.619669, acc: 0.667969]  [A loss: 0.884254, acc: 0.250000]\n",
      "424: [D loss: 0.728877, acc: 0.529297]  [A loss: 1.675477, acc: 0.000000]\n",
      "425: [D loss: 0.578646, acc: 0.736328]  [A loss: 0.921976, acc: 0.093750]\n",
      "426: [D loss: 0.637698, acc: 0.568359]  [A loss: 1.478795, acc: 0.000000]\n",
      "427: [D loss: 0.556938, acc: 0.759766]  [A loss: 1.046991, acc: 0.054688]\n",
      "428: [D loss: 0.636212, acc: 0.585938]  [A loss: 1.529750, acc: 0.000000]\n",
      "429: [D loss: 0.596734, acc: 0.697266]  [A loss: 1.121599, acc: 0.039062]\n",
      "430: [D loss: 0.664511, acc: 0.605469]  [A loss: 1.469828, acc: 0.003906]\n",
      "431: [D loss: 0.642827, acc: 0.625000]  [A loss: 1.090858, acc: 0.042969]\n",
      "432: [D loss: 0.649119, acc: 0.548828]  [A loss: 1.420380, acc: 0.000000]\n",
      "433: [D loss: 0.607276, acc: 0.681641]  [A loss: 1.202431, acc: 0.015625]\n",
      "434: [D loss: 0.649778, acc: 0.572266]  [A loss: 1.524203, acc: 0.003906]\n",
      "435: [D loss: 0.639081, acc: 0.617188]  [A loss: 1.099076, acc: 0.054688]\n",
      "436: [D loss: 0.708334, acc: 0.531250]  [A loss: 1.691964, acc: 0.000000]\n",
      "437: [D loss: 0.659583, acc: 0.597656]  [A loss: 0.722140, acc: 0.515625]\n",
      "438: [D loss: 0.780131, acc: 0.515625]  [A loss: 1.642697, acc: 0.000000]\n",
      "439: [D loss: 0.690136, acc: 0.554688]  [A loss: 0.756115, acc: 0.425781]\n",
      "440: [D loss: 0.737036, acc: 0.501953]  [A loss: 1.281116, acc: 0.000000]\n",
      "441: [D loss: 0.679694, acc: 0.576172]  [A loss: 0.852498, acc: 0.207031]\n",
      "442: [D loss: 0.713807, acc: 0.503906]  [A loss: 1.265701, acc: 0.000000]\n",
      "443: [D loss: 0.657997, acc: 0.582031]  [A loss: 0.895388, acc: 0.132812]\n",
      "444: [D loss: 0.662194, acc: 0.546875]  [A loss: 1.125643, acc: 0.011719]\n",
      "445: [D loss: 0.617612, acc: 0.632812]  [A loss: 1.063010, acc: 0.046875]\n",
      "446: [D loss: 0.645703, acc: 0.611328]  [A loss: 1.235861, acc: 0.003906]\n",
      "447: [D loss: 0.635973, acc: 0.636719]  [A loss: 1.101150, acc: 0.039062]\n",
      "448: [D loss: 0.637339, acc: 0.625000]  [A loss: 1.174481, acc: 0.039062]\n",
      "449: [D loss: 0.633208, acc: 0.671875]  [A loss: 1.166209, acc: 0.027344]\n",
      "450: [D loss: 0.675881, acc: 0.572266]  [A loss: 1.463145, acc: 0.000000]\n",
      "451: [D loss: 0.625243, acc: 0.634766]  [A loss: 1.179606, acc: 0.050781]\n",
      "452: [D loss: 0.722504, acc: 0.511719]  [A loss: 1.571693, acc: 0.000000]\n",
      "453: [D loss: 0.656884, acc: 0.597656]  [A loss: 0.819061, acc: 0.328125]\n",
      "454: [D loss: 0.728903, acc: 0.527344]  [A loss: 1.646926, acc: 0.000000]\n",
      "455: [D loss: 0.693572, acc: 0.539062]  [A loss: 0.656670, acc: 0.648438]\n",
      "456: [D loss: 0.776963, acc: 0.503906]  [A loss: 1.348316, acc: 0.000000]\n",
      "457: [D loss: 0.666538, acc: 0.595703]  [A loss: 0.801757, acc: 0.292969]\n",
      "458: [D loss: 0.702932, acc: 0.517578]  [A loss: 1.296500, acc: 0.000000]\n",
      "459: [D loss: 0.622187, acc: 0.673828]  [A loss: 0.922492, acc: 0.113281]\n",
      "460: [D loss: 0.666073, acc: 0.548828]  [A loss: 1.295267, acc: 0.003906]\n",
      "461: [D loss: 0.641254, acc: 0.640625]  [A loss: 0.885993, acc: 0.160156]\n",
      "462: [D loss: 0.702069, acc: 0.527344]  [A loss: 1.358296, acc: 0.000000]\n",
      "463: [D loss: 0.613163, acc: 0.714844]  [A loss: 0.928162, acc: 0.144531]\n",
      "464: [D loss: 0.677830, acc: 0.527344]  [A loss: 1.431792, acc: 0.000000]\n",
      "465: [D loss: 0.631050, acc: 0.621094]  [A loss: 0.999583, acc: 0.089844]\n",
      "466: [D loss: 0.662734, acc: 0.568359]  [A loss: 1.289232, acc: 0.015625]\n",
      "467: [D loss: 0.590343, acc: 0.701172]  [A loss: 0.839392, acc: 0.296875]\n",
      "468: [D loss: 0.710083, acc: 0.546875]  [A loss: 1.459866, acc: 0.007812]\n",
      "469: [D loss: 0.620579, acc: 0.667969]  [A loss: 0.851294, acc: 0.210938]\n",
      "470: [D loss: 0.719949, acc: 0.501953]  [A loss: 1.499495, acc: 0.000000]\n",
      "471: [D loss: 0.624363, acc: 0.685547]  [A loss: 0.761386, acc: 0.398438]\n",
      "472: [D loss: 0.736748, acc: 0.500000]  [A loss: 1.452633, acc: 0.000000]\n",
      "473: [D loss: 0.629761, acc: 0.677734]  [A loss: 0.774349, acc: 0.351562]\n",
      "474: [D loss: 0.762818, acc: 0.501953]  [A loss: 1.559309, acc: 0.000000]\n",
      "475: [D loss: 0.657298, acc: 0.621094]  [A loss: 0.841806, acc: 0.242188]\n",
      "476: [D loss: 0.760046, acc: 0.505859]  [A loss: 1.529247, acc: 0.000000]\n",
      "477: [D loss: 0.673764, acc: 0.576172]  [A loss: 0.870585, acc: 0.207031]\n",
      "478: [D loss: 0.747049, acc: 0.511719]  [A loss: 1.252690, acc: 0.019531]\n",
      "479: [D loss: 0.726666, acc: 0.554688]  [A loss: 0.896675, acc: 0.175781]\n",
      "480: [D loss: 0.740305, acc: 0.476562]  [A loss: 1.185011, acc: 0.011719]\n",
      "481: [D loss: 0.694247, acc: 0.544922]  [A loss: 1.068560, acc: 0.089844]\n",
      "482: [D loss: 0.679685, acc: 0.552734]  [A loss: 1.283321, acc: 0.019531]\n",
      "483: [D loss: 0.641450, acc: 0.656250]  [A loss: 1.072368, acc: 0.097656]\n",
      "484: [D loss: 0.647294, acc: 0.628906]  [A loss: 1.247467, acc: 0.007812]\n",
      "485: [D loss: 0.620687, acc: 0.650391]  [A loss: 1.174969, acc: 0.007812]\n",
      "486: [D loss: 0.656670, acc: 0.597656]  [A loss: 1.339073, acc: 0.007812]\n",
      "487: [D loss: 0.601700, acc: 0.705078]  [A loss: 1.102500, acc: 0.031250]\n",
      "488: [D loss: 0.620181, acc: 0.623047]  [A loss: 1.432947, acc: 0.003906]\n",
      "489: [D loss: 0.611095, acc: 0.671875]  [A loss: 1.199508, acc: 0.015625]\n",
      "490: [D loss: 0.611023, acc: 0.648438]  [A loss: 1.642139, acc: 0.000000]\n",
      "491: [D loss: 0.600919, acc: 0.699219]  [A loss: 0.961842, acc: 0.164062]\n",
      "492: [D loss: 0.697130, acc: 0.517578]  [A loss: 2.204503, acc: 0.000000]\n",
      "493: [D loss: 0.650802, acc: 0.595703]  [A loss: 0.560890, acc: 0.812500]\n",
      "494: [D loss: 0.835824, acc: 0.507812]  [A loss: 1.698081, acc: 0.000000]\n",
      "495: [D loss: 0.656507, acc: 0.625000]  [A loss: 0.837446, acc: 0.253906]\n",
      "496: [D loss: 0.705139, acc: 0.525391]  [A loss: 1.295162, acc: 0.000000]\n",
      "497: [D loss: 0.653446, acc: 0.601562]  [A loss: 0.960140, acc: 0.101562]\n",
      "498: [D loss: 0.678881, acc: 0.523438]  [A loss: 1.291540, acc: 0.003906]\n",
      "499: [D loss: 0.659036, acc: 0.568359]  [A loss: 1.048511, acc: 0.042969]\n",
      "500: [D loss: 0.688880, acc: 0.527344]  [A loss: 1.165362, acc: 0.015625]\n",
      "501: [D loss: 0.658214, acc: 0.568359]  [A loss: 1.033935, acc: 0.042969]\n",
      "502: [D loss: 0.646936, acc: 0.591797]  [A loss: 1.213051, acc: 0.007812]\n",
      "503: [D loss: 0.645354, acc: 0.595703]  [A loss: 1.097324, acc: 0.039062]\n",
      "504: [D loss: 0.662307, acc: 0.587891]  [A loss: 1.359785, acc: 0.000000]\n",
      "505: [D loss: 0.628128, acc: 0.652344]  [A loss: 1.083108, acc: 0.027344]\n",
      "506: [D loss: 0.660955, acc: 0.578125]  [A loss: 1.454365, acc: 0.000000]\n",
      "507: [D loss: 0.619830, acc: 0.685547]  [A loss: 0.920007, acc: 0.164062]\n",
      "508: [D loss: 0.650908, acc: 0.582031]  [A loss: 1.635162, acc: 0.000000]\n",
      "509: [D loss: 0.584400, acc: 0.724609]  [A loss: 0.723624, acc: 0.453125]\n",
      "510: [D loss: 0.715664, acc: 0.525391]  [A loss: 1.724777, acc: 0.000000]\n",
      "511: [D loss: 0.622656, acc: 0.650391]  [A loss: 0.697704, acc: 0.566406]\n",
      "512: [D loss: 0.708371, acc: 0.523438]  [A loss: 1.589035, acc: 0.000000]\n",
      "513: [D loss: 0.616754, acc: 0.699219]  [A loss: 0.994074, acc: 0.121094]\n",
      "514: [D loss: 0.645094, acc: 0.558594]  [A loss: 1.448909, acc: 0.000000]\n",
      "515: [D loss: 0.610683, acc: 0.660156]  [A loss: 1.031587, acc: 0.082031]\n",
      "516: [D loss: 0.684075, acc: 0.525391]  [A loss: 1.533950, acc: 0.000000]\n",
      "517: [D loss: 0.618987, acc: 0.666016]  [A loss: 0.933388, acc: 0.136719]\n",
      "518: [D loss: 0.663955, acc: 0.582031]  [A loss: 1.354916, acc: 0.007812]\n",
      "519: [D loss: 0.644082, acc: 0.613281]  [A loss: 0.823249, acc: 0.425781]\n",
      "520: [D loss: 0.668504, acc: 0.574219]  [A loss: 1.186040, acc: 0.023438]\n",
      "521: [D loss: 0.628538, acc: 0.623047]  [A loss: 0.998855, acc: 0.113281]\n",
      "522: [D loss: 0.625467, acc: 0.572266]  [A loss: 1.334276, acc: 0.007812]\n",
      "523: [D loss: 0.616376, acc: 0.625000]  [A loss: 1.107033, acc: 0.046875]\n",
      "524: [D loss: 0.668399, acc: 0.564453]  [A loss: 1.535966, acc: 0.000000]\n",
      "525: [D loss: 0.615996, acc: 0.677734]  [A loss: 1.042938, acc: 0.058594]\n",
      "526: [D loss: 0.664118, acc: 0.564453]  [A loss: 1.547961, acc: 0.000000]\n",
      "527: [D loss: 0.617739, acc: 0.673828]  [A loss: 0.994737, acc: 0.136719]\n",
      "528: [D loss: 0.709654, acc: 0.535156]  [A loss: 1.532202, acc: 0.000000]\n",
      "529: [D loss: 0.652838, acc: 0.591797]  [A loss: 0.846368, acc: 0.292969]\n",
      "530: [D loss: 0.692581, acc: 0.529297]  [A loss: 1.510107, acc: 0.003906]\n",
      "531: [D loss: 0.640157, acc: 0.623047]  [A loss: 0.869050, acc: 0.273438]\n",
      "532: [D loss: 0.674355, acc: 0.574219]  [A loss: 1.397366, acc: 0.003906]\n",
      "533: [D loss: 0.606251, acc: 0.681641]  [A loss: 0.983279, acc: 0.167969]\n",
      "534: [D loss: 0.687844, acc: 0.537109]  [A loss: 1.373879, acc: 0.003906]\n",
      "535: [D loss: 0.603653, acc: 0.701172]  [A loss: 0.887854, acc: 0.210938]\n",
      "536: [D loss: 0.715540, acc: 0.498047]  [A loss: 1.385593, acc: 0.003906]\n",
      "537: [D loss: 0.647689, acc: 0.595703]  [A loss: 0.881490, acc: 0.238281]\n",
      "538: [D loss: 0.695755, acc: 0.546875]  [A loss: 1.296209, acc: 0.015625]\n",
      "539: [D loss: 0.641050, acc: 0.611328]  [A loss: 0.926371, acc: 0.152344]\n",
      "540: [D loss: 0.672857, acc: 0.542969]  [A loss: 1.297808, acc: 0.011719]\n",
      "541: [D loss: 0.672792, acc: 0.580078]  [A loss: 1.025578, acc: 0.089844]\n",
      "542: [D loss: 0.667716, acc: 0.580078]  [A loss: 1.312677, acc: 0.000000]\n",
      "543: [D loss: 0.656531, acc: 0.568359]  [A loss: 0.983443, acc: 0.128906]\n",
      "544: [D loss: 0.732207, acc: 0.517578]  [A loss: 1.440984, acc: 0.003906]\n",
      "545: [D loss: 0.642337, acc: 0.611328]  [A loss: 0.874134, acc: 0.238281]\n",
      "546: [D loss: 0.668542, acc: 0.539062]  [A loss: 1.434487, acc: 0.003906]\n",
      "547: [D loss: 0.577424, acc: 0.742188]  [A loss: 0.858133, acc: 0.199219]\n",
      "548: [D loss: 0.668699, acc: 0.546875]  [A loss: 1.532879, acc: 0.000000]\n",
      "549: [D loss: 0.572137, acc: 0.765625]  [A loss: 0.821172, acc: 0.332031]\n",
      "550: [D loss: 0.646117, acc: 0.535156]  [A loss: 1.423955, acc: 0.000000]\n",
      "551: [D loss: 0.621496, acc: 0.658203]  [A loss: 0.973038, acc: 0.128906]\n",
      "552: [D loss: 0.674736, acc: 0.548828]  [A loss: 1.154851, acc: 0.027344]\n",
      "553: [D loss: 0.714861, acc: 0.521484]  [A loss: 1.509717, acc: 0.003906]\n",
      "554: [D loss: 0.711244, acc: 0.482422]  [A loss: 0.910527, acc: 0.218750]\n",
      "555: [D loss: 0.752387, acc: 0.521484]  [A loss: 1.613782, acc: 0.003906]\n",
      "556: [D loss: 0.709915, acc: 0.500000]  [A loss: 0.877185, acc: 0.277344]\n",
      "557: [D loss: 0.761343, acc: 0.529297]  [A loss: 1.491080, acc: 0.000000]\n",
      "558: [D loss: 0.664499, acc: 0.578125]  [A loss: 0.929311, acc: 0.167969]\n",
      "559: [D loss: 0.680073, acc: 0.564453]  [A loss: 1.505111, acc: 0.003906]\n",
      "560: [D loss: 0.622572, acc: 0.662109]  [A loss: 0.942784, acc: 0.117188]\n",
      "561: [D loss: 0.670967, acc: 0.554688]  [A loss: 1.471602, acc: 0.000000]\n",
      "562: [D loss: 0.595012, acc: 0.728516]  [A loss: 0.934530, acc: 0.175781]\n",
      "563: [D loss: 0.670098, acc: 0.556641]  [A loss: 1.554033, acc: 0.000000]\n",
      "564: [D loss: 0.611692, acc: 0.662109]  [A loss: 0.973581, acc: 0.117188]\n",
      "565: [D loss: 0.661823, acc: 0.568359]  [A loss: 1.490011, acc: 0.000000]\n",
      "566: [D loss: 0.644496, acc: 0.628906]  [A loss: 0.875200, acc: 0.222656]\n",
      "567: [D loss: 0.673200, acc: 0.548828]  [A loss: 1.444333, acc: 0.000000]\n",
      "568: [D loss: 0.605477, acc: 0.708984]  [A loss: 0.844326, acc: 0.292969]\n",
      "569: [D loss: 0.696183, acc: 0.521484]  [A loss: 1.340435, acc: 0.000000]\n",
      "570: [D loss: 0.636891, acc: 0.625000]  [A loss: 0.909866, acc: 0.136719]\n",
      "571: [D loss: 0.691174, acc: 0.496094]  [A loss: 1.219456, acc: 0.019531]\n",
      "572: [D loss: 0.649243, acc: 0.623047]  [A loss: 0.887250, acc: 0.226562]\n",
      "573: [D loss: 0.683737, acc: 0.513672]  [A loss: 1.239504, acc: 0.000000]\n",
      "574: [D loss: 0.630363, acc: 0.644531]  [A loss: 0.915316, acc: 0.152344]\n",
      "575: [D loss: 0.670789, acc: 0.541016]  [A loss: 1.391193, acc: 0.000000]\n",
      "576: [D loss: 0.624314, acc: 0.666016]  [A loss: 0.842325, acc: 0.265625]\n",
      "577: [D loss: 0.716815, acc: 0.501953]  [A loss: 1.327902, acc: 0.000000]\n",
      "578: [D loss: 0.660818, acc: 0.597656]  [A loss: 0.973913, acc: 0.144531]\n",
      "579: [D loss: 0.692849, acc: 0.523438]  [A loss: 1.470037, acc: 0.000000]\n",
      "580: [D loss: 0.737028, acc: 0.457031]  [A loss: 0.923990, acc: 0.121094]\n",
      "581: [D loss: 0.748652, acc: 0.453125]  [A loss: 1.244454, acc: 0.003906]\n",
      "582: [D loss: 0.675442, acc: 0.574219]  [A loss: 0.892809, acc: 0.191406]\n",
      "583: [D loss: 0.723991, acc: 0.498047]  [A loss: 1.213391, acc: 0.003906]\n",
      "584: [D loss: 0.653332, acc: 0.601562]  [A loss: 0.942706, acc: 0.101562]\n",
      "585: [D loss: 0.679814, acc: 0.541016]  [A loss: 1.333577, acc: 0.000000]\n",
      "586: [D loss: 0.616596, acc: 0.691406]  [A loss: 0.957734, acc: 0.093750]\n",
      "587: [D loss: 0.684840, acc: 0.542969]  [A loss: 1.628105, acc: 0.000000]\n",
      "588: [D loss: 0.626020, acc: 0.658203]  [A loss: 0.891118, acc: 0.199219]\n",
      "589: [D loss: 0.664387, acc: 0.576172]  [A loss: 1.581634, acc: 0.000000]\n",
      "590: [D loss: 0.605665, acc: 0.705078]  [A loss: 0.819749, acc: 0.312500]\n",
      "591: [D loss: 0.672893, acc: 0.541016]  [A loss: 1.514632, acc: 0.003906]\n",
      "592: [D loss: 0.582247, acc: 0.736328]  [A loss: 0.792661, acc: 0.324219]\n",
      "593: [D loss: 0.685075, acc: 0.539062]  [A loss: 1.644627, acc: 0.000000]\n",
      "594: [D loss: 0.580578, acc: 0.740234]  [A loss: 0.796784, acc: 0.328125]\n",
      "595: [D loss: 0.706622, acc: 0.544922]  [A loss: 1.523127, acc: 0.000000]\n",
      "596: [D loss: 0.626112, acc: 0.638672]  [A loss: 0.837057, acc: 0.296875]\n",
      "597: [D loss: 0.688700, acc: 0.574219]  [A loss: 1.329329, acc: 0.000000]\n",
      "598: [D loss: 0.628417, acc: 0.648438]  [A loss: 0.977328, acc: 0.105469]\n",
      "599: [D loss: 0.664555, acc: 0.566406]  [A loss: 1.356852, acc: 0.003906]\n",
      "600: [D loss: 0.637309, acc: 0.632812]  [A loss: 1.015952, acc: 0.062500]\n",
      "601: [D loss: 0.641128, acc: 0.570312]  [A loss: 1.342071, acc: 0.007812]\n",
      "602: [D loss: 0.579546, acc: 0.712891]  [A loss: 1.062162, acc: 0.082031]\n",
      "603: [D loss: 0.662431, acc: 0.576172]  [A loss: 1.451188, acc: 0.000000]\n",
      "604: [D loss: 0.598081, acc: 0.664062]  [A loss: 0.895866, acc: 0.210938]\n",
      "605: [D loss: 0.685837, acc: 0.562500]  [A loss: 1.512345, acc: 0.003906]\n",
      "606: [D loss: 0.649484, acc: 0.605469]  [A loss: 0.955398, acc: 0.179688]\n",
      "607: [D loss: 0.707363, acc: 0.535156]  [A loss: 1.406437, acc: 0.003906]\n",
      "608: [D loss: 0.652131, acc: 0.587891]  [A loss: 0.949348, acc: 0.156250]\n",
      "609: [D loss: 0.666253, acc: 0.576172]  [A loss: 1.227868, acc: 0.011719]\n",
      "610: [D loss: 0.666040, acc: 0.554688]  [A loss: 1.030776, acc: 0.082031]\n",
      "611: [D loss: 0.683507, acc: 0.539062]  [A loss: 1.319919, acc: 0.000000]\n",
      "612: [D loss: 0.643767, acc: 0.601562]  [A loss: 0.962943, acc: 0.113281]\n",
      "613: [D loss: 0.684138, acc: 0.509766]  [A loss: 1.421338, acc: 0.007812]\n",
      "614: [D loss: 0.631027, acc: 0.650391]  [A loss: 0.978026, acc: 0.097656]\n",
      "615: [D loss: 0.681805, acc: 0.527344]  [A loss: 1.549639, acc: 0.000000]\n",
      "616: [D loss: 0.616341, acc: 0.683594]  [A loss: 0.812667, acc: 0.285156]\n",
      "617: [D loss: 0.704435, acc: 0.509766]  [A loss: 1.599380, acc: 0.000000]\n",
      "618: [D loss: 0.612136, acc: 0.714844]  [A loss: 0.804304, acc: 0.312500]\n",
      "619: [D loss: 0.688457, acc: 0.509766]  [A loss: 1.549716, acc: 0.000000]\n",
      "620: [D loss: 0.618737, acc: 0.683594]  [A loss: 0.853791, acc: 0.289062]\n",
      "621: [D loss: 0.751488, acc: 0.474609]  [A loss: 1.710282, acc: 0.000000]\n",
      "622: [D loss: 0.674767, acc: 0.587891]  [A loss: 0.872058, acc: 0.207031]\n",
      "623: [D loss: 0.793224, acc: 0.460938]  [A loss: 1.465036, acc: 0.000000]\n",
      "624: [D loss: 0.697204, acc: 0.554688]  [A loss: 0.881562, acc: 0.191406]\n",
      "625: [D loss: 0.759474, acc: 0.488281]  [A loss: 1.257149, acc: 0.019531]\n",
      "626: [D loss: 0.693268, acc: 0.527344]  [A loss: 0.895286, acc: 0.207031]\n",
      "627: [D loss: 0.698612, acc: 0.548828]  [A loss: 1.127972, acc: 0.035156]\n",
      "628: [D loss: 0.667742, acc: 0.578125]  [A loss: 0.984545, acc: 0.113281]\n",
      "629: [D loss: 0.666898, acc: 0.560547]  [A loss: 1.164979, acc: 0.031250]\n",
      "630: [D loss: 0.637768, acc: 0.625000]  [A loss: 1.020787, acc: 0.085938]\n",
      "631: [D loss: 0.668065, acc: 0.548828]  [A loss: 1.349821, acc: 0.007812]\n",
      "632: [D loss: 0.648626, acc: 0.626953]  [A loss: 0.936253, acc: 0.125000]\n",
      "633: [D loss: 0.660721, acc: 0.542969]  [A loss: 1.451653, acc: 0.000000]\n",
      "634: [D loss: 0.622607, acc: 0.675781]  [A loss: 0.877956, acc: 0.207031]\n",
      "635: [D loss: 0.679273, acc: 0.535156]  [A loss: 1.523843, acc: 0.000000]\n",
      "636: [D loss: 0.602428, acc: 0.730469]  [A loss: 0.872792, acc: 0.156250]\n",
      "637: [D loss: 0.674333, acc: 0.505859]  [A loss: 1.462968, acc: 0.000000]\n",
      "638: [D loss: 0.597170, acc: 0.728516]  [A loss: 0.890716, acc: 0.183594]\n",
      "639: [D loss: 0.624903, acc: 0.570312]  [A loss: 1.421660, acc: 0.000000]\n",
      "640: [D loss: 0.592064, acc: 0.718750]  [A loss: 0.924316, acc: 0.164062]\n",
      "641: [D loss: 0.682615, acc: 0.531250]  [A loss: 1.439618, acc: 0.000000]\n",
      "642: [D loss: 0.639382, acc: 0.646484]  [A loss: 0.913425, acc: 0.199219]\n",
      "643: [D loss: 0.664286, acc: 0.558594]  [A loss: 1.429995, acc: 0.000000]\n",
      "644: [D loss: 0.593085, acc: 0.724609]  [A loss: 1.001881, acc: 0.121094]\n",
      "645: [D loss: 0.623498, acc: 0.609375]  [A loss: 1.360724, acc: 0.003906]\n",
      "646: [D loss: 0.580440, acc: 0.707031]  [A loss: 1.131396, acc: 0.082031]\n",
      "647: [D loss: 0.603682, acc: 0.625000]  [A loss: 1.514352, acc: 0.003906]\n",
      "648: [D loss: 0.603795, acc: 0.683594]  [A loss: 1.542087, acc: 0.003906]\n",
      "649: [D loss: 0.599255, acc: 0.652344]  [A loss: 1.407847, acc: 0.007812]\n",
      "650: [D loss: 0.606922, acc: 0.638672]  [A loss: 1.607196, acc: 0.000000]\n",
      "651: [D loss: 0.550273, acc: 0.742188]  [A loss: 1.292740, acc: 0.015625]\n",
      "652: [D loss: 0.571796, acc: 0.685547]  [A loss: 1.667488, acc: 0.000000]\n",
      "653: [D loss: 0.580866, acc: 0.685547]  [A loss: 1.090103, acc: 0.132812]\n",
      "654: [D loss: 0.692101, acc: 0.541016]  [A loss: 1.749348, acc: 0.000000]\n",
      "655: [D loss: 0.647926, acc: 0.640625]  [A loss: 0.809147, acc: 0.359375]\n",
      "656: [D loss: 0.719888, acc: 0.523438]  [A loss: 1.845096, acc: 0.000000]\n",
      "657: [D loss: 0.626177, acc: 0.677734]  [A loss: 0.794422, acc: 0.449219]\n",
      "658: [D loss: 0.844039, acc: 0.509766]  [A loss: 1.629793, acc: 0.003906]\n",
      "659: [D loss: 0.654371, acc: 0.617188]  [A loss: 0.823112, acc: 0.367188]\n",
      "660: [D loss: 0.779266, acc: 0.503906]  [A loss: 1.475242, acc: 0.000000]\n",
      "661: [D loss: 0.692811, acc: 0.607422]  [A loss: 0.813702, acc: 0.367188]\n",
      "662: [D loss: 0.786940, acc: 0.494141]  [A loss: 1.292219, acc: 0.019531]\n",
      "663: [D loss: 0.717404, acc: 0.546875]  [A loss: 0.986718, acc: 0.175781]\n",
      "664: [D loss: 0.767745, acc: 0.513672]  [A loss: 1.181346, acc: 0.039062]\n",
      "665: [D loss: 0.753735, acc: 0.488281]  [A loss: 1.005243, acc: 0.097656]\n",
      "666: [D loss: 0.706686, acc: 0.548828]  [A loss: 1.122978, acc: 0.035156]\n",
      "667: [D loss: 0.668182, acc: 0.578125]  [A loss: 1.008029, acc: 0.136719]\n",
      "668: [D loss: 0.691271, acc: 0.556641]  [A loss: 1.274772, acc: 0.019531]\n",
      "669: [D loss: 0.675249, acc: 0.566406]  [A loss: 1.018173, acc: 0.066406]\n",
      "670: [D loss: 0.670095, acc: 0.572266]  [A loss: 1.233773, acc: 0.031250]\n",
      "671: [D loss: 0.662814, acc: 0.593750]  [A loss: 1.088735, acc: 0.054688]\n",
      "672: [D loss: 0.689260, acc: 0.525391]  [A loss: 1.254819, acc: 0.019531]\n",
      "673: [D loss: 0.687715, acc: 0.550781]  [A loss: 0.981570, acc: 0.125000]\n",
      "674: [D loss: 0.673972, acc: 0.541016]  [A loss: 1.182858, acc: 0.027344]\n",
      "675: [D loss: 0.634600, acc: 0.646484]  [A loss: 1.001343, acc: 0.128906]\n",
      "676: [D loss: 0.660438, acc: 0.560547]  [A loss: 1.171808, acc: 0.042969]\n",
      "677: [D loss: 0.661517, acc: 0.574219]  [A loss: 1.045758, acc: 0.093750]\n",
      "678: [D loss: 0.700383, acc: 0.517578]  [A loss: 1.321217, acc: 0.003906]\n",
      "679: [D loss: 0.646523, acc: 0.630859]  [A loss: 0.990110, acc: 0.128906]\n",
      "680: [D loss: 0.676080, acc: 0.525391]  [A loss: 1.423127, acc: 0.007812]\n",
      "681: [D loss: 0.628169, acc: 0.662109]  [A loss: 0.888326, acc: 0.199219]\n",
      "682: [D loss: 0.688227, acc: 0.488281]  [A loss: 1.523922, acc: 0.000000]\n",
      "683: [D loss: 0.616889, acc: 0.707031]  [A loss: 0.873255, acc: 0.203125]\n",
      "684: [D loss: 0.685220, acc: 0.525391]  [A loss: 1.529806, acc: 0.000000]\n",
      "685: [D loss: 0.599185, acc: 0.703125]  [A loss: 0.894794, acc: 0.195312]\n",
      "686: [D loss: 0.687718, acc: 0.533203]  [A loss: 1.455616, acc: 0.000000]\n",
      "687: [D loss: 0.625251, acc: 0.666016]  [A loss: 0.898302, acc: 0.144531]\n",
      "688: [D loss: 0.663764, acc: 0.552734]  [A loss: 1.419023, acc: 0.000000]\n",
      "689: [D loss: 0.625403, acc: 0.646484]  [A loss: 0.961276, acc: 0.089844]\n",
      "690: [D loss: 0.708591, acc: 0.533203]  [A loss: 1.377089, acc: 0.000000]\n",
      "691: [D loss: 0.696017, acc: 0.501953]  [A loss: 0.969924, acc: 0.125000]\n",
      "692: [D loss: 0.735394, acc: 0.490234]  [A loss: 1.179287, acc: 0.011719]\n",
      "693: [D loss: 0.680281, acc: 0.523438]  [A loss: 0.951419, acc: 0.144531]\n",
      "694: [D loss: 0.669168, acc: 0.568359]  [A loss: 1.191948, acc: 0.015625]\n",
      "695: [D loss: 0.655527, acc: 0.591797]  [A loss: 1.129725, acc: 0.039062]\n",
      "696: [D loss: 0.654003, acc: 0.589844]  [A loss: 1.059677, acc: 0.125000]\n",
      "697: [D loss: 0.640914, acc: 0.605469]  [A loss: 1.444238, acc: 0.000000]\n",
      "698: [D loss: 0.608101, acc: 0.679688]  [A loss: 1.067089, acc: 0.109375]\n",
      "699: [D loss: 0.600984, acc: 0.623047]  [A loss: 1.429117, acc: 0.000000]\n",
      "700: [D loss: 0.599241, acc: 0.673828]  [A loss: 1.178436, acc: 0.031250]\n",
      "701: [D loss: 0.648369, acc: 0.585938]  [A loss: 1.715822, acc: 0.000000]\n",
      "702: [D loss: 0.603379, acc: 0.664062]  [A loss: 0.948740, acc: 0.167969]\n",
      "703: [D loss: 0.705788, acc: 0.533203]  [A loss: 1.760432, acc: 0.000000]\n",
      "704: [D loss: 0.640761, acc: 0.664062]  [A loss: 0.744777, acc: 0.453125]\n",
      "705: [D loss: 0.752770, acc: 0.507812]  [A loss: 1.589161, acc: 0.000000]\n",
      "706: [D loss: 0.616942, acc: 0.701172]  [A loss: 0.837641, acc: 0.300781]\n",
      "707: [D loss: 0.763336, acc: 0.484375]  [A loss: 1.411217, acc: 0.000000]\n",
      "708: [D loss: 0.682375, acc: 0.552734]  [A loss: 0.952267, acc: 0.132812]\n",
      "709: [D loss: 0.720560, acc: 0.503906]  [A loss: 1.149414, acc: 0.035156]\n",
      "710: [D loss: 0.670211, acc: 0.535156]  [A loss: 0.935143, acc: 0.164062]\n",
      "711: [D loss: 0.719904, acc: 0.500000]  [A loss: 1.179247, acc: 0.000000]\n",
      "712: [D loss: 0.706085, acc: 0.490234]  [A loss: 0.980060, acc: 0.085938]\n",
      "713: [D loss: 0.720093, acc: 0.474609]  [A loss: 1.105731, acc: 0.023438]\n",
      "714: [D loss: 0.694874, acc: 0.544922]  [A loss: 1.003392, acc: 0.105469]\n",
      "715: [D loss: 0.712727, acc: 0.498047]  [A loss: 1.033822, acc: 0.058594]\n",
      "716: [D loss: 0.653363, acc: 0.568359]  [A loss: 0.930182, acc: 0.128906]\n",
      "717: [D loss: 0.654303, acc: 0.572266]  [A loss: 1.099565, acc: 0.031250]\n",
      "718: [D loss: 0.627419, acc: 0.630859]  [A loss: 1.078583, acc: 0.058594]\n",
      "719: [D loss: 0.622518, acc: 0.642578]  [A loss: 1.188815, acc: 0.023438]\n",
      "720: [D loss: 0.610288, acc: 0.667969]  [A loss: 1.097265, acc: 0.062500]\n",
      "721: [D loss: 0.651620, acc: 0.615234]  [A loss: 1.310726, acc: 0.015625]\n",
      "722: [D loss: 0.617492, acc: 0.656250]  [A loss: 1.075028, acc: 0.062500]\n",
      "723: [D loss: 0.640420, acc: 0.595703]  [A loss: 1.436839, acc: 0.000000]\n",
      "724: [D loss: 0.609284, acc: 0.658203]  [A loss: 0.863735, acc: 0.222656]\n",
      "725: [D loss: 0.712104, acc: 0.511719]  [A loss: 1.761741, acc: 0.000000]\n",
      "726: [D loss: 0.655169, acc: 0.623047]  [A loss: 0.788471, acc: 0.375000]\n",
      "727: [D loss: 0.706838, acc: 0.537109]  [A loss: 1.380079, acc: 0.003906]\n",
      "728: [D loss: 0.614923, acc: 0.640625]  [A loss: 0.958557, acc: 0.160156]\n",
      "729: [D loss: 0.682538, acc: 0.550781]  [A loss: 1.357361, acc: 0.000000]\n",
      "730: [D loss: 0.600797, acc: 0.695312]  [A loss: 1.049313, acc: 0.058594]\n",
      "731: [D loss: 0.661545, acc: 0.548828]  [A loss: 1.431014, acc: 0.000000]\n",
      "732: [D loss: 0.593463, acc: 0.748047]  [A loss: 1.007541, acc: 0.097656]\n",
      "733: [D loss: 0.641733, acc: 0.570312]  [A loss: 1.397787, acc: 0.003906]\n",
      "734: [D loss: 0.578708, acc: 0.718750]  [A loss: 0.839435, acc: 0.308594]\n",
      "735: [D loss: 0.689192, acc: 0.533203]  [A loss: 1.488075, acc: 0.000000]\n",
      "736: [D loss: 0.641108, acc: 0.638672]  [A loss: 0.814402, acc: 0.339844]\n",
      "737: [D loss: 0.748274, acc: 0.509766]  [A loss: 1.608690, acc: 0.000000]\n",
      "738: [D loss: 0.680948, acc: 0.607422]  [A loss: 0.873613, acc: 0.171875]\n",
      "739: [D loss: 0.734426, acc: 0.478516]  [A loss: 1.443773, acc: 0.003906]\n",
      "740: [D loss: 0.689378, acc: 0.556641]  [A loss: 0.890258, acc: 0.183594]\n",
      "741: [D loss: 0.690776, acc: 0.539062]  [A loss: 1.303242, acc: 0.003906]\n",
      "742: [D loss: 0.635311, acc: 0.654297]  [A loss: 1.029408, acc: 0.089844]\n",
      "743: [D loss: 0.669352, acc: 0.533203]  [A loss: 1.379599, acc: 0.003906]\n",
      "744: [D loss: 0.608823, acc: 0.683594]  [A loss: 1.040085, acc: 0.089844]\n",
      "745: [D loss: 0.618984, acc: 0.621094]  [A loss: 1.402091, acc: 0.007812]\n",
      "746: [D loss: 0.627695, acc: 0.640625]  [A loss: 0.982738, acc: 0.113281]\n",
      "747: [D loss: 0.660600, acc: 0.564453]  [A loss: 1.400158, acc: 0.007812]\n",
      "748: [D loss: 0.612596, acc: 0.664062]  [A loss: 0.987341, acc: 0.097656]\n",
      "749: [D loss: 0.652209, acc: 0.570312]  [A loss: 1.536557, acc: 0.000000]\n",
      "750: [D loss: 0.629239, acc: 0.654297]  [A loss: 1.013865, acc: 0.125000]\n",
      "751: [D loss: 0.697527, acc: 0.576172]  [A loss: 1.501991, acc: 0.023438]\n",
      "752: [D loss: 0.653788, acc: 0.603516]  [A loss: 0.888991, acc: 0.234375]\n",
      "753: [D loss: 0.718663, acc: 0.548828]  [A loss: 1.431193, acc: 0.035156]\n",
      "754: [D loss: 0.635713, acc: 0.625000]  [A loss: 0.847265, acc: 0.296875]\n",
      "755: [D loss: 0.735641, acc: 0.509766]  [A loss: 1.366641, acc: 0.003906]\n",
      "756: [D loss: 0.641877, acc: 0.646484]  [A loss: 0.893337, acc: 0.195312]\n",
      "757: [D loss: 0.696952, acc: 0.505859]  [A loss: 1.219981, acc: 0.019531]\n",
      "758: [D loss: 0.667725, acc: 0.580078]  [A loss: 0.957506, acc: 0.113281]\n",
      "759: [D loss: 0.696909, acc: 0.496094]  [A loss: 1.181175, acc: 0.011719]\n",
      "760: [D loss: 0.692985, acc: 0.511719]  [A loss: 0.896881, acc: 0.230469]\n",
      "761: [D loss: 0.681022, acc: 0.556641]  [A loss: 1.127619, acc: 0.042969]\n",
      "762: [D loss: 0.648145, acc: 0.628906]  [A loss: 0.902973, acc: 0.171875]\n",
      "763: [D loss: 0.686119, acc: 0.531250]  [A loss: 1.184153, acc: 0.019531]\n",
      "764: [D loss: 0.664160, acc: 0.570312]  [A loss: 0.876018, acc: 0.199219]\n",
      "765: [D loss: 0.729895, acc: 0.492188]  [A loss: 1.355049, acc: 0.003906]\n",
      "766: [D loss: 0.679478, acc: 0.564453]  [A loss: 0.792234, acc: 0.347656]\n",
      "767: [D loss: 0.687727, acc: 0.539062]  [A loss: 1.123620, acc: 0.023438]\n",
      "768: [D loss: 0.655689, acc: 0.607422]  [A loss: 1.016678, acc: 0.101562]\n",
      "769: [D loss: 0.691874, acc: 0.529297]  [A loss: 1.396009, acc: 0.015625]\n",
      "770: [D loss: 0.661215, acc: 0.572266]  [A loss: 0.832919, acc: 0.339844]\n",
      "771: [D loss: 0.715037, acc: 0.503906]  [A loss: 1.563622, acc: 0.003906]\n",
      "772: [D loss: 0.641247, acc: 0.625000]  [A loss: 0.812343, acc: 0.375000]\n",
      "773: [D loss: 0.726130, acc: 0.482422]  [A loss: 1.341686, acc: 0.011719]\n",
      "774: [D loss: 0.684418, acc: 0.562500]  [A loss: 0.830700, acc: 0.277344]\n",
      "775: [D loss: 0.723153, acc: 0.515625]  [A loss: 1.209404, acc: 0.007812]\n",
      "776: [D loss: 0.640548, acc: 0.611328]  [A loss: 0.940081, acc: 0.113281]\n",
      "777: [D loss: 0.695757, acc: 0.535156]  [A loss: 1.216740, acc: 0.003906]\n",
      "778: [D loss: 0.624235, acc: 0.671875]  [A loss: 0.911358, acc: 0.160156]\n",
      "779: [D loss: 0.682318, acc: 0.523438]  [A loss: 1.143537, acc: 0.027344]\n",
      "780: [D loss: 0.651298, acc: 0.593750]  [A loss: 0.911724, acc: 0.144531]\n",
      "781: [D loss: 0.736728, acc: 0.480469]  [A loss: 1.237875, acc: 0.000000]\n",
      "782: [D loss: 0.688452, acc: 0.548828]  [A loss: 0.897033, acc: 0.160156]\n",
      "783: [D loss: 0.711176, acc: 0.501953]  [A loss: 1.289830, acc: 0.007812]\n",
      "784: [D loss: 0.678045, acc: 0.554688]  [A loss: 0.900490, acc: 0.187500]\n",
      "785: [D loss: 0.712724, acc: 0.496094]  [A loss: 1.252308, acc: 0.011719]\n",
      "786: [D loss: 0.665328, acc: 0.585938]  [A loss: 0.983958, acc: 0.132812]\n",
      "787: [D loss: 0.730104, acc: 0.494141]  [A loss: 1.168722, acc: 0.011719]\n",
      "788: [D loss: 0.703239, acc: 0.533203]  [A loss: 0.940356, acc: 0.136719]\n",
      "789: [D loss: 0.702948, acc: 0.525391]  [A loss: 1.157558, acc: 0.019531]\n",
      "790: [D loss: 0.682566, acc: 0.574219]  [A loss: 0.942559, acc: 0.136719]\n",
      "791: [D loss: 0.676067, acc: 0.574219]  [A loss: 1.144761, acc: 0.015625]\n",
      "792: [D loss: 0.653689, acc: 0.607422]  [A loss: 0.961077, acc: 0.144531]\n",
      "793: [D loss: 0.670691, acc: 0.613281]  [A loss: 1.349693, acc: 0.000000]\n",
      "794: [D loss: 0.624788, acc: 0.660156]  [A loss: 0.990910, acc: 0.113281]\n",
      "795: [D loss: 0.639259, acc: 0.605469]  [A loss: 1.295719, acc: 0.003906]\n",
      "796: [D loss: 0.629678, acc: 0.660156]  [A loss: 0.907687, acc: 0.191406]\n",
      "797: [D loss: 0.623348, acc: 0.623047]  [A loss: 1.384093, acc: 0.000000]\n",
      "798: [D loss: 0.567375, acc: 0.748047]  [A loss: 0.900942, acc: 0.242188]\n",
      "799: [D loss: 0.643157, acc: 0.583984]  [A loss: 1.578715, acc: 0.000000]\n",
      "800: [D loss: 0.592045, acc: 0.671875]  [A loss: 0.767592, acc: 0.488281]\n",
      "801: [D loss: 0.695964, acc: 0.541016]  [A loss: 1.449447, acc: 0.003906]\n",
      "802: [D loss: 0.568020, acc: 0.726562]  [A loss: 0.896941, acc: 0.281250]\n",
      "803: [D loss: 0.702266, acc: 0.537109]  [A loss: 1.189442, acc: 0.066406]\n",
      "804: [D loss: 0.703060, acc: 0.542969]  [A loss: 0.975960, acc: 0.132812]\n",
      "805: [D loss: 0.731394, acc: 0.484375]  [A loss: 1.071690, acc: 0.058594]\n",
      "806: [D loss: 0.683041, acc: 0.529297]  [A loss: 1.033793, acc: 0.093750]\n",
      "807: [D loss: 0.675941, acc: 0.535156]  [A loss: 1.169202, acc: 0.031250]\n",
      "808: [D loss: 0.642285, acc: 0.597656]  [A loss: 1.092030, acc: 0.042969]\n",
      "809: [D loss: 0.619089, acc: 0.613281]  [A loss: 1.119864, acc: 0.054688]\n",
      "810: [D loss: 0.620134, acc: 0.603516]  [A loss: 1.216842, acc: 0.019531]\n",
      "811: [D loss: 0.660744, acc: 0.560547]  [A loss: 1.208793, acc: 0.042969]\n",
      "812: [D loss: 0.683978, acc: 0.560547]  [A loss: 1.039762, acc: 0.085938]\n",
      "813: [D loss: 0.686465, acc: 0.519531]  [A loss: 1.302453, acc: 0.011719]\n",
      "814: [D loss: 0.685482, acc: 0.529297]  [A loss: 0.939309, acc: 0.160156]\n",
      "815: [D loss: 0.712967, acc: 0.535156]  [A loss: 1.352019, acc: 0.000000]\n",
      "816: [D loss: 0.684563, acc: 0.542969]  [A loss: 0.824606, acc: 0.285156]\n",
      "817: [D loss: 0.723790, acc: 0.513672]  [A loss: 1.389564, acc: 0.003906]\n",
      "818: [D loss: 0.682370, acc: 0.568359]  [A loss: 0.836206, acc: 0.230469]\n",
      "819: [D loss: 0.734555, acc: 0.500000]  [A loss: 1.393752, acc: 0.000000]\n",
      "820: [D loss: 0.664516, acc: 0.595703]  [A loss: 0.796673, acc: 0.308594]\n",
      "821: [D loss: 0.687810, acc: 0.529297]  [A loss: 1.270295, acc: 0.003906]\n",
      "822: [D loss: 0.633196, acc: 0.648438]  [A loss: 0.868289, acc: 0.140625]\n",
      "823: [D loss: 0.695530, acc: 0.535156]  [A loss: 1.257451, acc: 0.003906]\n",
      "824: [D loss: 0.630607, acc: 0.675781]  [A loss: 0.846057, acc: 0.242188]\n",
      "825: [D loss: 0.666049, acc: 0.564453]  [A loss: 1.234187, acc: 0.011719]\n",
      "826: [D loss: 0.625741, acc: 0.654297]  [A loss: 0.941277, acc: 0.140625]\n",
      "827: [D loss: 0.643052, acc: 0.593750]  [A loss: 1.166077, acc: 0.035156]\n",
      "828: [D loss: 0.654959, acc: 0.611328]  [A loss: 0.953823, acc: 0.132812]\n",
      "829: [D loss: 0.681661, acc: 0.564453]  [A loss: 1.229851, acc: 0.011719]\n",
      "830: [D loss: 0.694562, acc: 0.552734]  [A loss: 0.894819, acc: 0.175781]\n",
      "831: [D loss: 0.683003, acc: 0.533203]  [A loss: 1.220078, acc: 0.000000]\n",
      "832: [D loss: 0.635585, acc: 0.617188]  [A loss: 0.967646, acc: 0.125000]\n",
      "833: [D loss: 0.744427, acc: 0.515625]  [A loss: 1.330018, acc: 0.000000]\n",
      "834: [D loss: 0.685106, acc: 0.554688]  [A loss: 0.836949, acc: 0.265625]\n",
      "835: [D loss: 0.726047, acc: 0.527344]  [A loss: 1.239121, acc: 0.035156]\n",
      "836: [D loss: 0.669957, acc: 0.593750]  [A loss: 0.903815, acc: 0.187500]\n",
      "837: [D loss: 0.704828, acc: 0.529297]  [A loss: 1.218955, acc: 0.011719]\n",
      "838: [D loss: 0.665737, acc: 0.603516]  [A loss: 1.003222, acc: 0.117188]\n",
      "839: [D loss: 0.660654, acc: 0.609375]  [A loss: 1.231330, acc: 0.019531]\n",
      "840: [D loss: 0.638145, acc: 0.632812]  [A loss: 0.918099, acc: 0.222656]\n",
      "841: [D loss: 0.696189, acc: 0.527344]  [A loss: 1.509449, acc: 0.000000]\n",
      "842: [D loss: 0.658097, acc: 0.625000]  [A loss: 0.839519, acc: 0.320312]\n",
      "843: [D loss: 0.708687, acc: 0.548828]  [A loss: 1.512642, acc: 0.000000]\n",
      "844: [D loss: 0.624964, acc: 0.667969]  [A loss: 0.758952, acc: 0.390625]\n",
      "845: [D loss: 0.739533, acc: 0.507812]  [A loss: 1.534265, acc: 0.000000]\n",
      "846: [D loss: 0.631671, acc: 0.671875]  [A loss: 0.837393, acc: 0.277344]\n",
      "847: [D loss: 0.690718, acc: 0.548828]  [A loss: 1.307424, acc: 0.003906]\n",
      "848: [D loss: 0.623553, acc: 0.671875]  [A loss: 0.957229, acc: 0.121094]\n",
      "849: [D loss: 0.651134, acc: 0.550781]  [A loss: 1.323723, acc: 0.003906]\n",
      "850: [D loss: 0.613095, acc: 0.673828]  [A loss: 1.019986, acc: 0.062500]\n",
      "851: [D loss: 0.668121, acc: 0.537109]  [A loss: 1.360637, acc: 0.011719]\n",
      "852: [D loss: 0.602935, acc: 0.683594]  [A loss: 0.940546, acc: 0.140625]\n",
      "853: [D loss: 0.655528, acc: 0.537109]  [A loss: 1.465336, acc: 0.000000]\n",
      "854: [D loss: 0.602175, acc: 0.697266]  [A loss: 0.713470, acc: 0.511719]\n",
      "855: [D loss: 0.713289, acc: 0.501953]  [A loss: 1.414360, acc: 0.003906]\n",
      "856: [D loss: 0.634000, acc: 0.634766]  [A loss: 0.761442, acc: 0.402344]\n",
      "857: [D loss: 0.705305, acc: 0.515625]  [A loss: 1.402156, acc: 0.003906]\n",
      "858: [D loss: 0.663377, acc: 0.593750]  [A loss: 0.783410, acc: 0.382812]\n",
      "859: [D loss: 0.760968, acc: 0.529297]  [A loss: 1.210206, acc: 0.031250]\n",
      "860: [D loss: 0.676972, acc: 0.564453]  [A loss: 0.766185, acc: 0.402344]\n",
      "861: [D loss: 0.720085, acc: 0.513672]  [A loss: 1.104959, acc: 0.039062]\n",
      "862: [D loss: 0.679522, acc: 0.546875]  [A loss: 1.022955, acc: 0.085938]\n",
      "863: [D loss: 0.707411, acc: 0.539062]  [A loss: 1.221893, acc: 0.023438]\n",
      "864: [D loss: 0.682617, acc: 0.568359]  [A loss: 0.910801, acc: 0.218750]\n",
      "865: [D loss: 0.741939, acc: 0.537109]  [A loss: 1.186188, acc: 0.027344]\n",
      "866: [D loss: 0.669340, acc: 0.583984]  [A loss: 0.856446, acc: 0.230469]\n",
      "867: [D loss: 0.698923, acc: 0.542969]  [A loss: 1.216956, acc: 0.015625]\n",
      "868: [D loss: 0.679731, acc: 0.544922]  [A loss: 0.869501, acc: 0.238281]\n",
      "869: [D loss: 0.717452, acc: 0.498047]  [A loss: 1.168981, acc: 0.035156]\n",
      "870: [D loss: 0.679520, acc: 0.550781]  [A loss: 0.899528, acc: 0.167969]\n",
      "871: [D loss: 0.695077, acc: 0.523438]  [A loss: 1.187792, acc: 0.003906]\n",
      "872: [D loss: 0.659477, acc: 0.601562]  [A loss: 0.826366, acc: 0.273438]\n",
      "873: [D loss: 0.702694, acc: 0.511719]  [A loss: 1.340854, acc: 0.000000]\n",
      "874: [D loss: 0.604949, acc: 0.695312]  [A loss: 0.838829, acc: 0.222656]\n",
      "875: [D loss: 0.699347, acc: 0.523438]  [A loss: 1.453496, acc: 0.000000]\n",
      "876: [D loss: 0.638943, acc: 0.648438]  [A loss: 0.822603, acc: 0.308594]\n",
      "877: [D loss: 0.692673, acc: 0.517578]  [A loss: 1.390032, acc: 0.000000]\n",
      "878: [D loss: 0.625232, acc: 0.626953]  [A loss: 0.829563, acc: 0.257812]\n",
      "879: [D loss: 0.720678, acc: 0.527344]  [A loss: 1.313509, acc: 0.000000]\n",
      "880: [D loss: 0.624007, acc: 0.675781]  [A loss: 0.890052, acc: 0.261719]\n",
      "881: [D loss: 0.691450, acc: 0.570312]  [A loss: 1.258760, acc: 0.011719]\n",
      "882: [D loss: 0.656873, acc: 0.582031]  [A loss: 0.925546, acc: 0.164062]\n",
      "883: [D loss: 0.690679, acc: 0.550781]  [A loss: 1.337602, acc: 0.000000]\n",
      "884: [D loss: 0.652204, acc: 0.601562]  [A loss: 0.932045, acc: 0.152344]\n",
      "885: [D loss: 0.703885, acc: 0.550781]  [A loss: 1.286835, acc: 0.000000]\n",
      "886: [D loss: 0.633425, acc: 0.623047]  [A loss: 0.954000, acc: 0.160156]\n",
      "887: [D loss: 0.687206, acc: 0.515625]  [A loss: 1.352997, acc: 0.003906]\n",
      "888: [D loss: 0.634894, acc: 0.634766]  [A loss: 0.895647, acc: 0.195312]\n",
      "889: [D loss: 0.700427, acc: 0.509766]  [A loss: 1.267916, acc: 0.023438]\n",
      "890: [D loss: 0.632478, acc: 0.609375]  [A loss: 0.890193, acc: 0.207031]\n",
      "891: [D loss: 0.719350, acc: 0.484375]  [A loss: 1.340397, acc: 0.003906]\n",
      "892: [D loss: 0.666060, acc: 0.585938]  [A loss: 0.853643, acc: 0.253906]\n",
      "893: [D loss: 0.697922, acc: 0.535156]  [A loss: 1.248746, acc: 0.011719]\n",
      "894: [D loss: 0.655651, acc: 0.599609]  [A loss: 0.793747, acc: 0.324219]\n",
      "895: [D loss: 0.676505, acc: 0.544922]  [A loss: 1.188663, acc: 0.023438]\n",
      "896: [D loss: 0.651937, acc: 0.605469]  [A loss: 0.851238, acc: 0.230469]\n",
      "897: [D loss: 0.688748, acc: 0.548828]  [A loss: 1.151112, acc: 0.015625]\n",
      "898: [D loss: 0.629626, acc: 0.650391]  [A loss: 0.922731, acc: 0.136719]\n",
      "899: [D loss: 0.670579, acc: 0.591797]  [A loss: 1.144264, acc: 0.007812]\n",
      "900: [D loss: 0.639033, acc: 0.611328]  [A loss: 0.937161, acc: 0.144531]\n",
      "901: [D loss: 0.631267, acc: 0.605469]  [A loss: 1.122615, acc: 0.031250]\n",
      "902: [D loss: 0.620451, acc: 0.664062]  [A loss: 1.045982, acc: 0.042969]\n",
      "903: [D loss: 0.611333, acc: 0.646484]  [A loss: 1.180333, acc: 0.019531]\n",
      "904: [D loss: 0.617380, acc: 0.664062]  [A loss: 0.980520, acc: 0.144531]\n",
      "905: [D loss: 0.629192, acc: 0.628906]  [A loss: 1.241414, acc: 0.007812]\n",
      "906: [D loss: 0.613990, acc: 0.656250]  [A loss: 0.970434, acc: 0.128906]\n",
      "907: [D loss: 0.647382, acc: 0.599609]  [A loss: 1.301605, acc: 0.011719]\n",
      "908: [D loss: 0.600067, acc: 0.685547]  [A loss: 0.839889, acc: 0.343750]\n",
      "909: [D loss: 0.652371, acc: 0.595703]  [A loss: 1.375326, acc: 0.003906]\n",
      "910: [D loss: 0.600306, acc: 0.679688]  [A loss: 0.871397, acc: 0.324219]\n",
      "911: [D loss: 0.641990, acc: 0.591797]  [A loss: 1.331243, acc: 0.000000]\n",
      "912: [D loss: 0.586308, acc: 0.710938]  [A loss: 0.873138, acc: 0.308594]\n",
      "913: [D loss: 0.685132, acc: 0.535156]  [A loss: 1.425687, acc: 0.000000]\n",
      "914: [D loss: 0.607268, acc: 0.675781]  [A loss: 0.887589, acc: 0.183594]\n",
      "915: [D loss: 0.659821, acc: 0.541016]  [A loss: 1.427364, acc: 0.000000]\n",
      "916: [D loss: 0.622643, acc: 0.644531]  [A loss: 0.911975, acc: 0.199219]\n",
      "917: [D loss: 0.710412, acc: 0.509766]  [A loss: 1.394806, acc: 0.003906]\n",
      "918: [D loss: 0.662145, acc: 0.578125]  [A loss: 0.936442, acc: 0.238281]\n",
      "919: [D loss: 0.731834, acc: 0.568359]  [A loss: 1.224052, acc: 0.015625]\n",
      "920: [D loss: 0.685478, acc: 0.558594]  [A loss: 0.911437, acc: 0.175781]\n",
      "921: [D loss: 0.667085, acc: 0.578125]  [A loss: 1.149394, acc: 0.027344]\n",
      "922: [D loss: 0.632026, acc: 0.628906]  [A loss: 1.006233, acc: 0.105469]\n",
      "923: [D loss: 0.654441, acc: 0.568359]  [A loss: 1.156694, acc: 0.058594]\n",
      "924: [D loss: 0.629961, acc: 0.626953]  [A loss: 1.111729, acc: 0.054688]\n",
      "925: [D loss: 0.688736, acc: 0.542969]  [A loss: 1.143020, acc: 0.054688]\n",
      "926: [D loss: 0.646334, acc: 0.585938]  [A loss: 0.922832, acc: 0.246094]\n",
      "927: [D loss: 0.693913, acc: 0.537109]  [A loss: 1.234964, acc: 0.023438]\n",
      "928: [D loss: 0.656011, acc: 0.595703]  [A loss: 0.853065, acc: 0.281250]\n",
      "929: [D loss: 0.720060, acc: 0.488281]  [A loss: 1.342049, acc: 0.003906]\n",
      "930: [D loss: 0.681656, acc: 0.566406]  [A loss: 0.818042, acc: 0.320312]\n",
      "931: [D loss: 0.728831, acc: 0.486328]  [A loss: 1.397400, acc: 0.003906]\n",
      "932: [D loss: 0.675876, acc: 0.570312]  [A loss: 0.745143, acc: 0.429688]\n",
      "933: [D loss: 0.746116, acc: 0.468750]  [A loss: 1.488902, acc: 0.000000]\n",
      "934: [D loss: 0.707329, acc: 0.537109]  [A loss: 0.764561, acc: 0.367188]\n",
      "935: [D loss: 0.783681, acc: 0.500000]  [A loss: 1.405834, acc: 0.000000]\n",
      "936: [D loss: 0.650464, acc: 0.623047]  [A loss: 0.781812, acc: 0.410156]\n",
      "937: [D loss: 0.737370, acc: 0.486328]  [A loss: 1.310498, acc: 0.000000]\n",
      "938: [D loss: 0.647853, acc: 0.638672]  [A loss: 0.861698, acc: 0.214844]\n",
      "939: [D loss: 0.707183, acc: 0.529297]  [A loss: 1.219201, acc: 0.007812]\n",
      "940: [D loss: 0.679512, acc: 0.552734]  [A loss: 0.929916, acc: 0.113281]\n",
      "941: [D loss: 0.690310, acc: 0.531250]  [A loss: 1.115798, acc: 0.035156]\n",
      "942: [D loss: 0.689781, acc: 0.544922]  [A loss: 0.973366, acc: 0.117188]\n",
      "943: [D loss: 0.687778, acc: 0.507812]  [A loss: 1.006649, acc: 0.093750]\n",
      "944: [D loss: 0.679772, acc: 0.529297]  [A loss: 1.144458, acc: 0.042969]\n",
      "945: [D loss: 0.651405, acc: 0.599609]  [A loss: 0.922925, acc: 0.160156]\n",
      "946: [D loss: 0.677346, acc: 0.533203]  [A loss: 1.258254, acc: 0.000000]\n",
      "947: [D loss: 0.643467, acc: 0.611328]  [A loss: 0.921007, acc: 0.203125]\n",
      "948: [D loss: 0.697145, acc: 0.548828]  [A loss: 1.519692, acc: 0.003906]\n",
      "949: [D loss: 0.660472, acc: 0.607422]  [A loss: 0.770216, acc: 0.406250]\n",
      "950: [D loss: 0.725533, acc: 0.521484]  [A loss: 1.476050, acc: 0.003906]\n",
      "951: [D loss: 0.641048, acc: 0.648438]  [A loss: 0.754703, acc: 0.441406]\n",
      "952: [D loss: 0.727938, acc: 0.533203]  [A loss: 1.401788, acc: 0.003906]\n",
      "953: [D loss: 0.619219, acc: 0.677734]  [A loss: 0.874033, acc: 0.257812]\n",
      "954: [D loss: 0.686330, acc: 0.542969]  [A loss: 1.277201, acc: 0.000000]\n",
      "955: [D loss: 0.641321, acc: 0.630859]  [A loss: 0.931397, acc: 0.179688]\n",
      "956: [D loss: 0.749043, acc: 0.484375]  [A loss: 1.270708, acc: 0.019531]\n",
      "957: [D loss: 0.668526, acc: 0.597656]  [A loss: 0.880354, acc: 0.187500]\n",
      "958: [D loss: 0.690129, acc: 0.537109]  [A loss: 1.330747, acc: 0.003906]\n",
      "959: [D loss: 0.652628, acc: 0.625000]  [A loss: 0.951407, acc: 0.109375]\n",
      "960: [D loss: 0.683602, acc: 0.513672]  [A loss: 1.232797, acc: 0.011719]\n",
      "961: [D loss: 0.641336, acc: 0.630859]  [A loss: 0.937314, acc: 0.148438]\n",
      "962: [D loss: 0.683248, acc: 0.550781]  [A loss: 1.298850, acc: 0.015625]\n",
      "963: [D loss: 0.650775, acc: 0.599609]  [A loss: 1.001291, acc: 0.093750]\n",
      "964: [D loss: 0.668559, acc: 0.556641]  [A loss: 1.323992, acc: 0.015625]\n",
      "965: [D loss: 0.649313, acc: 0.613281]  [A loss: 0.943604, acc: 0.132812]\n",
      "966: [D loss: 0.684964, acc: 0.566406]  [A loss: 1.391142, acc: 0.000000]\n",
      "967: [D loss: 0.643208, acc: 0.623047]  [A loss: 0.942314, acc: 0.152344]\n",
      "968: [D loss: 0.654449, acc: 0.587891]  [A loss: 1.240765, acc: 0.015625]\n",
      "969: [D loss: 0.608752, acc: 0.705078]  [A loss: 0.968736, acc: 0.175781]\n",
      "970: [D loss: 0.684957, acc: 0.523438]  [A loss: 1.456475, acc: 0.011719]\n",
      "971: [D loss: 0.630722, acc: 0.636719]  [A loss: 0.806997, acc: 0.292969]\n",
      "972: [D loss: 0.729294, acc: 0.505859]  [A loss: 1.521113, acc: 0.000000]\n",
      "973: [D loss: 0.648647, acc: 0.625000]  [A loss: 0.921392, acc: 0.156250]\n",
      "974: [D loss: 0.730687, acc: 0.505859]  [A loss: 1.430926, acc: 0.003906]\n",
      "975: [D loss: 0.670251, acc: 0.597656]  [A loss: 0.930220, acc: 0.140625]\n",
      "976: [D loss: 0.694712, acc: 0.511719]  [A loss: 1.429479, acc: 0.000000]\n",
      "977: [D loss: 0.652574, acc: 0.632812]  [A loss: 0.962506, acc: 0.183594]\n",
      "978: [D loss: 0.747720, acc: 0.541016]  [A loss: 1.400666, acc: 0.003906]\n",
      "979: [D loss: 0.653235, acc: 0.623047]  [A loss: 0.962038, acc: 0.128906]\n",
      "980: [D loss: 0.698710, acc: 0.519531]  [A loss: 1.305327, acc: 0.015625]\n",
      "981: [D loss: 0.650867, acc: 0.607422]  [A loss: 0.978457, acc: 0.117188]\n",
      "982: [D loss: 0.672664, acc: 0.537109]  [A loss: 1.279064, acc: 0.023438]\n",
      "983: [D loss: 0.632167, acc: 0.605469]  [A loss: 0.930934, acc: 0.164062]\n",
      "984: [D loss: 0.705024, acc: 0.564453]  [A loss: 1.306769, acc: 0.003906]\n",
      "985: [D loss: 0.653542, acc: 0.597656]  [A loss: 0.853726, acc: 0.324219]\n",
      "986: [D loss: 0.723429, acc: 0.515625]  [A loss: 1.216520, acc: 0.039062]\n",
      "987: [D loss: 0.670136, acc: 0.570312]  [A loss: 0.824906, acc: 0.332031]\n",
      "988: [D loss: 0.673703, acc: 0.613281]  [A loss: 1.082287, acc: 0.062500]\n",
      "989: [D loss: 0.653148, acc: 0.580078]  [A loss: 0.864084, acc: 0.285156]\n",
      "990: [D loss: 0.660372, acc: 0.597656]  [A loss: 1.038778, acc: 0.058594]\n",
      "991: [D loss: 0.658822, acc: 0.564453]  [A loss: 0.896463, acc: 0.175781]\n",
      "992: [D loss: 0.635459, acc: 0.625000]  [A loss: 1.115106, acc: 0.066406]\n",
      "993: [D loss: 0.681175, acc: 0.572266]  [A loss: 1.147414, acc: 0.070312]\n",
      "994: [D loss: 0.711372, acc: 0.574219]  [A loss: 1.261543, acc: 0.019531]\n",
      "995: [D loss: 0.682452, acc: 0.568359]  [A loss: 0.977125, acc: 0.113281]\n",
      "996: [D loss: 0.675817, acc: 0.562500]  [A loss: 1.272013, acc: 0.015625]\n",
      "997: [D loss: 0.652631, acc: 0.595703]  [A loss: 1.003619, acc: 0.156250]\n",
      "998: [D loss: 0.679090, acc: 0.562500]  [A loss: 1.621799, acc: 0.003906]\n",
      "999: [D loss: 0.631860, acc: 0.658203]  [A loss: 0.719154, acc: 0.519531]\n"
     ]
    }
   ],
   "source": [
    "train_steps=1000\n",
    "batch_size=256\n",
    "save_interval=100\n",
    "\n",
    "def plot_images(save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "    filename = 'mnist.png'\n",
    "    if fake:\n",
    "        if noise is None:\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[samples, noise_dim])\n",
    "        else:\n",
    "            filename = \"mnist_%d.png\" % step\n",
    "        images = generator.predict(noise)\n",
    "    else:\n",
    "        i = np.random.randint(0, x_train.shape[0], samples)\n",
    "        images = x_train[i, :, :, :]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [28,28])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save2file:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "noise_input = None\n",
    "if save_interval>0:\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, noise_dim])\n",
    "    \n",
    "a_loss = np.zeros((train_steps,2))\n",
    "d_loss = np.zeros((train_steps,2))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    images_train = x_train[np.random.randint(0,x_train.shape[0], size=batch_size), :, :, :]\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, noise_dim])\n",
    "    images_fake = generator.predict(noise)\n",
    "    x = np.concatenate((images_train, images_fake))\n",
    "    y = np.ones([2*batch_size, 1])\n",
    "    y[batch_size:, :] = 0\n",
    "    discriminator.trainable = True\n",
    "    #discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy']) # slows down, why?\n",
    "    d_loss[i] = discriminator.train_on_batch(x, y)\n",
    "    discriminator.trainable = False\n",
    "    #adversarial.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy']) # slows down, why?\n",
    "    y = np.ones([batch_size, 1])\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, noise_dim])\n",
    "    a_loss[i] = adversarial.train_on_batch(noise, y)\n",
    "    log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[i,0], d_loss[i,1])\n",
    "    log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[i,0], a_loss[i,1])\n",
    "    print(log_mesg)\n",
    "    if save_interval>0:\n",
    "        if (i+1)%save_interval==0:\n",
    "            plot_images(save2file=True, samples=noise_input.shape[0],noise=noise_input, step=(i+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x306dd5dd8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title('Discriminator network')\n",
    "plt.plot(d_loss[:,:])\n",
    "plt.legend(discriminator.metrics_names)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Adversarial network')\n",
    "plt.plot(a_loss[:,:])\n",
    "plt.legend(adversarial.metrics_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/GAN_mnist.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "- GAN architecture not set in stone. Key idea: use \"discriminator\" to generate self-supervised training examples and use backpropagation to train the \"generator\" to trick the discriminator. \n",
    "- Other example: derive generator structure to generate camouflage patterns. Use detector to detect object. Backpropagation will change generator to minimize detection.\n",
    "- Games: generator to generate move, discriminator to decide whether move was good\n",
    "- Controllers: generator to generate controller \"phenotype\", discriminator to decide whether controller is good\n",
    "- Fine balance between improving generator and discriminator is needed and the approach does not necessarily need to succeed.\n",
    "- The amount of randomness you are using impacts the variety of the output you are getting. Below are three snapshots after 1000 iterations trained with 100, 10, and 1 random number as input:\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/mnist-gan-100/mnist_1000.png\" width=\"25%\"><img src=\"figs/mnist-gan-10/mnist_1000.png\" width=\"25%\">\n",
    "    <img src=\"figs/mnist-gan-1/mnist_1000.png\" width=\"25%\">\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
