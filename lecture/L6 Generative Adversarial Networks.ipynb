{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution reduces spatial information (2D images or 1D time series) to components by means of filters. The filters have usually been trained based on a large number of examples, describing features that these examples have in common. The result of the convolutional layers is then a representation that describes how much of each feature has been in the original image.\n",
    "- \"One-way streets\" such as max- and average pooling aside, this process could also be used the other way round to generate images using \"deconvolution\"\n",
    "- Lets assume a training set consists of single pixel images, which color is drawn from a 3D normal distribution centered around the color orange. The \"generator\" G() does not know this distribution, but turns uniform random input drawn from a distribution z into colors. The \"discriminator\" has learned the original distribution and can say \"hot\" (yes) or \"cold\" (no) and anything in between. The network can now use this feedback to improve the generator to better match the true distribution. At the same time, we can use knowledge of the fact that generated images are fake to improve the discriminator. \n",
    "\n",
    "Formally, the optimization problem is to \n",
    "\n",
    "maximize D performance while minimizing generator error\n",
    "\n",
    "min_G max_D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator is using deconvolution to turn a seed of noise into an image. Initially, the generated images are just noise, but will eventually be distributed as the training set. The generator itself is never trained by itself, which is indicated by all parameters colored in light gray, but only when part of an \"adversarial\" network, see below. As this network is trained, the generator gets better and better, here showing the output after zero and 2000 iterations. \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_generator.svg\" width=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a \"discriminator\" to decide whether an image is following the desired distribution, i.e. like the training set, or a generated image. We can train this discriminator by using two batches of equal size, one of which contains training images, the other generated images. The generated images are labeled by a zero, the training images by a one.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_discriminator.svg\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "As the generator gets better, the discriminator will get presented with better and better generated images, becoming more and more sophisticated. Here, the training set at 0 and after 2000 iterations of training the generator are shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GAN consists of a generator and a discriminator network that are connected in series and can be used individually. When training the GAN, the generator pretends that images it generates are real images, which leads to a loss if the discriminator detects the fake image. In order to prevent this loss from backpropagating into the discriminator, all discriminator parameters are locked during training, which is indicated by all parameters of the discriminator shown in light gray.  \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_adversarial.svg\" width=\"75%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "test.shape\n",
    "for I in range(4):\n",
    "    for J in range(4):\n",
    "        plt.subplot(4,4,I*4+J+1)\n",
    "        plt.imshow(test[I*4+J,:].reshape(10,10),cmap='gray')\n",
    "        plt.axis('off')\n",
    "#plt.tight_layout()        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DT (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_30 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_31 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_32 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.692658, acc: 0.521484]  [A loss: 0.917537, acc: 0.000000]\n",
      "1: [D loss: 0.659362, acc: 0.515625]  [A loss: 0.838945, acc: 0.000000]\n",
      "2: [D loss: 0.575393, acc: 0.828125]  [A loss: 0.895446, acc: 0.000000]\n",
      "3: [D loss: 0.408765, acc: 0.996094]  [A loss: 1.064919, acc: 0.000000]\n",
      "4: [D loss: 0.269294, acc: 1.000000]  [A loss: 0.052733, acc: 1.000000]\n",
      "5: [D loss: 0.350999, acc: 0.742188]  [A loss: 2.073725, acc: 0.000000]\n",
      "6: [D loss: 0.153294, acc: 0.996094]  [A loss: 0.147757, acc: 1.000000]\n",
      "7: [D loss: 0.100810, acc: 1.000000]  [A loss: 0.034032, acc: 1.000000]\n",
      "8: [D loss: 0.076391, acc: 1.000000]  [A loss: 0.027407, acc: 1.000000]\n",
      "9: [D loss: 0.062534, acc: 1.000000]  [A loss: 0.023081, acc: 1.000000]\n",
      "10: [D loss: 0.054244, acc: 1.000000]  [A loss: 0.018876, acc: 1.000000]\n",
      "11: [D loss: 0.049115, acc: 1.000000]  [A loss: 0.013538, acc: 1.000000]\n",
      "12: [D loss: 0.041011, acc: 1.000000]  [A loss: 0.011770, acc: 1.000000]\n",
      "13: [D loss: 0.034706, acc: 1.000000]  [A loss: 0.008463, acc: 1.000000]\n",
      "14: [D loss: 0.028586, acc: 1.000000]  [A loss: 0.011593, acc: 1.000000]\n",
      "15: [D loss: 0.027598, acc: 1.000000]  [A loss: 0.003658, acc: 1.000000]\n",
      "16: [D loss: 0.021372, acc: 1.000000]  [A loss: 0.002717, acc: 1.000000]\n",
      "17: [D loss: 0.018066, acc: 1.000000]  [A loss: 0.001283, acc: 1.000000]\n",
      "18: [D loss: 0.022297, acc: 0.996094]  [A loss: 0.000200, acc: 1.000000]\n",
      "19: [D loss: 0.015047, acc: 1.000000]  [A loss: 0.000960, acc: 1.000000]\n",
      "20: [D loss: 0.011107, acc: 1.000000]  [A loss: 0.000727, acc: 1.000000]\n",
      "21: [D loss: 0.009439, acc: 1.000000]  [A loss: 0.000664, acc: 1.000000]\n",
      "22: [D loss: 0.008350, acc: 1.000000]  [A loss: 0.000711, acc: 1.000000]\n",
      "23: [D loss: 0.009694, acc: 0.998047]  [A loss: 0.000013, acc: 1.000000]\n",
      "24: [D loss: 0.007837, acc: 1.000000]  [A loss: 0.000038, acc: 1.000000]\n",
      "25: [D loss: 0.005321, acc: 1.000000]  [A loss: 0.000107, acc: 1.000000]\n",
      "26: [D loss: 0.004762, acc: 1.000000]  [A loss: 0.000420, acc: 1.000000]\n",
      "27: [D loss: 0.003758, acc: 1.000000]  [A loss: 0.000201, acc: 1.000000]\n",
      "28: [D loss: 0.003615, acc: 1.000000]  [A loss: 0.000038, acc: 1.000000]\n",
      "29: [D loss: 0.002793, acc: 1.000000]  [A loss: 0.000110, acc: 1.000000]\n",
      "30: [D loss: 0.004094, acc: 1.000000]  [A loss: 0.000004, acc: 1.000000]\n",
      "31: [D loss: 0.003077, acc: 1.000000]  [A loss: 0.000004, acc: 1.000000]\n",
      "32: [D loss: 0.002021, acc: 1.000000]  [A loss: 0.000011, acc: 1.000000]\n",
      "33: [D loss: 0.001718, acc: 1.000000]  [A loss: 0.000028, acc: 1.000000]\n",
      "34: [D loss: 0.001667, acc: 1.000000]  [A loss: 0.000018, acc: 1.000000]\n",
      "35: [D loss: 0.001282, acc: 1.000000]  [A loss: 0.000024, acc: 1.000000]\n",
      "36: [D loss: 0.007039, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "37: [D loss: 0.003767, acc: 1.000000]  [A loss: 0.000003, acc: 1.000000]\n",
      "38: [D loss: 0.001440, acc: 1.000000]  [A loss: 0.000004, acc: 1.000000]\n",
      "39: [D loss: 0.001085, acc: 1.000000]  [A loss: 0.000004, acc: 1.000000]\n",
      "40: [D loss: 0.001001, acc: 1.000000]  [A loss: 0.000022, acc: 1.000000]\n",
      "41: [D loss: 0.000902, acc: 1.000000]  [A loss: 0.000051, acc: 1.000000]\n",
      "42: [D loss: 0.005581, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "43: [D loss: 0.007546, acc: 1.000000]  [A loss: 0.042124, acc: 0.984375]\n",
      "44: [D loss: 0.681487, acc: 0.730469]  [A loss: 17.904203, acc: 0.000000]\n",
      "45: [D loss: 0.032336, acc: 0.996094]  [A loss: 0.017129, acc: 1.000000]\n",
      "46: [D loss: 0.048705, acc: 0.990234]  [A loss: 0.009863, acc: 1.000000]\n",
      "47: [D loss: 0.055196, acc: 0.982422]  [A loss: 2.760859, acc: 0.109375]\n",
      "48: [D loss: 2.214879, acc: 0.544922]  [A loss: 27.144844, acc: 0.000000]\n",
      "49: [D loss: 2.779153, acc: 0.501953]  [A loss: 0.000005, acc: 1.000000]\n",
      "50: [D loss: 1.341895, acc: 0.570312]  [A loss: 0.681391, acc: 0.621094]\n",
      "51: [D loss: 0.498920, acc: 0.736328]  [A loss: 3.722745, acc: 0.000000]\n",
      "52: [D loss: 0.505367, acc: 0.720703]  [A loss: 5.403592, acc: 0.000000]\n",
      "53: [D loss: 0.289130, acc: 0.890625]  [A loss: 3.899866, acc: 0.003906]\n",
      "54: [D loss: 0.514827, acc: 0.687500]  [A loss: 7.215927, acc: 0.000000]\n",
      "55: [D loss: 0.186395, acc: 0.968750]  [A loss: 1.625209, acc: 0.113281]\n",
      "56: [D loss: 0.824700, acc: 0.587891]  [A loss: 8.754976, acc: 0.000000]\n",
      "57: [D loss: 0.412330, acc: 0.822266]  [A loss: 0.465158, acc: 0.789062]\n",
      "58: [D loss: 0.785551, acc: 0.564453]  [A loss: 7.144301, acc: 0.000000]\n",
      "59: [D loss: 0.246223, acc: 0.931641]  [A loss: 1.378774, acc: 0.171875]\n",
      "60: [D loss: 0.788931, acc: 0.546875]  [A loss: 6.693244, acc: 0.000000]\n",
      "61: [D loss: 0.226717, acc: 0.947266]  [A loss: 1.677144, acc: 0.058594]\n",
      "62: [D loss: 0.664142, acc: 0.585938]  [A loss: 5.396031, acc: 0.000000]\n",
      "63: [D loss: 0.188098, acc: 0.972656]  [A loss: 2.037536, acc: 0.042969]\n",
      "64: [D loss: 0.544648, acc: 0.648438]  [A loss: 4.632738, acc: 0.000000]\n",
      "65: [D loss: 0.200343, acc: 0.968750]  [A loss: 2.121984, acc: 0.015625]\n",
      "66: [D loss: 0.494883, acc: 0.699219]  [A loss: 4.604747, acc: 0.000000]\n",
      "67: [D loss: 0.241079, acc: 0.945312]  [A loss: 2.338616, acc: 0.023438]\n",
      "68: [D loss: 0.515357, acc: 0.710938]  [A loss: 4.942299, acc: 0.000000]\n",
      "69: [D loss: 0.283700, acc: 0.923828]  [A loss: 1.286290, acc: 0.199219]\n",
      "70: [D loss: 0.845429, acc: 0.572266]  [A loss: 6.354496, acc: 0.000000]\n",
      "71: [D loss: 0.620802, acc: 0.658203]  [A loss: 0.359936, acc: 0.894531]\n",
      "72: [D loss: 0.902718, acc: 0.554688]  [A loss: 3.618283, acc: 0.000000]\n",
      "73: [D loss: 0.283343, acc: 0.943359]  [A loss: 1.701868, acc: 0.062500]\n",
      "74: [D loss: 0.582352, acc: 0.652344]  [A loss: 4.022490, acc: 0.000000]\n",
      "75: [D loss: 0.304868, acc: 0.929688]  [A loss: 1.420344, acc: 0.097656]\n",
      "76: [D loss: 0.659366, acc: 0.625000]  [A loss: 4.287673, acc: 0.000000]\n",
      "77: [D loss: 0.383563, acc: 0.853516]  [A loss: 1.007910, acc: 0.300781]\n",
      "78: [D loss: 0.791183, acc: 0.566406]  [A loss: 4.229633, acc: 0.000000]\n",
      "79: [D loss: 0.375333, acc: 0.849609]  [A loss: 1.122488, acc: 0.234375]\n",
      "80: [D loss: 0.675784, acc: 0.595703]  [A loss: 3.468203, acc: 0.000000]\n",
      "81: [D loss: 0.334266, acc: 0.908203]  [A loss: 1.393360, acc: 0.070312]\n",
      "82: [D loss: 0.564192, acc: 0.656250]  [A loss: 3.328137, acc: 0.000000]\n",
      "83: [D loss: 0.346209, acc: 0.910156]  [A loss: 1.183181, acc: 0.156250]\n",
      "84: [D loss: 0.634680, acc: 0.607422]  [A loss: 3.479074, acc: 0.000000]\n",
      "85: [D loss: 0.383430, acc: 0.882812]  [A loss: 0.962237, acc: 0.328125]\n",
      "86: [D loss: 0.649072, acc: 0.585938]  [A loss: 3.211448, acc: 0.000000]\n",
      "87: [D loss: 0.318035, acc: 0.929688]  [A loss: 1.407197, acc: 0.050781]\n",
      "88: [D loss: 0.528484, acc: 0.658203]  [A loss: 2.953171, acc: 0.000000]\n",
      "89: [D loss: 0.334456, acc: 0.919922]  [A loss: 1.379854, acc: 0.062500]\n",
      "90: [D loss: 0.541690, acc: 0.646484]  [A loss: 3.110100, acc: 0.000000]\n",
      "91: [D loss: 0.357241, acc: 0.894531]  [A loss: 1.081128, acc: 0.218750]\n",
      "92: [D loss: 0.596338, acc: 0.597656]  [A loss: 3.216846, acc: 0.000000]\n",
      "93: [D loss: 0.382987, acc: 0.873047]  [A loss: 0.877624, acc: 0.371094]\n",
      "94: [D loss: 0.665311, acc: 0.582031]  [A loss: 3.066258, acc: 0.000000]\n",
      "95: [D loss: 0.414557, acc: 0.847656]  [A loss: 0.837925, acc: 0.386719]\n",
      "96: [D loss: 0.655730, acc: 0.560547]  [A loss: 2.724997, acc: 0.000000]\n",
      "97: [D loss: 0.382704, acc: 0.884766]  [A loss: 1.032582, acc: 0.218750]\n",
      "98: [D loss: 0.582157, acc: 0.583984]  [A loss: 2.669192, acc: 0.000000]\n",
      "99: [D loss: 0.366030, acc: 0.890625]  [A loss: 1.185737, acc: 0.085938]\n",
      "100: [D loss: 0.567524, acc: 0.583984]  [A loss: 2.659474, acc: 0.000000]\n",
      "101: [D loss: 0.383201, acc: 0.896484]  [A loss: 1.001473, acc: 0.246094]\n",
      "102: [D loss: 0.612946, acc: 0.556641]  [A loss: 2.749347, acc: 0.000000]\n",
      "103: [D loss: 0.388392, acc: 0.884766]  [A loss: 0.918922, acc: 0.261719]\n",
      "104: [D loss: 0.614788, acc: 0.570312]  [A loss: 2.561988, acc: 0.000000]\n",
      "105: [D loss: 0.409347, acc: 0.886719]  [A loss: 0.932848, acc: 0.214844]\n",
      "106: [D loss: 0.613760, acc: 0.566406]  [A loss: 2.580100, acc: 0.000000]\n",
      "107: [D loss: 0.411012, acc: 0.886719]  [A loss: 0.908825, acc: 0.230469]\n",
      "108: [D loss: 0.616017, acc: 0.550781]  [A loss: 2.450181, acc: 0.000000]\n",
      "109: [D loss: 0.427098, acc: 0.878906]  [A loss: 0.894987, acc: 0.269531]\n",
      "110: [D loss: 0.653689, acc: 0.537109]  [A loss: 2.466491, acc: 0.000000]\n",
      "111: [D loss: 0.404288, acc: 0.906250]  [A loss: 0.943584, acc: 0.226562]\n",
      "112: [D loss: 0.643681, acc: 0.529297]  [A loss: 2.434987, acc: 0.000000]\n",
      "113: [D loss: 0.436555, acc: 0.873047]  [A loss: 0.911045, acc: 0.246094]\n",
      "114: [D loss: 0.666541, acc: 0.533203]  [A loss: 2.281881, acc: 0.000000]\n",
      "115: [D loss: 0.433971, acc: 0.890625]  [A loss: 0.919664, acc: 0.234375]\n",
      "116: [D loss: 0.605174, acc: 0.542969]  [A loss: 2.177784, acc: 0.000000]\n",
      "117: [D loss: 0.440409, acc: 0.886719]  [A loss: 0.938201, acc: 0.214844]\n",
      "118: [D loss: 0.648387, acc: 0.523438]  [A loss: 2.299844, acc: 0.000000]\n",
      "119: [D loss: 0.485437, acc: 0.853516]  [A loss: 0.769148, acc: 0.429688]\n",
      "120: [D loss: 0.679897, acc: 0.513672]  [A loss: 2.134171, acc: 0.000000]\n",
      "121: [D loss: 0.491873, acc: 0.843750]  [A loss: 0.811928, acc: 0.320312]\n",
      "122: [D loss: 0.680940, acc: 0.521484]  [A loss: 2.048979, acc: 0.000000]\n",
      "123: [D loss: 0.475283, acc: 0.875000]  [A loss: 0.872724, acc: 0.261719]\n",
      "124: [D loss: 0.641716, acc: 0.533203]  [A loss: 2.016796, acc: 0.000000]\n",
      "125: [D loss: 0.461004, acc: 0.898438]  [A loss: 0.900720, acc: 0.214844]\n",
      "126: [D loss: 0.628243, acc: 0.533203]  [A loss: 2.144971, acc: 0.000000]\n",
      "127: [D loss: 0.477268, acc: 0.873047]  [A loss: 0.814443, acc: 0.347656]\n",
      "128: [D loss: 0.665894, acc: 0.517578]  [A loss: 2.113421, acc: 0.000000]\n",
      "129: [D loss: 0.483430, acc: 0.875000]  [A loss: 0.774355, acc: 0.398438]\n",
      "130: [D loss: 0.705166, acc: 0.519531]  [A loss: 2.068219, acc: 0.000000]\n",
      "131: [D loss: 0.493153, acc: 0.857422]  [A loss: 0.752702, acc: 0.414062]\n",
      "132: [D loss: 0.677940, acc: 0.521484]  [A loss: 1.890425, acc: 0.000000]\n",
      "133: [D loss: 0.503286, acc: 0.832031]  [A loss: 0.902598, acc: 0.203125]\n",
      "134: [D loss: 0.617631, acc: 0.529297]  [A loss: 1.883066, acc: 0.000000]\n",
      "135: [D loss: 0.500256, acc: 0.830078]  [A loss: 0.918910, acc: 0.203125]\n",
      "136: [D loss: 0.641767, acc: 0.527344]  [A loss: 2.185374, acc: 0.000000]\n",
      "137: [D loss: 0.479732, acc: 0.875000]  [A loss: 0.817851, acc: 0.292969]\n",
      "138: [D loss: 0.704140, acc: 0.515625]  [A loss: 2.309382, acc: 0.000000]\n",
      "139: [D loss: 0.530153, acc: 0.792969]  [A loss: 0.628749, acc: 0.632812]\n",
      "140: [D loss: 0.752670, acc: 0.500000]  [A loss: 1.895163, acc: 0.000000]\n",
      "141: [D loss: 0.508896, acc: 0.869141]  [A loss: 0.897220, acc: 0.214844]\n",
      "142: [D loss: 0.639580, acc: 0.521484]  [A loss: 1.853011, acc: 0.000000]\n",
      "143: [D loss: 0.521474, acc: 0.843750]  [A loss: 0.938337, acc: 0.140625]\n",
      "144: [D loss: 0.651431, acc: 0.525391]  [A loss: 2.020973, acc: 0.000000]\n",
      "145: [D loss: 0.511107, acc: 0.839844]  [A loss: 0.859209, acc: 0.218750]\n",
      "146: [D loss: 0.673564, acc: 0.515625]  [A loss: 2.116095, acc: 0.000000]\n",
      "147: [D loss: 0.516931, acc: 0.837891]  [A loss: 0.828237, acc: 0.296875]\n",
      "148: [D loss: 0.679606, acc: 0.505859]  [A loss: 2.117762, acc: 0.000000]\n",
      "149: [D loss: 0.516061, acc: 0.843750]  [A loss: 0.833946, acc: 0.277344]\n",
      "150: [D loss: 0.654555, acc: 0.527344]  [A loss: 1.995234, acc: 0.000000]\n",
      "151: [D loss: 0.524386, acc: 0.830078]  [A loss: 0.843585, acc: 0.242188]\n",
      "152: [D loss: 0.661510, acc: 0.527344]  [A loss: 1.955205, acc: 0.000000]\n",
      "153: [D loss: 0.519279, acc: 0.822266]  [A loss: 0.861700, acc: 0.242188]\n",
      "154: [D loss: 0.666218, acc: 0.513672]  [A loss: 2.119390, acc: 0.000000]\n",
      "155: [D loss: 0.531853, acc: 0.804688]  [A loss: 0.708652, acc: 0.468750]\n",
      "156: [D loss: 0.702286, acc: 0.503906]  [A loss: 1.974783, acc: 0.000000]\n",
      "157: [D loss: 0.535313, acc: 0.798828]  [A loss: 0.748029, acc: 0.414062]\n",
      "158: [D loss: 0.677832, acc: 0.503906]  [A loss: 1.710728, acc: 0.000000]\n",
      "159: [D loss: 0.517486, acc: 0.835938]  [A loss: 0.828256, acc: 0.273438]\n",
      "160: [D loss: 0.643206, acc: 0.521484]  [A loss: 1.784803, acc: 0.000000]\n",
      "161: [D loss: 0.512737, acc: 0.869141]  [A loss: 0.882146, acc: 0.187500]\n",
      "162: [D loss: 0.624896, acc: 0.537109]  [A loss: 1.920738, acc: 0.000000]\n",
      "163: [D loss: 0.503187, acc: 0.855469]  [A loss: 0.756805, acc: 0.410156]\n",
      "164: [D loss: 0.704519, acc: 0.517578]  [A loss: 2.264321, acc: 0.000000]\n",
      "165: [D loss: 0.550029, acc: 0.734375]  [A loss: 0.565396, acc: 0.781250]\n",
      "166: [D loss: 0.701653, acc: 0.507812]  [A loss: 1.500943, acc: 0.000000]\n",
      "167: [D loss: 0.517201, acc: 0.837891]  [A loss: 0.873792, acc: 0.199219]\n",
      "168: [D loss: 0.581881, acc: 0.566406]  [A loss: 1.485525, acc: 0.000000]\n",
      "169: [D loss: 0.514554, acc: 0.818359]  [A loss: 1.048639, acc: 0.082031]\n",
      "170: [D loss: 0.578911, acc: 0.578125]  [A loss: 1.874274, acc: 0.000000]\n",
      "171: [D loss: 0.475741, acc: 0.876953]  [A loss: 0.774021, acc: 0.390625]\n",
      "172: [D loss: 0.671122, acc: 0.527344]  [A loss: 2.504100, acc: 0.000000]\n",
      "173: [D loss: 0.526636, acc: 0.722656]  [A loss: 0.505849, acc: 0.863281]\n",
      "174: [D loss: 0.748712, acc: 0.500000]  [A loss: 1.576443, acc: 0.000000]\n",
      "175: [D loss: 0.489734, acc: 0.878906]  [A loss: 0.941375, acc: 0.156250]\n",
      "176: [D loss: 0.590409, acc: 0.591797]  [A loss: 1.608086, acc: 0.003906]\n",
      "177: [D loss: 0.498369, acc: 0.777344]  [A loss: 1.236989, acc: 0.019531]\n",
      "178: [D loss: 0.537709, acc: 0.650391]  [A loss: 1.987568, acc: 0.000000]\n",
      "179: [D loss: 0.483632, acc: 0.841797]  [A loss: 0.965865, acc: 0.167969]\n",
      "180: [D loss: 0.671461, acc: 0.542969]  [A loss: 2.829940, acc: 0.000000]\n",
      "181: [D loss: 0.602284, acc: 0.646484]  [A loss: 0.455231, acc: 0.871094]\n",
      "182: [D loss: 0.882828, acc: 0.500000]  [A loss: 1.566035, acc: 0.000000]\n",
      "183: [D loss: 0.535561, acc: 0.822266]  [A loss: 0.927648, acc: 0.128906]\n",
      "184: [D loss: 0.611773, acc: 0.564453]  [A loss: 1.588386, acc: 0.000000]\n",
      "185: [D loss: 0.529237, acc: 0.785156]  [A loss: 1.130623, acc: 0.054688]\n",
      "186: [D loss: 0.576047, acc: 0.607422]  [A loss: 1.939279, acc: 0.000000]\n",
      "187: [D loss: 0.493365, acc: 0.841797]  [A loss: 0.873842, acc: 0.273438]\n",
      "188: [D loss: 0.704879, acc: 0.529297]  [A loss: 2.408431, acc: 0.000000]\n",
      "189: [D loss: 0.569735, acc: 0.662109]  [A loss: 0.540265, acc: 0.789062]\n",
      "190: [D loss: 0.788853, acc: 0.503906]  [A loss: 1.564400, acc: 0.003906]\n",
      "191: [D loss: 0.536248, acc: 0.835938]  [A loss: 0.875554, acc: 0.203125]\n",
      "192: [D loss: 0.639596, acc: 0.542969]  [A loss: 1.602046, acc: 0.000000]\n",
      "193: [D loss: 0.533769, acc: 0.828125]  [A loss: 0.992366, acc: 0.140625]\n",
      "194: [D loss: 0.599017, acc: 0.595703]  [A loss: 1.778603, acc: 0.000000]\n",
      "195: [D loss: 0.527786, acc: 0.796875]  [A loss: 0.951164, acc: 0.132812]\n",
      "196: [D loss: 0.660165, acc: 0.550781]  [A loss: 2.099301, acc: 0.000000]\n",
      "197: [D loss: 0.561290, acc: 0.730469]  [A loss: 0.524125, acc: 0.828125]\n",
      "198: [D loss: 0.820484, acc: 0.511719]  [A loss: 1.744307, acc: 0.000000]\n",
      "199: [D loss: 0.569677, acc: 0.738281]  [A loss: 0.685916, acc: 0.500000]\n",
      "200: [D loss: 0.701347, acc: 0.525391]  [A loss: 1.481073, acc: 0.000000]\n",
      "201: [D loss: 0.559383, acc: 0.769531]  [A loss: 0.950685, acc: 0.152344]\n",
      "202: [D loss: 0.643681, acc: 0.572266]  [A loss: 1.503698, acc: 0.000000]\n",
      "203: [D loss: 0.556434, acc: 0.753906]  [A loss: 0.972371, acc: 0.144531]\n",
      "204: [D loss: 0.652748, acc: 0.572266]  [A loss: 1.724317, acc: 0.000000]\n",
      "205: [D loss: 0.556359, acc: 0.785156]  [A loss: 0.731647, acc: 0.460938]\n",
      "206: [D loss: 0.706648, acc: 0.521484]  [A loss: 1.898503, acc: 0.000000]\n",
      "207: [D loss: 0.576757, acc: 0.699219]  [A loss: 0.563260, acc: 0.781250]\n",
      "208: [D loss: 0.755841, acc: 0.515625]  [A loss: 1.623359, acc: 0.000000]\n",
      "209: [D loss: 0.585518, acc: 0.707031]  [A loss: 0.752889, acc: 0.414062]\n",
      "210: [D loss: 0.680683, acc: 0.544922]  [A loss: 1.469626, acc: 0.003906]\n",
      "211: [D loss: 0.559013, acc: 0.781250]  [A loss: 0.836744, acc: 0.265625]\n",
      "212: [D loss: 0.670003, acc: 0.568359]  [A loss: 1.647426, acc: 0.000000]\n",
      "213: [D loss: 0.563158, acc: 0.781250]  [A loss: 0.780369, acc: 0.351562]\n",
      "214: [D loss: 0.681718, acc: 0.523438]  [A loss: 1.680356, acc: 0.000000]\n",
      "215: [D loss: 0.577506, acc: 0.742188]  [A loss: 0.675006, acc: 0.562500]\n",
      "216: [D loss: 0.728175, acc: 0.507812]  [A loss: 1.729204, acc: 0.000000]\n",
      "217: [D loss: 0.563766, acc: 0.755859]  [A loss: 0.738985, acc: 0.445312]\n",
      "218: [D loss: 0.694358, acc: 0.527344]  [A loss: 1.598212, acc: 0.000000]\n",
      "219: [D loss: 0.557383, acc: 0.783203]  [A loss: 0.755000, acc: 0.406250]\n",
      "220: [D loss: 0.680716, acc: 0.542969]  [A loss: 1.571803, acc: 0.000000]\n",
      "221: [D loss: 0.576717, acc: 0.763672]  [A loss: 0.757241, acc: 0.390625]\n",
      "222: [D loss: 0.657092, acc: 0.539062]  [A loss: 1.546155, acc: 0.007812]\n",
      "223: [D loss: 0.551260, acc: 0.787109]  [A loss: 0.734733, acc: 0.480469]\n",
      "224: [D loss: 0.645449, acc: 0.587891]  [A loss: 1.598650, acc: 0.003906]\n",
      "225: [D loss: 0.562819, acc: 0.769531]  [A loss: 0.633644, acc: 0.640625]\n",
      "226: [D loss: 0.653854, acc: 0.562500]  [A loss: 1.521652, acc: 0.011719]\n",
      "227: [D loss: 0.567707, acc: 0.720703]  [A loss: 0.683051, acc: 0.535156]\n",
      "228: [D loss: 0.687257, acc: 0.542969]  [A loss: 1.901581, acc: 0.000000]\n",
      "229: [D loss: 0.556255, acc: 0.750000]  [A loss: 0.573319, acc: 0.730469]\n",
      "230: [D loss: 0.761789, acc: 0.507812]  [A loss: 2.026841, acc: 0.000000]\n",
      "231: [D loss: 0.573369, acc: 0.718750]  [A loss: 0.649524, acc: 0.644531]\n",
      "232: [D loss: 0.718119, acc: 0.519531]  [A loss: 1.942396, acc: 0.000000]\n",
      "233: [D loss: 0.542278, acc: 0.775391]  [A loss: 0.915369, acc: 0.226562]\n",
      "234: [D loss: 0.626671, acc: 0.568359]  [A loss: 1.934534, acc: 0.000000]\n",
      "235: [D loss: 0.529013, acc: 0.789062]  [A loss: 0.945333, acc: 0.230469]\n",
      "236: [D loss: 0.690971, acc: 0.552734]  [A loss: 2.036484, acc: 0.000000]\n",
      "237: [D loss: 0.564390, acc: 0.742188]  [A loss: 0.690199, acc: 0.546875]\n",
      "238: [D loss: 0.746770, acc: 0.509766]  [A loss: 1.942968, acc: 0.000000]\n",
      "239: [D loss: 0.579149, acc: 0.744141]  [A loss: 0.772074, acc: 0.398438]\n",
      "240: [D loss: 0.708415, acc: 0.521484]  [A loss: 1.785056, acc: 0.000000]\n",
      "241: [D loss: 0.556276, acc: 0.753906]  [A loss: 0.758966, acc: 0.421875]\n",
      "242: [D loss: 0.692723, acc: 0.523438]  [A loss: 1.665210, acc: 0.003906]\n",
      "243: [D loss: 0.570491, acc: 0.734375]  [A loss: 0.920313, acc: 0.230469]\n",
      "244: [D loss: 0.663367, acc: 0.564453]  [A loss: 1.691940, acc: 0.000000]\n",
      "245: [D loss: 0.580045, acc: 0.740234]  [A loss: 0.835119, acc: 0.351562]\n",
      "246: [D loss: 0.692849, acc: 0.546875]  [A loss: 1.905676, acc: 0.000000]\n",
      "247: [D loss: 0.578939, acc: 0.712891]  [A loss: 0.652291, acc: 0.597656]\n",
      "248: [D loss: 0.798819, acc: 0.521484]  [A loss: 1.932173, acc: 0.000000]\n",
      "249: [D loss: 0.621267, acc: 0.648438]  [A loss: 0.688929, acc: 0.523438]\n",
      "250: [D loss: 0.725489, acc: 0.509766]  [A loss: 1.426898, acc: 0.000000]\n",
      "251: [D loss: 0.598440, acc: 0.701172]  [A loss: 0.828670, acc: 0.324219]\n",
      "252: [D loss: 0.664859, acc: 0.550781]  [A loss: 1.428856, acc: 0.003906]\n",
      "253: [D loss: 0.608663, acc: 0.683594]  [A loss: 1.022612, acc: 0.082031]\n",
      "254: [D loss: 0.660100, acc: 0.587891]  [A loss: 1.429580, acc: 0.003906]\n",
      "255: [D loss: 0.628074, acc: 0.671875]  [A loss: 0.936834, acc: 0.195312]\n",
      "256: [D loss: 0.679494, acc: 0.554688]  [A loss: 1.591658, acc: 0.000000]\n",
      "257: [D loss: 0.616029, acc: 0.681641]  [A loss: 0.775345, acc: 0.367188]\n",
      "258: [D loss: 0.723875, acc: 0.525391]  [A loss: 1.867156, acc: 0.000000]\n",
      "259: [D loss: 0.641695, acc: 0.621094]  [A loss: 0.607991, acc: 0.695312]\n",
      "260: [D loss: 0.780874, acc: 0.509766]  [A loss: 1.572779, acc: 0.000000]\n",
      "261: [D loss: 0.621766, acc: 0.677734]  [A loss: 0.717875, acc: 0.433594]\n",
      "262: [D loss: 0.722981, acc: 0.517578]  [A loss: 1.421053, acc: 0.000000]\n",
      "263: [D loss: 0.626509, acc: 0.667969]  [A loss: 0.757910, acc: 0.398438]\n",
      "264: [D loss: 0.694283, acc: 0.527344]  [A loss: 1.363605, acc: 0.000000]\n",
      "265: [D loss: 0.610052, acc: 0.708984]  [A loss: 0.822116, acc: 0.285156]\n",
      "266: [D loss: 0.674023, acc: 0.541016]  [A loss: 1.390478, acc: 0.000000]\n",
      "267: [D loss: 0.617720, acc: 0.716797]  [A loss: 0.775315, acc: 0.382812]\n",
      "268: [D loss: 0.688627, acc: 0.517578]  [A loss: 1.478026, acc: 0.000000]\n",
      "269: [D loss: 0.625410, acc: 0.697266]  [A loss: 0.739497, acc: 0.414062]\n",
      "270: [D loss: 0.693038, acc: 0.542969]  [A loss: 1.514923, acc: 0.000000]\n",
      "271: [D loss: 0.602969, acc: 0.718750]  [A loss: 0.703687, acc: 0.488281]\n",
      "272: [D loss: 0.693068, acc: 0.517578]  [A loss: 1.520656, acc: 0.000000]\n",
      "273: [D loss: 0.619541, acc: 0.697266]  [A loss: 0.718218, acc: 0.476562]\n",
      "274: [D loss: 0.710065, acc: 0.529297]  [A loss: 1.521944, acc: 0.000000]\n",
      "275: [D loss: 0.625920, acc: 0.667969]  [A loss: 0.683019, acc: 0.550781]\n",
      "276: [D loss: 0.704147, acc: 0.517578]  [A loss: 1.425098, acc: 0.000000]\n",
      "277: [D loss: 0.613676, acc: 0.691406]  [A loss: 0.727251, acc: 0.437500]\n",
      "278: [D loss: 0.691277, acc: 0.544922]  [A loss: 1.412414, acc: 0.000000]\n",
      "279: [D loss: 0.618340, acc: 0.705078]  [A loss: 0.758267, acc: 0.378906]\n",
      "280: [D loss: 0.670862, acc: 0.537109]  [A loss: 1.336800, acc: 0.000000]\n",
      "281: [D loss: 0.617732, acc: 0.691406]  [A loss: 0.874951, acc: 0.210938]\n",
      "282: [D loss: 0.663092, acc: 0.566406]  [A loss: 1.437313, acc: 0.003906]\n",
      "283: [D loss: 0.617396, acc: 0.689453]  [A loss: 0.754945, acc: 0.414062]\n",
      "284: [D loss: 0.679937, acc: 0.537109]  [A loss: 1.547313, acc: 0.000000]\n",
      "285: [D loss: 0.617258, acc: 0.671875]  [A loss: 0.631174, acc: 0.695312]\n",
      "286: [D loss: 0.737467, acc: 0.501953]  [A loss: 1.521304, acc: 0.000000]\n",
      "287: [D loss: 0.627022, acc: 0.677734]  [A loss: 0.659904, acc: 0.570312]\n",
      "288: [D loss: 0.707383, acc: 0.513672]  [A loss: 1.445581, acc: 0.000000]\n",
      "289: [D loss: 0.634681, acc: 0.673828]  [A loss: 0.700521, acc: 0.527344]\n",
      "290: [D loss: 0.684235, acc: 0.535156]  [A loss: 1.344999, acc: 0.000000]\n",
      "291: [D loss: 0.614325, acc: 0.736328]  [A loss: 0.763460, acc: 0.339844]\n",
      "292: [D loss: 0.663076, acc: 0.537109]  [A loss: 1.329999, acc: 0.000000]\n",
      "293: [D loss: 0.619025, acc: 0.683594]  [A loss: 0.782733, acc: 0.339844]\n",
      "294: [D loss: 0.685186, acc: 0.521484]  [A loss: 1.457472, acc: 0.000000]\n",
      "295: [D loss: 0.614858, acc: 0.708984]  [A loss: 0.732352, acc: 0.433594]\n",
      "296: [D loss: 0.694569, acc: 0.517578]  [A loss: 1.461954, acc: 0.000000]\n",
      "297: [D loss: 0.629776, acc: 0.656250]  [A loss: 0.671038, acc: 0.578125]\n",
      "298: [D loss: 0.709433, acc: 0.517578]  [A loss: 1.585371, acc: 0.000000]\n",
      "299: [D loss: 0.629690, acc: 0.658203]  [A loss: 0.598303, acc: 0.750000]\n",
      "300: [D loss: 0.739863, acc: 0.505859]  [A loss: 1.441329, acc: 0.000000]\n",
      "301: [D loss: 0.642359, acc: 0.623047]  [A loss: 0.646792, acc: 0.605469]\n",
      "302: [D loss: 0.718114, acc: 0.517578]  [A loss: 1.311027, acc: 0.000000]\n",
      "303: [D loss: 0.632034, acc: 0.691406]  [A loss: 0.749216, acc: 0.351562]\n",
      "304: [D loss: 0.674536, acc: 0.539062]  [A loss: 1.206232, acc: 0.007812]\n",
      "305: [D loss: 0.622602, acc: 0.701172]  [A loss: 0.909542, acc: 0.179688]\n",
      "306: [D loss: 0.674893, acc: 0.550781]  [A loss: 1.321631, acc: 0.003906]\n",
      "307: [D loss: 0.623256, acc: 0.681641]  [A loss: 0.805329, acc: 0.320312]\n",
      "308: [D loss: 0.682369, acc: 0.541016]  [A loss: 1.557491, acc: 0.000000]\n",
      "309: [D loss: 0.629896, acc: 0.667969]  [A loss: 0.620546, acc: 0.695312]\n",
      "310: [D loss: 0.768733, acc: 0.507812]  [A loss: 1.614887, acc: 0.000000]\n",
      "311: [D loss: 0.676149, acc: 0.570312]  [A loss: 0.570275, acc: 0.789062]\n",
      "312: [D loss: 0.733622, acc: 0.494141]  [A loss: 1.196986, acc: 0.000000]\n",
      "313: [D loss: 0.631505, acc: 0.691406]  [A loss: 0.774284, acc: 0.339844]\n",
      "314: [D loss: 0.685189, acc: 0.546875]  [A loss: 1.140214, acc: 0.011719]\n",
      "315: [D loss: 0.632924, acc: 0.673828]  [A loss: 0.797702, acc: 0.292969]\n",
      "316: [D loss: 0.678014, acc: 0.554688]  [A loss: 1.208215, acc: 0.019531]\n",
      "317: [D loss: 0.636924, acc: 0.648438]  [A loss: 0.820817, acc: 0.289062]\n",
      "318: [D loss: 0.693848, acc: 0.546875]  [A loss: 1.322851, acc: 0.003906]\n",
      "319: [D loss: 0.640240, acc: 0.646484]  [A loss: 0.792321, acc: 0.343750]\n",
      "320: [D loss: 0.700621, acc: 0.533203]  [A loss: 1.432997, acc: 0.000000]\n",
      "321: [D loss: 0.650095, acc: 0.611328]  [A loss: 0.632137, acc: 0.691406]\n",
      "322: [D loss: 0.732857, acc: 0.507812]  [A loss: 1.465792, acc: 0.000000]\n",
      "323: [D loss: 0.655485, acc: 0.599609]  [A loss: 0.634506, acc: 0.652344]\n",
      "324: [D loss: 0.720387, acc: 0.507812]  [A loss: 1.313646, acc: 0.000000]\n",
      "325: [D loss: 0.652450, acc: 0.597656]  [A loss: 0.738453, acc: 0.437500]\n",
      "326: [D loss: 0.688359, acc: 0.525391]  [A loss: 1.200039, acc: 0.011719]\n",
      "327: [D loss: 0.639879, acc: 0.667969]  [A loss: 0.739323, acc: 0.402344]\n",
      "328: [D loss: 0.706210, acc: 0.513672]  [A loss: 1.283734, acc: 0.000000]\n",
      "329: [D loss: 0.653120, acc: 0.646484]  [A loss: 0.750018, acc: 0.398438]\n",
      "330: [D loss: 0.709743, acc: 0.515625]  [A loss: 1.294286, acc: 0.003906]\n",
      "331: [D loss: 0.643737, acc: 0.664062]  [A loss: 0.704847, acc: 0.507812]\n",
      "332: [D loss: 0.700417, acc: 0.515625]  [A loss: 1.301008, acc: 0.000000]\n",
      "333: [D loss: 0.650812, acc: 0.642578]  [A loss: 0.717258, acc: 0.500000]\n",
      "334: [D loss: 0.698936, acc: 0.519531]  [A loss: 1.262163, acc: 0.000000]\n",
      "335: [D loss: 0.666457, acc: 0.613281]  [A loss: 0.702558, acc: 0.480469]\n",
      "336: [D loss: 0.706802, acc: 0.519531]  [A loss: 1.261216, acc: 0.007812]\n",
      "337: [D loss: 0.651857, acc: 0.638672]  [A loss: 0.714695, acc: 0.492188]\n",
      "338: [D loss: 0.707664, acc: 0.515625]  [A loss: 1.200218, acc: 0.000000]\n",
      "339: [D loss: 0.641032, acc: 0.662109]  [A loss: 0.754154, acc: 0.402344]\n",
      "340: [D loss: 0.688735, acc: 0.523438]  [A loss: 1.246907, acc: 0.000000]\n",
      "341: [D loss: 0.649467, acc: 0.644531]  [A loss: 0.758723, acc: 0.359375]\n",
      "342: [D loss: 0.689116, acc: 0.539062]  [A loss: 1.260324, acc: 0.000000]\n",
      "343: [D loss: 0.653282, acc: 0.638672]  [A loss: 0.684344, acc: 0.539062]\n",
      "344: [D loss: 0.728421, acc: 0.505859]  [A loss: 1.349389, acc: 0.000000]\n",
      "345: [D loss: 0.650344, acc: 0.607422]  [A loss: 0.649997, acc: 0.605469]\n",
      "346: [D loss: 0.711886, acc: 0.517578]  [A loss: 1.243835, acc: 0.003906]\n",
      "347: [D loss: 0.658520, acc: 0.613281]  [A loss: 0.689129, acc: 0.542969]\n",
      "348: [D loss: 0.706752, acc: 0.505859]  [A loss: 1.184013, acc: 0.000000]\n",
      "349: [D loss: 0.657042, acc: 0.607422]  [A loss: 0.726864, acc: 0.390625]\n",
      "350: [D loss: 0.692304, acc: 0.513672]  [A loss: 1.127742, acc: 0.007812]\n",
      "351: [D loss: 0.644291, acc: 0.640625]  [A loss: 0.769112, acc: 0.316406]\n",
      "352: [D loss: 0.692006, acc: 0.539062]  [A loss: 1.183813, acc: 0.000000]\n",
      "353: [D loss: 0.656367, acc: 0.617188]  [A loss: 0.716063, acc: 0.445312]\n",
      "354: [D loss: 0.699666, acc: 0.525391]  [A loss: 1.234257, acc: 0.007812]\n",
      "355: [D loss: 0.643147, acc: 0.667969]  [A loss: 0.675602, acc: 0.539062]\n",
      "356: [D loss: 0.712408, acc: 0.523438]  [A loss: 1.290806, acc: 0.000000]\n",
      "357: [D loss: 0.656310, acc: 0.615234]  [A loss: 0.706229, acc: 0.503906]\n",
      "358: [D loss: 0.707866, acc: 0.525391]  [A loss: 1.315091, acc: 0.000000]\n",
      "359: [D loss: 0.665717, acc: 0.582031]  [A loss: 0.672787, acc: 0.558594]\n",
      "360: [D loss: 0.723975, acc: 0.503906]  [A loss: 1.207229, acc: 0.003906]\n",
      "361: [D loss: 0.654284, acc: 0.638672]  [A loss: 0.673669, acc: 0.589844]\n",
      "362: [D loss: 0.691344, acc: 0.519531]  [A loss: 1.058048, acc: 0.023438]\n",
      "363: [D loss: 0.672748, acc: 0.580078]  [A loss: 0.814534, acc: 0.199219]\n",
      "364: [D loss: 0.678151, acc: 0.544922]  [A loss: 1.095127, acc: 0.015625]\n",
      "365: [D loss: 0.652622, acc: 0.646484]  [A loss: 0.798176, acc: 0.296875]\n",
      "366: [D loss: 0.672015, acc: 0.554688]  [A loss: 1.080043, acc: 0.023438]\n",
      "367: [D loss: 0.658666, acc: 0.636719]  [A loss: 0.762338, acc: 0.390625]\n",
      "368: [D loss: 0.682184, acc: 0.529297]  [A loss: 1.292867, acc: 0.000000]\n",
      "369: [D loss: 0.659908, acc: 0.585938]  [A loss: 0.641847, acc: 0.675781]\n",
      "370: [D loss: 0.710909, acc: 0.509766]  [A loss: 1.348473, acc: 0.000000]\n",
      "371: [D loss: 0.660394, acc: 0.626953]  [A loss: 0.620382, acc: 0.691406]\n",
      "372: [D loss: 0.735095, acc: 0.509766]  [A loss: 1.201365, acc: 0.003906]\n",
      "373: [D loss: 0.661086, acc: 0.617188]  [A loss: 0.737698, acc: 0.417969]\n",
      "374: [D loss: 0.688230, acc: 0.542969]  [A loss: 1.065151, acc: 0.011719]\n",
      "375: [D loss: 0.664233, acc: 0.621094]  [A loss: 0.756164, acc: 0.367188]\n",
      "376: [D loss: 0.678601, acc: 0.537109]  [A loss: 1.088348, acc: 0.011719]\n",
      "377: [D loss: 0.644635, acc: 0.642578]  [A loss: 0.819703, acc: 0.230469]\n",
      "378: [D loss: 0.688858, acc: 0.550781]  [A loss: 1.165658, acc: 0.003906]\n",
      "379: [D loss: 0.662691, acc: 0.607422]  [A loss: 0.808342, acc: 0.257812]\n",
      "380: [D loss: 0.676267, acc: 0.556641]  [A loss: 1.134188, acc: 0.015625]\n",
      "381: [D loss: 0.654181, acc: 0.615234]  [A loss: 0.804064, acc: 0.281250]\n",
      "382: [D loss: 0.697120, acc: 0.539062]  [A loss: 1.311406, acc: 0.000000]\n",
      "383: [D loss: 0.661198, acc: 0.607422]  [A loss: 0.629059, acc: 0.714844]\n",
      "384: [D loss: 0.722911, acc: 0.507812]  [A loss: 1.314015, acc: 0.000000]\n",
      "385: [D loss: 0.662809, acc: 0.595703]  [A loss: 0.634923, acc: 0.703125]\n",
      "386: [D loss: 0.722813, acc: 0.501953]  [A loss: 1.142555, acc: 0.000000]\n",
      "387: [D loss: 0.657221, acc: 0.634766]  [A loss: 0.736824, acc: 0.410156]\n",
      "388: [D loss: 0.675783, acc: 0.542969]  [A loss: 1.019676, acc: 0.035156]\n",
      "389: [D loss: 0.661827, acc: 0.595703]  [A loss: 0.827825, acc: 0.203125]\n",
      "390: [D loss: 0.665747, acc: 0.558594]  [A loss: 1.008144, acc: 0.031250]\n",
      "391: [D loss: 0.659365, acc: 0.619141]  [A loss: 0.863561, acc: 0.128906]\n",
      "392: [D loss: 0.674582, acc: 0.572266]  [A loss: 1.054369, acc: 0.027344]\n",
      "393: [D loss: 0.653109, acc: 0.628906]  [A loss: 0.810973, acc: 0.281250]\n",
      "394: [D loss: 0.705458, acc: 0.537109]  [A loss: 1.263138, acc: 0.000000]\n",
      "395: [D loss: 0.643149, acc: 0.638672]  [A loss: 0.657025, acc: 0.660156]\n",
      "396: [D loss: 0.712476, acc: 0.523438]  [A loss: 1.248474, acc: 0.000000]\n",
      "397: [D loss: 0.660671, acc: 0.599609]  [A loss: 0.654631, acc: 0.625000]\n",
      "398: [D loss: 0.709996, acc: 0.517578]  [A loss: 1.161785, acc: 0.011719]\n",
      "399: [D loss: 0.675660, acc: 0.554688]  [A loss: 0.687976, acc: 0.558594]\n",
      "400: [D loss: 0.709607, acc: 0.515625]  [A loss: 1.094553, acc: 0.011719]\n",
      "401: [D loss: 0.662421, acc: 0.625000]  [A loss: 0.694747, acc: 0.531250]\n",
      "402: [D loss: 0.691846, acc: 0.527344]  [A loss: 1.006615, acc: 0.042969]\n",
      "403: [D loss: 0.660704, acc: 0.628906]  [A loss: 0.782784, acc: 0.300781]\n",
      "404: [D loss: 0.684787, acc: 0.554688]  [A loss: 0.994129, acc: 0.050781]\n",
      "405: [D loss: 0.671836, acc: 0.578125]  [A loss: 0.807357, acc: 0.238281]\n",
      "406: [D loss: 0.674294, acc: 0.544922]  [A loss: 1.096950, acc: 0.000000]\n",
      "407: [D loss: 0.669970, acc: 0.591797]  [A loss: 0.827166, acc: 0.226562]\n",
      "408: [D loss: 0.687034, acc: 0.556641]  [A loss: 1.085439, acc: 0.007812]\n",
      "409: [D loss: 0.662784, acc: 0.605469]  [A loss: 0.729822, acc: 0.417969]\n",
      "410: [D loss: 0.703369, acc: 0.523438]  [A loss: 1.257684, acc: 0.000000]\n",
      "411: [D loss: 0.673023, acc: 0.578125]  [A loss: 0.634947, acc: 0.628906]\n",
      "412: [D loss: 0.723060, acc: 0.515625]  [A loss: 1.161013, acc: 0.011719]\n",
      "413: [D loss: 0.660926, acc: 0.601562]  [A loss: 0.712772, acc: 0.472656]\n",
      "414: [D loss: 0.701133, acc: 0.527344]  [A loss: 1.096140, acc: 0.007812]\n",
      "415: [D loss: 0.664525, acc: 0.603516]  [A loss: 0.736602, acc: 0.425781]\n",
      "416: [D loss: 0.699006, acc: 0.544922]  [A loss: 1.089562, acc: 0.011719]\n",
      "417: [D loss: 0.656618, acc: 0.632812]  [A loss: 0.779191, acc: 0.324219]\n",
      "418: [D loss: 0.697012, acc: 0.541016]  [A loss: 1.059408, acc: 0.015625]\n",
      "419: [D loss: 0.672046, acc: 0.597656]  [A loss: 0.794091, acc: 0.253906]\n",
      "420: [D loss: 0.686082, acc: 0.552734]  [A loss: 0.958278, acc: 0.093750]\n",
      "421: [D loss: 0.668260, acc: 0.593750]  [A loss: 0.889255, acc: 0.121094]\n",
      "422: [D loss: 0.671451, acc: 0.574219]  [A loss: 1.001777, acc: 0.039062]\n",
      "423: [D loss: 0.667387, acc: 0.603516]  [A loss: 0.932624, acc: 0.074219]\n",
      "424: [D loss: 0.680399, acc: 0.562500]  [A loss: 1.037278, acc: 0.039062]\n",
      "425: [D loss: 0.671522, acc: 0.591797]  [A loss: 0.921657, acc: 0.085938]\n",
      "426: [D loss: 0.676421, acc: 0.582031]  [A loss: 1.103316, acc: 0.011719]\n",
      "427: [D loss: 0.674926, acc: 0.566406]  [A loss: 0.829472, acc: 0.253906]\n",
      "428: [D loss: 0.691949, acc: 0.544922]  [A loss: 1.193221, acc: 0.007812]\n",
      "429: [D loss: 0.663476, acc: 0.607422]  [A loss: 0.660317, acc: 0.628906]\n",
      "430: [D loss: 0.734588, acc: 0.513672]  [A loss: 1.410416, acc: 0.000000]\n",
      "431: [D loss: 0.686223, acc: 0.539062]  [A loss: 0.542851, acc: 0.894531]\n",
      "432: [D loss: 0.754742, acc: 0.503906]  [A loss: 1.111597, acc: 0.000000]\n",
      "433: [D loss: 0.679341, acc: 0.560547]  [A loss: 0.763506, acc: 0.351562]\n",
      "434: [D loss: 0.690666, acc: 0.533203]  [A loss: 0.944958, acc: 0.050781]\n",
      "435: [D loss: 0.674162, acc: 0.570312]  [A loss: 0.804667, acc: 0.261719]\n",
      "436: [D loss: 0.675067, acc: 0.568359]  [A loss: 0.887525, acc: 0.125000]\n",
      "437: [D loss: 0.675582, acc: 0.583984]  [A loss: 0.858136, acc: 0.144531]\n",
      "438: [D loss: 0.672256, acc: 0.585938]  [A loss: 0.939903, acc: 0.070312]\n",
      "439: [D loss: 0.679652, acc: 0.562500]  [A loss: 0.886989, acc: 0.058594]\n",
      "440: [D loss: 0.675424, acc: 0.552734]  [A loss: 0.985382, acc: 0.039062]\n",
      "441: [D loss: 0.661758, acc: 0.619141]  [A loss: 0.866621, acc: 0.136719]\n",
      "442: [D loss: 0.680018, acc: 0.558594]  [A loss: 1.050267, acc: 0.023438]\n",
      "443: [D loss: 0.662466, acc: 0.591797]  [A loss: 0.761544, acc: 0.332031]\n",
      "444: [D loss: 0.681545, acc: 0.564453]  [A loss: 1.149052, acc: 0.011719]\n",
      "445: [D loss: 0.668860, acc: 0.562500]  [A loss: 0.707206, acc: 0.472656]\n",
      "446: [D loss: 0.719304, acc: 0.523438]  [A loss: 1.299649, acc: 0.000000]\n",
      "447: [D loss: 0.679787, acc: 0.562500]  [A loss: 0.614246, acc: 0.718750]\n",
      "448: [D loss: 0.708608, acc: 0.525391]  [A loss: 1.085294, acc: 0.003906]\n",
      "449: [D loss: 0.667022, acc: 0.591797]  [A loss: 0.696786, acc: 0.527344]\n",
      "450: [D loss: 0.709338, acc: 0.523438]  [A loss: 1.019718, acc: 0.019531]\n",
      "451: [D loss: 0.659791, acc: 0.613281]  [A loss: 0.750344, acc: 0.355469]\n",
      "452: [D loss: 0.685973, acc: 0.541016]  [A loss: 0.957975, acc: 0.074219]\n",
      "453: [D loss: 0.662703, acc: 0.638672]  [A loss: 0.809657, acc: 0.230469]\n",
      "454: [D loss: 0.683472, acc: 0.562500]  [A loss: 0.997960, acc: 0.050781]\n",
      "455: [D loss: 0.666062, acc: 0.599609]  [A loss: 0.851556, acc: 0.187500]\n",
      "456: [D loss: 0.671252, acc: 0.583984]  [A loss: 1.016615, acc: 0.011719]\n",
      "457: [D loss: 0.670986, acc: 0.550781]  [A loss: 0.793520, acc: 0.265625]\n",
      "458: [D loss: 0.681692, acc: 0.572266]  [A loss: 1.086835, acc: 0.007812]\n",
      "459: [D loss: 0.667517, acc: 0.591797]  [A loss: 0.801902, acc: 0.222656]\n",
      "460: [D loss: 0.685976, acc: 0.544922]  [A loss: 1.106403, acc: 0.015625]\n",
      "461: [D loss: 0.683659, acc: 0.541016]  [A loss: 0.754984, acc: 0.351562]\n",
      "462: [D loss: 0.679739, acc: 0.517578]  [A loss: 1.069694, acc: 0.019531]\n",
      "463: [D loss: 0.657623, acc: 0.603516]  [A loss: 0.756729, acc: 0.351562]\n",
      "464: [D loss: 0.697334, acc: 0.541016]  [A loss: 1.171501, acc: 0.011719]\n",
      "465: [D loss: 0.666676, acc: 0.572266]  [A loss: 0.689783, acc: 0.558594]\n",
      "466: [D loss: 0.698610, acc: 0.544922]  [A loss: 1.133053, acc: 0.011719]\n",
      "467: [D loss: 0.654796, acc: 0.636719]  [A loss: 0.673225, acc: 0.582031]\n",
      "468: [D loss: 0.718383, acc: 0.521484]  [A loss: 1.167861, acc: 0.003906]\n",
      "469: [D loss: 0.675665, acc: 0.562500]  [A loss: 0.683430, acc: 0.566406]\n",
      "470: [D loss: 0.698554, acc: 0.529297]  [A loss: 1.039531, acc: 0.015625]\n",
      "471: [D loss: 0.665416, acc: 0.605469]  [A loss: 0.755477, acc: 0.316406]\n",
      "472: [D loss: 0.688745, acc: 0.542969]  [A loss: 1.024956, acc: 0.015625]\n",
      "473: [D loss: 0.651585, acc: 0.644531]  [A loss: 0.775817, acc: 0.296875]\n",
      "474: [D loss: 0.661684, acc: 0.548828]  [A loss: 0.977047, acc: 0.042969]\n",
      "475: [D loss: 0.662142, acc: 0.609375]  [A loss: 0.843825, acc: 0.167969]\n",
      "476: [D loss: 0.672419, acc: 0.578125]  [A loss: 1.017935, acc: 0.031250]\n",
      "477: [D loss: 0.661184, acc: 0.619141]  [A loss: 0.875545, acc: 0.125000]\n",
      "478: [D loss: 0.660225, acc: 0.587891]  [A loss: 1.062676, acc: 0.015625]\n",
      "479: [D loss: 0.655403, acc: 0.621094]  [A loss: 0.814865, acc: 0.261719]\n",
      "480: [D loss: 0.676936, acc: 0.570312]  [A loss: 1.183562, acc: 0.003906]\n",
      "481: [D loss: 0.669699, acc: 0.578125]  [A loss: 0.778059, acc: 0.320312]\n",
      "482: [D loss: 0.688593, acc: 0.564453]  [A loss: 1.274504, acc: 0.007812]\n",
      "483: [D loss: 0.665169, acc: 0.589844]  [A loss: 0.571296, acc: 0.816406]\n",
      "484: [D loss: 0.724063, acc: 0.511719]  [A loss: 1.166477, acc: 0.015625]\n",
      "485: [D loss: 0.686342, acc: 0.552734]  [A loss: 0.725842, acc: 0.421875]\n",
      "486: [D loss: 0.699016, acc: 0.519531]  [A loss: 1.034157, acc: 0.031250]\n",
      "487: [D loss: 0.660369, acc: 0.636719]  [A loss: 0.724182, acc: 0.445312]\n",
      "488: [D loss: 0.682212, acc: 0.556641]  [A loss: 0.935375, acc: 0.117188]\n",
      "489: [D loss: 0.662583, acc: 0.634766]  [A loss: 0.827530, acc: 0.261719]\n",
      "490: [D loss: 0.685631, acc: 0.566406]  [A loss: 0.940192, acc: 0.074219]\n",
      "491: [D loss: 0.671570, acc: 0.585938]  [A loss: 0.896032, acc: 0.117188]\n",
      "492: [D loss: 0.672275, acc: 0.568359]  [A loss: 0.915743, acc: 0.105469]\n",
      "493: [D loss: 0.649668, acc: 0.646484]  [A loss: 0.864939, acc: 0.171875]\n",
      "494: [D loss: 0.677823, acc: 0.562500]  [A loss: 1.117033, acc: 0.035156]\n",
      "495: [D loss: 0.667127, acc: 0.603516]  [A loss: 0.719202, acc: 0.449219]\n",
      "496: [D loss: 0.691093, acc: 0.541016]  [A loss: 1.142527, acc: 0.011719]\n",
      "497: [D loss: 0.660749, acc: 0.585938]  [A loss: 0.721222, acc: 0.464844]\n",
      "498: [D loss: 0.705459, acc: 0.541016]  [A loss: 1.137769, acc: 0.000000]\n",
      "499: [D loss: 0.666154, acc: 0.589844]  [A loss: 0.729267, acc: 0.425781]\n",
      "500: [D loss: 0.701655, acc: 0.511719]  [A loss: 1.021428, acc: 0.035156]\n",
      "501: [D loss: 0.654784, acc: 0.662109]  [A loss: 0.774286, acc: 0.343750]\n",
      "502: [D loss: 0.692869, acc: 0.544922]  [A loss: 1.011014, acc: 0.031250]\n",
      "503: [D loss: 0.662751, acc: 0.609375]  [A loss: 0.773813, acc: 0.320312]\n",
      "504: [D loss: 0.669170, acc: 0.558594]  [A loss: 1.108623, acc: 0.003906]\n",
      "505: [D loss: 0.660522, acc: 0.623047]  [A loss: 0.732863, acc: 0.453125]\n",
      "506: [D loss: 0.713435, acc: 0.535156]  [A loss: 1.182384, acc: 0.003906]\n",
      "507: [D loss: 0.680178, acc: 0.570312]  [A loss: 0.695741, acc: 0.511719]\n",
      "508: [D loss: 0.700523, acc: 0.529297]  [A loss: 1.024331, acc: 0.039062]\n",
      "509: [D loss: 0.661992, acc: 0.613281]  [A loss: 0.754193, acc: 0.363281]\n",
      "510: [D loss: 0.668311, acc: 0.570312]  [A loss: 0.961953, acc: 0.046875]\n",
      "511: [D loss: 0.651565, acc: 0.626953]  [A loss: 0.805083, acc: 0.265625]\n",
      "512: [D loss: 0.666518, acc: 0.576172]  [A loss: 0.985599, acc: 0.031250]\n",
      "513: [D loss: 0.663464, acc: 0.591797]  [A loss: 0.833588, acc: 0.187500]\n",
      "514: [D loss: 0.679328, acc: 0.570312]  [A loss: 0.989756, acc: 0.066406]\n",
      "515: [D loss: 0.669930, acc: 0.583984]  [A loss: 0.840793, acc: 0.179688]\n",
      "516: [D loss: 0.683461, acc: 0.552734]  [A loss: 1.024562, acc: 0.042969]\n",
      "517: [D loss: 0.650194, acc: 0.654297]  [A loss: 0.755501, acc: 0.390625]\n",
      "518: [D loss: 0.709061, acc: 0.515625]  [A loss: 1.149308, acc: 0.007812]\n",
      "519: [D loss: 0.661447, acc: 0.605469]  [A loss: 0.682995, acc: 0.558594]\n",
      "520: [D loss: 0.707543, acc: 0.542969]  [A loss: 1.168146, acc: 0.003906]\n",
      "521: [D loss: 0.677129, acc: 0.564453]  [A loss: 0.716251, acc: 0.445312]\n",
      "522: [D loss: 0.693556, acc: 0.539062]  [A loss: 1.012527, acc: 0.058594]\n",
      "523: [D loss: 0.656476, acc: 0.621094]  [A loss: 0.749726, acc: 0.390625]\n",
      "524: [D loss: 0.695807, acc: 0.537109]  [A loss: 1.039596, acc: 0.042969]\n",
      "525: [D loss: 0.667591, acc: 0.591797]  [A loss: 0.763833, acc: 0.343750]\n",
      "526: [D loss: 0.677639, acc: 0.564453]  [A loss: 1.004954, acc: 0.046875]\n",
      "527: [D loss: 0.672639, acc: 0.593750]  [A loss: 0.793898, acc: 0.277344]\n",
      "528: [D loss: 0.688411, acc: 0.523438]  [A loss: 0.991579, acc: 0.058594]\n",
      "529: [D loss: 0.655706, acc: 0.630859]  [A loss: 0.829084, acc: 0.210938]\n",
      "530: [D loss: 0.690836, acc: 0.544922]  [A loss: 1.077331, acc: 0.003906]\n",
      "531: [D loss: 0.658118, acc: 0.613281]  [A loss: 0.783617, acc: 0.292969]\n",
      "532: [D loss: 0.678814, acc: 0.574219]  [A loss: 1.040737, acc: 0.039062]\n",
      "533: [D loss: 0.661126, acc: 0.623047]  [A loss: 0.771265, acc: 0.351562]\n",
      "534: [D loss: 0.670097, acc: 0.583984]  [A loss: 1.097885, acc: 0.011719]\n",
      "535: [D loss: 0.664751, acc: 0.605469]  [A loss: 0.762126, acc: 0.359375]\n",
      "536: [D loss: 0.682047, acc: 0.552734]  [A loss: 1.129782, acc: 0.015625]\n",
      "537: [D loss: 0.640587, acc: 0.658203]  [A loss: 0.761375, acc: 0.386719]\n",
      "538: [D loss: 0.693552, acc: 0.521484]  [A loss: 1.115518, acc: 0.019531]\n",
      "539: [D loss: 0.676083, acc: 0.583984]  [A loss: 0.736171, acc: 0.382812]\n",
      "540: [D loss: 0.699127, acc: 0.529297]  [A loss: 1.162372, acc: 0.011719]\n",
      "541: [D loss: 0.659246, acc: 0.613281]  [A loss: 0.760015, acc: 0.382812]\n",
      "542: [D loss: 0.674013, acc: 0.578125]  [A loss: 1.046914, acc: 0.042969]\n",
      "543: [D loss: 0.657001, acc: 0.640625]  [A loss: 0.792383, acc: 0.289062]\n",
      "544: [D loss: 0.685137, acc: 0.560547]  [A loss: 1.126313, acc: 0.023438]\n",
      "545: [D loss: 0.651037, acc: 0.628906]  [A loss: 0.723631, acc: 0.472656]\n",
      "546: [D loss: 0.691004, acc: 0.537109]  [A loss: 1.122050, acc: 0.003906]\n",
      "547: [D loss: 0.651824, acc: 0.654297]  [A loss: 0.736421, acc: 0.429688]\n",
      "548: [D loss: 0.711110, acc: 0.519531]  [A loss: 1.205566, acc: 0.007812]\n",
      "549: [D loss: 0.670126, acc: 0.583984]  [A loss: 0.675312, acc: 0.574219]\n",
      "550: [D loss: 0.704437, acc: 0.511719]  [A loss: 1.022198, acc: 0.031250]\n",
      "551: [D loss: 0.670451, acc: 0.599609]  [A loss: 0.769704, acc: 0.316406]\n",
      "552: [D loss: 0.684081, acc: 0.558594]  [A loss: 1.025807, acc: 0.046875]\n",
      "553: [D loss: 0.672987, acc: 0.593750]  [A loss: 0.811118, acc: 0.257812]\n",
      "554: [D loss: 0.674737, acc: 0.572266]  [A loss: 1.001968, acc: 0.054688]\n",
      "555: [D loss: 0.652890, acc: 0.644531]  [A loss: 0.858463, acc: 0.148438]\n",
      "556: [D loss: 0.678152, acc: 0.566406]  [A loss: 1.027982, acc: 0.039062]\n",
      "557: [D loss: 0.671439, acc: 0.591797]  [A loss: 0.778007, acc: 0.320312]\n",
      "558: [D loss: 0.685050, acc: 0.572266]  [A loss: 1.090935, acc: 0.031250]\n",
      "559: [D loss: 0.653944, acc: 0.619141]  [A loss: 0.786360, acc: 0.300781]\n",
      "560: [D loss: 0.685289, acc: 0.541016]  [A loss: 1.070907, acc: 0.027344]\n",
      "561: [D loss: 0.656012, acc: 0.621094]  [A loss: 0.736275, acc: 0.429688]\n",
      "562: [D loss: 0.685688, acc: 0.552734]  [A loss: 1.153003, acc: 0.019531]\n",
      "563: [D loss: 0.656110, acc: 0.621094]  [A loss: 0.750324, acc: 0.394531]\n",
      "564: [D loss: 0.678193, acc: 0.550781]  [A loss: 1.035853, acc: 0.031250]\n",
      "565: [D loss: 0.652373, acc: 0.634766]  [A loss: 0.796135, acc: 0.292969]\n",
      "566: [D loss: 0.669462, acc: 0.564453]  [A loss: 1.068990, acc: 0.035156]\n",
      "567: [D loss: 0.656600, acc: 0.621094]  [A loss: 0.756173, acc: 0.363281]\n",
      "568: [D loss: 0.685417, acc: 0.544922]  [A loss: 1.165377, acc: 0.027344]\n",
      "569: [D loss: 0.660388, acc: 0.595703]  [A loss: 0.731588, acc: 0.464844]\n",
      "570: [D loss: 0.712908, acc: 0.507812]  [A loss: 1.079354, acc: 0.035156]\n",
      "571: [D loss: 0.671626, acc: 0.578125]  [A loss: 0.843704, acc: 0.222656]\n",
      "572: [D loss: 0.670903, acc: 0.560547]  [A loss: 0.984071, acc: 0.117188]\n",
      "573: [D loss: 0.658709, acc: 0.605469]  [A loss: 0.825879, acc: 0.230469]\n",
      "574: [D loss: 0.673540, acc: 0.593750]  [A loss: 0.979861, acc: 0.070312]\n",
      "575: [D loss: 0.663949, acc: 0.621094]  [A loss: 0.886887, acc: 0.171875]\n",
      "576: [D loss: 0.669477, acc: 0.589844]  [A loss: 0.966781, acc: 0.062500]\n",
      "577: [D loss: 0.669479, acc: 0.589844]  [A loss: 0.925362, acc: 0.156250]\n",
      "578: [D loss: 0.654029, acc: 0.595703]  [A loss: 0.972679, acc: 0.066406]\n",
      "579: [D loss: 0.669881, acc: 0.611328]  [A loss: 0.981601, acc: 0.074219]\n",
      "580: [D loss: 0.662864, acc: 0.638672]  [A loss: 0.937035, acc: 0.125000]\n",
      "581: [D loss: 0.669427, acc: 0.595703]  [A loss: 0.978826, acc: 0.074219]\n",
      "582: [D loss: 0.655305, acc: 0.623047]  [A loss: 0.941799, acc: 0.125000]\n",
      "583: [D loss: 0.659062, acc: 0.597656]  [A loss: 0.939681, acc: 0.101562]\n",
      "584: [D loss: 0.674563, acc: 0.570312]  [A loss: 1.096977, acc: 0.046875]\n",
      "585: [D loss: 0.673001, acc: 0.570312]  [A loss: 0.823456, acc: 0.281250]\n",
      "586: [D loss: 0.679608, acc: 0.566406]  [A loss: 1.330053, acc: 0.000000]\n",
      "587: [D loss: 0.675129, acc: 0.582031]  [A loss: 0.572741, acc: 0.800781]\n",
      "588: [D loss: 0.771944, acc: 0.501953]  [A loss: 1.234550, acc: 0.003906]\n",
      "589: [D loss: 0.664111, acc: 0.583984]  [A loss: 0.654749, acc: 0.625000]\n",
      "590: [D loss: 0.709364, acc: 0.531250]  [A loss: 1.001546, acc: 0.035156]\n",
      "591: [D loss: 0.654295, acc: 0.636719]  [A loss: 0.814124, acc: 0.269531]\n",
      "592: [D loss: 0.684136, acc: 0.541016]  [A loss: 0.971314, acc: 0.078125]\n",
      "593: [D loss: 0.678351, acc: 0.597656]  [A loss: 0.831187, acc: 0.238281]\n",
      "594: [D loss: 0.684510, acc: 0.564453]  [A loss: 0.953939, acc: 0.148438]\n",
      "595: [D loss: 0.678954, acc: 0.558594]  [A loss: 0.893217, acc: 0.136719]\n",
      "596: [D loss: 0.670817, acc: 0.595703]  [A loss: 0.882858, acc: 0.171875]\n",
      "597: [D loss: 0.668124, acc: 0.582031]  [A loss: 0.851987, acc: 0.171875]\n",
      "598: [D loss: 0.666171, acc: 0.585938]  [A loss: 0.945357, acc: 0.085938]\n",
      "599: [D loss: 0.659879, acc: 0.597656]  [A loss: 0.850565, acc: 0.207031]\n",
      "600: [D loss: 0.672220, acc: 0.576172]  [A loss: 0.925774, acc: 0.121094]\n",
      "601: [D loss: 0.662168, acc: 0.593750]  [A loss: 0.907022, acc: 0.136719]\n",
      "602: [D loss: 0.664671, acc: 0.591797]  [A loss: 0.972429, acc: 0.097656]\n",
      "603: [D loss: 0.644784, acc: 0.640625]  [A loss: 0.902224, acc: 0.128906]\n",
      "604: [D loss: 0.665877, acc: 0.576172]  [A loss: 0.989824, acc: 0.105469]\n",
      "605: [D loss: 0.670828, acc: 0.583984]  [A loss: 0.923174, acc: 0.136719]\n",
      "606: [D loss: 0.687438, acc: 0.554688]  [A loss: 1.148664, acc: 0.027344]\n",
      "607: [D loss: 0.652116, acc: 0.625000]  [A loss: 0.785097, acc: 0.363281]\n",
      "608: [D loss: 0.678170, acc: 0.570312]  [A loss: 1.169226, acc: 0.007812]\n",
      "609: [D loss: 0.658832, acc: 0.597656]  [A loss: 0.669670, acc: 0.566406]\n",
      "610: [D loss: 0.714807, acc: 0.521484]  [A loss: 1.289123, acc: 0.015625]\n",
      "611: [D loss: 0.662254, acc: 0.595703]  [A loss: 0.639538, acc: 0.648438]\n",
      "612: [D loss: 0.733433, acc: 0.533203]  [A loss: 1.170298, acc: 0.007812]\n",
      "613: [D loss: 0.662006, acc: 0.609375]  [A loss: 0.748339, acc: 0.367188]\n",
      "614: [D loss: 0.685417, acc: 0.568359]  [A loss: 0.975946, acc: 0.101562]\n",
      "615: [D loss: 0.675630, acc: 0.574219]  [A loss: 0.852648, acc: 0.179688]\n",
      "616: [D loss: 0.674204, acc: 0.558594]  [A loss: 0.945279, acc: 0.101562]\n",
      "617: [D loss: 0.665433, acc: 0.595703]  [A loss: 0.888637, acc: 0.171875]\n",
      "618: [D loss: 0.667659, acc: 0.585938]  [A loss: 0.954230, acc: 0.097656]\n",
      "619: [D loss: 0.676524, acc: 0.582031]  [A loss: 0.801853, acc: 0.277344]\n",
      "620: [D loss: 0.675047, acc: 0.560547]  [A loss: 1.014820, acc: 0.066406]\n",
      "621: [D loss: 0.652574, acc: 0.626953]  [A loss: 0.809534, acc: 0.269531]\n",
      "622: [D loss: 0.684061, acc: 0.568359]  [A loss: 1.051805, acc: 0.058594]\n",
      "623: [D loss: 0.658168, acc: 0.615234]  [A loss: 0.765476, acc: 0.363281]\n",
      "624: [D loss: 0.699636, acc: 0.542969]  [A loss: 1.238282, acc: 0.003906]\n",
      "625: [D loss: 0.662306, acc: 0.599609]  [A loss: 0.702134, acc: 0.492188]\n",
      "626: [D loss: 0.709419, acc: 0.511719]  [A loss: 1.121324, acc: 0.019531]\n",
      "627: [D loss: 0.663739, acc: 0.613281]  [A loss: 0.714830, acc: 0.445312]\n",
      "628: [D loss: 0.702307, acc: 0.537109]  [A loss: 1.086843, acc: 0.023438]\n",
      "629: [D loss: 0.666739, acc: 0.599609]  [A loss: 0.789122, acc: 0.335938]\n",
      "630: [D loss: 0.679550, acc: 0.570312]  [A loss: 0.980205, acc: 0.070312]\n",
      "631: [D loss: 0.664178, acc: 0.597656]  [A loss: 0.801949, acc: 0.285156]\n",
      "632: [D loss: 0.676029, acc: 0.544922]  [A loss: 1.043417, acc: 0.074219]\n",
      "633: [D loss: 0.658773, acc: 0.601562]  [A loss: 0.787937, acc: 0.292969]\n",
      "634: [D loss: 0.658230, acc: 0.587891]  [A loss: 0.967969, acc: 0.070312]\n",
      "635: [D loss: 0.667636, acc: 0.580078]  [A loss: 0.852690, acc: 0.222656]\n",
      "636: [D loss: 0.658732, acc: 0.613281]  [A loss: 0.954634, acc: 0.101562]\n",
      "637: [D loss: 0.655098, acc: 0.615234]  [A loss: 0.904161, acc: 0.195312]\n",
      "638: [D loss: 0.654962, acc: 0.603516]  [A loss: 0.897226, acc: 0.191406]\n",
      "639: [D loss: 0.667282, acc: 0.576172]  [A loss: 1.031152, acc: 0.066406]\n",
      "640: [D loss: 0.647143, acc: 0.632812]  [A loss: 0.811275, acc: 0.289062]\n",
      "641: [D loss: 0.680860, acc: 0.566406]  [A loss: 1.118937, acc: 0.031250]\n",
      "642: [D loss: 0.652725, acc: 0.613281]  [A loss: 0.771674, acc: 0.332031]\n",
      "643: [D loss: 0.699171, acc: 0.546875]  [A loss: 1.259349, acc: 0.031250]\n",
      "644: [D loss: 0.675219, acc: 0.578125]  [A loss: 0.719741, acc: 0.457031]\n",
      "645: [D loss: 0.691990, acc: 0.542969]  [A loss: 1.046103, acc: 0.050781]\n",
      "646: [D loss: 0.662068, acc: 0.593750]  [A loss: 0.814618, acc: 0.281250]\n",
      "647: [D loss: 0.697854, acc: 0.550781]  [A loss: 1.089576, acc: 0.027344]\n",
      "648: [D loss: 0.652199, acc: 0.621094]  [A loss: 0.805796, acc: 0.316406]\n",
      "649: [D loss: 0.676512, acc: 0.554688]  [A loss: 0.997006, acc: 0.070312]\n",
      "650: [D loss: 0.658090, acc: 0.615234]  [A loss: 0.875955, acc: 0.214844]\n",
      "651: [D loss: 0.678389, acc: 0.560547]  [A loss: 1.020852, acc: 0.074219]\n",
      "652: [D loss: 0.666290, acc: 0.605469]  [A loss: 0.825905, acc: 0.277344]\n",
      "653: [D loss: 0.676635, acc: 0.578125]  [A loss: 1.041656, acc: 0.042969]\n",
      "654: [D loss: 0.662802, acc: 0.580078]  [A loss: 0.780341, acc: 0.339844]\n",
      "655: [D loss: 0.683852, acc: 0.562500]  [A loss: 1.095292, acc: 0.027344]\n",
      "656: [D loss: 0.656660, acc: 0.628906]  [A loss: 0.757706, acc: 0.394531]\n",
      "657: [D loss: 0.693382, acc: 0.558594]  [A loss: 1.096717, acc: 0.039062]\n",
      "658: [D loss: 0.662923, acc: 0.617188]  [A loss: 0.703327, acc: 0.527344]\n",
      "659: [D loss: 0.690818, acc: 0.544922]  [A loss: 1.091587, acc: 0.031250]\n",
      "660: [D loss: 0.653223, acc: 0.638672]  [A loss: 0.763712, acc: 0.425781]\n",
      "661: [D loss: 0.677740, acc: 0.580078]  [A loss: 1.022091, acc: 0.082031]\n",
      "662: [D loss: 0.648114, acc: 0.621094]  [A loss: 0.881680, acc: 0.214844]\n",
      "663: [D loss: 0.670941, acc: 0.589844]  [A loss: 0.939367, acc: 0.101562]\n",
      "664: [D loss: 0.657740, acc: 0.605469]  [A loss: 0.977726, acc: 0.078125]\n",
      "665: [D loss: 0.653188, acc: 0.615234]  [A loss: 0.947706, acc: 0.144531]\n",
      "666: [D loss: 0.673051, acc: 0.597656]  [A loss: 1.043025, acc: 0.074219]\n",
      "667: [D loss: 0.674034, acc: 0.566406]  [A loss: 0.894678, acc: 0.167969]\n",
      "668: [D loss: 0.669918, acc: 0.568359]  [A loss: 1.010671, acc: 0.062500]\n",
      "669: [D loss: 0.662043, acc: 0.601562]  [A loss: 1.057623, acc: 0.062500]\n",
      "670: [D loss: 0.662362, acc: 0.599609]  [A loss: 0.833550, acc: 0.257812]\n",
      "671: [D loss: 0.686146, acc: 0.564453]  [A loss: 1.252008, acc: 0.003906]\n",
      "672: [D loss: 0.676262, acc: 0.570312]  [A loss: 0.765049, acc: 0.402344]\n",
      "673: [D loss: 0.686608, acc: 0.542969]  [A loss: 1.156074, acc: 0.023438]\n",
      "674: [D loss: 0.646398, acc: 0.619141]  [A loss: 0.751857, acc: 0.414062]\n",
      "675: [D loss: 0.748344, acc: 0.519531]  [A loss: 1.291149, acc: 0.003906]\n",
      "676: [D loss: 0.665114, acc: 0.580078]  [A loss: 0.678265, acc: 0.550781]\n",
      "677: [D loss: 0.712180, acc: 0.531250]  [A loss: 1.107160, acc: 0.042969]\n",
      "678: [D loss: 0.650910, acc: 0.626953]  [A loss: 0.755681, acc: 0.367188]\n",
      "679: [D loss: 0.672919, acc: 0.558594]  [A loss: 1.043084, acc: 0.050781]\n",
      "680: [D loss: 0.657287, acc: 0.597656]  [A loss: 0.765503, acc: 0.371094]\n",
      "681: [D loss: 0.684368, acc: 0.550781]  [A loss: 1.040441, acc: 0.058594]\n",
      "682: [D loss: 0.655900, acc: 0.609375]  [A loss: 0.801505, acc: 0.273438]\n",
      "683: [D loss: 0.679096, acc: 0.595703]  [A loss: 1.021779, acc: 0.050781]\n",
      "684: [D loss: 0.669850, acc: 0.591797]  [A loss: 0.842356, acc: 0.238281]\n",
      "685: [D loss: 0.658911, acc: 0.605469]  [A loss: 1.018916, acc: 0.066406]\n",
      "686: [D loss: 0.663572, acc: 0.605469]  [A loss: 0.831201, acc: 0.257812]\n",
      "687: [D loss: 0.685938, acc: 0.566406]  [A loss: 1.007635, acc: 0.082031]\n",
      "688: [D loss: 0.656187, acc: 0.611328]  [A loss: 0.855625, acc: 0.210938]\n",
      "689: [D loss: 0.672776, acc: 0.562500]  [A loss: 1.060395, acc: 0.058594]\n",
      "690: [D loss: 0.658092, acc: 0.587891]  [A loss: 0.873745, acc: 0.152344]\n",
      "691: [D loss: 0.686541, acc: 0.556641]  [A loss: 1.114158, acc: 0.035156]\n",
      "692: [D loss: 0.664888, acc: 0.611328]  [A loss: 0.787789, acc: 0.312500]\n",
      "693: [D loss: 0.680880, acc: 0.548828]  [A loss: 1.124871, acc: 0.046875]\n",
      "694: [D loss: 0.652777, acc: 0.615234]  [A loss: 0.774932, acc: 0.332031]\n",
      "695: [D loss: 0.677081, acc: 0.546875]  [A loss: 1.196424, acc: 0.019531]\n",
      "696: [D loss: 0.656329, acc: 0.603516]  [A loss: 0.732301, acc: 0.457031]\n",
      "697: [D loss: 0.699494, acc: 0.546875]  [A loss: 1.158304, acc: 0.027344]\n",
      "698: [D loss: 0.669443, acc: 0.599609]  [A loss: 0.726034, acc: 0.472656]\n",
      "699: [D loss: 0.693836, acc: 0.562500]  [A loss: 1.152856, acc: 0.023438]\n",
      "700: [D loss: 0.661581, acc: 0.593750]  [A loss: 0.767443, acc: 0.394531]\n",
      "701: [D loss: 0.699395, acc: 0.572266]  [A loss: 1.054808, acc: 0.062500]\n",
      "702: [D loss: 0.674500, acc: 0.578125]  [A loss: 0.815099, acc: 0.269531]\n",
      "703: [D loss: 0.664181, acc: 0.591797]  [A loss: 0.985148, acc: 0.121094]\n",
      "704: [D loss: 0.668621, acc: 0.580078]  [A loss: 0.889761, acc: 0.207031]\n",
      "705: [D loss: 0.669863, acc: 0.607422]  [A loss: 1.061419, acc: 0.058594]\n",
      "706: [D loss: 0.648884, acc: 0.632812]  [A loss: 0.806128, acc: 0.292969]\n",
      "707: [D loss: 0.678792, acc: 0.564453]  [A loss: 1.031691, acc: 0.074219]\n",
      "708: [D loss: 0.646227, acc: 0.646484]  [A loss: 0.872060, acc: 0.253906]\n",
      "709: [D loss: 0.680732, acc: 0.566406]  [A loss: 1.061086, acc: 0.074219]\n",
      "710: [D loss: 0.668665, acc: 0.568359]  [A loss: 0.776613, acc: 0.351562]\n",
      "711: [D loss: 0.683649, acc: 0.554688]  [A loss: 1.098213, acc: 0.042969]\n",
      "712: [D loss: 0.647133, acc: 0.644531]  [A loss: 0.768579, acc: 0.367188]\n",
      "713: [D loss: 0.710661, acc: 0.548828]  [A loss: 1.227959, acc: 0.046875]\n",
      "714: [D loss: 0.664830, acc: 0.589844]  [A loss: 0.739077, acc: 0.410156]\n",
      "715: [D loss: 0.684647, acc: 0.533203]  [A loss: 1.072838, acc: 0.042969]\n",
      "716: [D loss: 0.670332, acc: 0.587891]  [A loss: 0.759479, acc: 0.398438]\n",
      "717: [D loss: 0.695886, acc: 0.542969]  [A loss: 1.073765, acc: 0.027344]\n",
      "718: [D loss: 0.657861, acc: 0.601562]  [A loss: 0.763855, acc: 0.386719]\n",
      "719: [D loss: 0.677253, acc: 0.601562]  [A loss: 1.056242, acc: 0.042969]\n",
      "720: [D loss: 0.658924, acc: 0.609375]  [A loss: 0.810724, acc: 0.316406]\n",
      "721: [D loss: 0.693924, acc: 0.539062]  [A loss: 1.079274, acc: 0.058594]\n",
      "722: [D loss: 0.659852, acc: 0.619141]  [A loss: 0.893663, acc: 0.183594]\n",
      "723: [D loss: 0.670852, acc: 0.576172]  [A loss: 0.961696, acc: 0.109375]\n",
      "724: [D loss: 0.649778, acc: 0.617188]  [A loss: 0.854739, acc: 0.234375]\n",
      "725: [D loss: 0.680156, acc: 0.529297]  [A loss: 1.012554, acc: 0.074219]\n",
      "726: [D loss: 0.670190, acc: 0.576172]  [A loss: 0.931426, acc: 0.132812]\n",
      "727: [D loss: 0.673536, acc: 0.566406]  [A loss: 1.020805, acc: 0.066406]\n",
      "728: [D loss: 0.673031, acc: 0.595703]  [A loss: 0.925319, acc: 0.152344]\n",
      "729: [D loss: 0.656332, acc: 0.595703]  [A loss: 0.916397, acc: 0.136719]\n",
      "730: [D loss: 0.674368, acc: 0.558594]  [A loss: 1.051248, acc: 0.054688]\n",
      "731: [D loss: 0.658181, acc: 0.615234]  [A loss: 0.886288, acc: 0.187500]\n",
      "732: [D loss: 0.676962, acc: 0.568359]  [A loss: 1.088182, acc: 0.058594]\n",
      "733: [D loss: 0.661277, acc: 0.595703]  [A loss: 0.842194, acc: 0.246094]\n",
      "734: [D loss: 0.680927, acc: 0.576172]  [A loss: 1.151986, acc: 0.027344]\n",
      "735: [D loss: 0.664915, acc: 0.589844]  [A loss: 0.825314, acc: 0.285156]\n",
      "736: [D loss: 0.654567, acc: 0.589844]  [A loss: 1.115326, acc: 0.054688]\n",
      "737: [D loss: 0.662040, acc: 0.603516]  [A loss: 0.777032, acc: 0.363281]\n",
      "738: [D loss: 0.687235, acc: 0.550781]  [A loss: 1.282208, acc: 0.007812]\n",
      "739: [D loss: 0.672312, acc: 0.572266]  [A loss: 0.683195, acc: 0.554688]\n",
      "740: [D loss: 0.720833, acc: 0.525391]  [A loss: 1.153998, acc: 0.031250]\n",
      "741: [D loss: 0.668515, acc: 0.585938]  [A loss: 0.699405, acc: 0.476562]\n",
      "742: [D loss: 0.688515, acc: 0.521484]  [A loss: 1.021767, acc: 0.062500]\n",
      "743: [D loss: 0.652457, acc: 0.621094]  [A loss: 0.802208, acc: 0.300781]\n",
      "744: [D loss: 0.679893, acc: 0.578125]  [A loss: 1.010504, acc: 0.074219]\n",
      "745: [D loss: 0.656248, acc: 0.603516]  [A loss: 0.818909, acc: 0.257812]\n",
      "746: [D loss: 0.671431, acc: 0.574219]  [A loss: 0.981620, acc: 0.109375]\n",
      "747: [D loss: 0.667541, acc: 0.599609]  [A loss: 0.837079, acc: 0.265625]\n",
      "748: [D loss: 0.680217, acc: 0.558594]  [A loss: 1.018312, acc: 0.082031]\n",
      "749: [D loss: 0.664367, acc: 0.582031]  [A loss: 0.815941, acc: 0.277344]\n",
      "750: [D loss: 0.679862, acc: 0.560547]  [A loss: 1.022084, acc: 0.082031]\n",
      "751: [D loss: 0.664823, acc: 0.576172]  [A loss: 0.866215, acc: 0.230469]\n",
      "752: [D loss: 0.671683, acc: 0.572266]  [A loss: 1.084867, acc: 0.070312]\n",
      "753: [D loss: 0.656902, acc: 0.634766]  [A loss: 0.787092, acc: 0.351562]\n",
      "754: [D loss: 0.712687, acc: 0.531250]  [A loss: 1.127448, acc: 0.031250]\n",
      "755: [D loss: 0.662844, acc: 0.603516]  [A loss: 0.749531, acc: 0.414062]\n",
      "756: [D loss: 0.694116, acc: 0.539062]  [A loss: 1.056697, acc: 0.054688]\n",
      "757: [D loss: 0.659127, acc: 0.623047]  [A loss: 0.793579, acc: 0.296875]\n",
      "758: [D loss: 0.671713, acc: 0.589844]  [A loss: 1.038145, acc: 0.078125]\n",
      "759: [D loss: 0.653723, acc: 0.625000]  [A loss: 0.884218, acc: 0.199219]\n",
      "760: [D loss: 0.670320, acc: 0.603516]  [A loss: 0.999046, acc: 0.105469]\n",
      "761: [D loss: 0.653984, acc: 0.615234]  [A loss: 0.874805, acc: 0.207031]\n",
      "762: [D loss: 0.659273, acc: 0.593750]  [A loss: 0.998850, acc: 0.113281]\n",
      "763: [D loss: 0.653017, acc: 0.613281]  [A loss: 0.935542, acc: 0.136719]\n",
      "764: [D loss: 0.664518, acc: 0.587891]  [A loss: 0.958797, acc: 0.132812]\n",
      "765: [D loss: 0.673017, acc: 0.589844]  [A loss: 0.975460, acc: 0.128906]\n",
      "766: [D loss: 0.660763, acc: 0.603516]  [A loss: 0.900823, acc: 0.210938]\n",
      "767: [D loss: 0.663533, acc: 0.619141]  [A loss: 1.108973, acc: 0.039062]\n",
      "768: [D loss: 0.649501, acc: 0.619141]  [A loss: 0.761695, acc: 0.406250]\n",
      "769: [D loss: 0.697954, acc: 0.539062]  [A loss: 1.334835, acc: 0.011719]\n",
      "770: [D loss: 0.673711, acc: 0.556641]  [A loss: 0.618212, acc: 0.648438]\n",
      "771: [D loss: 0.744841, acc: 0.505859]  [A loss: 1.143481, acc: 0.023438]\n",
      "772: [D loss: 0.689426, acc: 0.568359]  [A loss: 0.787019, acc: 0.355469]\n",
      "773: [D loss: 0.704973, acc: 0.544922]  [A loss: 0.960714, acc: 0.117188]\n",
      "774: [D loss: 0.658610, acc: 0.611328]  [A loss: 0.892333, acc: 0.183594]\n",
      "775: [D loss: 0.685273, acc: 0.546875]  [A loss: 0.936359, acc: 0.148438]\n",
      "776: [D loss: 0.664379, acc: 0.580078]  [A loss: 0.926493, acc: 0.144531]\n",
      "777: [D loss: 0.686479, acc: 0.593750]  [A loss: 0.932976, acc: 0.136719]\n",
      "778: [D loss: 0.665592, acc: 0.613281]  [A loss: 0.901143, acc: 0.144531]\n",
      "779: [D loss: 0.661261, acc: 0.611328]  [A loss: 0.956501, acc: 0.117188]\n",
      "780: [D loss: 0.669680, acc: 0.603516]  [A loss: 0.909484, acc: 0.187500]\n",
      "781: [D loss: 0.661183, acc: 0.593750]  [A loss: 0.918632, acc: 0.171875]\n",
      "782: [D loss: 0.673884, acc: 0.583984]  [A loss: 1.056182, acc: 0.070312]\n",
      "783: [D loss: 0.667557, acc: 0.576172]  [A loss: 0.865986, acc: 0.226562]\n",
      "784: [D loss: 0.672657, acc: 0.589844]  [A loss: 1.052724, acc: 0.050781]\n",
      "785: [D loss: 0.664672, acc: 0.578125]  [A loss: 0.873946, acc: 0.226562]\n",
      "786: [D loss: 0.678288, acc: 0.570312]  [A loss: 1.068839, acc: 0.062500]\n",
      "787: [D loss: 0.662694, acc: 0.607422]  [A loss: 0.829597, acc: 0.269531]\n",
      "788: [D loss: 0.681040, acc: 0.564453]  [A loss: 1.185987, acc: 0.042969]\n",
      "789: [D loss: 0.655568, acc: 0.619141]  [A loss: 0.711988, acc: 0.515625]\n",
      "790: [D loss: 0.715863, acc: 0.552734]  [A loss: 1.243980, acc: 0.023438]\n",
      "791: [D loss: 0.657051, acc: 0.625000]  [A loss: 0.743307, acc: 0.449219]\n",
      "792: [D loss: 0.685470, acc: 0.560547]  [A loss: 1.099294, acc: 0.054688]\n",
      "793: [D loss: 0.674299, acc: 0.576172]  [A loss: 0.832570, acc: 0.253906]\n",
      "794: [D loss: 0.673518, acc: 0.591797]  [A loss: 0.999219, acc: 0.093750]\n",
      "795: [D loss: 0.659950, acc: 0.613281]  [A loss: 0.872653, acc: 0.214844]\n",
      "796: [D loss: 0.665988, acc: 0.580078]  [A loss: 0.959529, acc: 0.148438]\n",
      "797: [D loss: 0.679069, acc: 0.578125]  [A loss: 0.974771, acc: 0.128906]\n",
      "798: [D loss: 0.670171, acc: 0.589844]  [A loss: 0.866568, acc: 0.230469]\n",
      "799: [D loss: 0.673818, acc: 0.583984]  [A loss: 0.951203, acc: 0.132812]\n",
      "800: [D loss: 0.680613, acc: 0.560547]  [A loss: 0.914538, acc: 0.179688]\n",
      "801: [D loss: 0.672899, acc: 0.556641]  [A loss: 0.938797, acc: 0.136719]\n",
      "802: [D loss: 0.676095, acc: 0.589844]  [A loss: 0.900114, acc: 0.175781]\n",
      "803: [D loss: 0.683293, acc: 0.552734]  [A loss: 0.999542, acc: 0.105469]\n",
      "804: [D loss: 0.657343, acc: 0.644531]  [A loss: 0.804092, acc: 0.347656]\n",
      "805: [D loss: 0.688657, acc: 0.552734]  [A loss: 1.134588, acc: 0.050781]\n",
      "806: [D loss: 0.650706, acc: 0.623047]  [A loss: 0.747574, acc: 0.398438]\n",
      "807: [D loss: 0.707119, acc: 0.564453]  [A loss: 1.181482, acc: 0.023438]\n",
      "808: [D loss: 0.679453, acc: 0.574219]  [A loss: 0.745903, acc: 0.410156]\n",
      "809: [D loss: 0.706041, acc: 0.521484]  [A loss: 1.111562, acc: 0.027344]\n",
      "810: [D loss: 0.661969, acc: 0.601562]  [A loss: 0.802879, acc: 0.320312]\n",
      "811: [D loss: 0.698243, acc: 0.527344]  [A loss: 1.026468, acc: 0.058594]\n",
      "812: [D loss: 0.663724, acc: 0.619141]  [A loss: 0.814172, acc: 0.296875]\n",
      "813: [D loss: 0.685723, acc: 0.587891]  [A loss: 0.985976, acc: 0.101562]\n",
      "814: [D loss: 0.673239, acc: 0.587891]  [A loss: 0.814980, acc: 0.292969]\n",
      "815: [D loss: 0.674669, acc: 0.576172]  [A loss: 0.999450, acc: 0.078125]\n",
      "816: [D loss: 0.662718, acc: 0.582031]  [A loss: 0.864124, acc: 0.195312]\n",
      "817: [D loss: 0.685158, acc: 0.558594]  [A loss: 1.044188, acc: 0.062500]\n",
      "818: [D loss: 0.672575, acc: 0.587891]  [A loss: 0.837720, acc: 0.265625]\n",
      "819: [D loss: 0.681576, acc: 0.560547]  [A loss: 1.017559, acc: 0.121094]\n",
      "820: [D loss: 0.661892, acc: 0.593750]  [A loss: 0.884437, acc: 0.218750]\n",
      "821: [D loss: 0.699035, acc: 0.542969]  [A loss: 1.074003, acc: 0.042969]\n",
      "822: [D loss: 0.665343, acc: 0.582031]  [A loss: 0.821266, acc: 0.242188]\n",
      "823: [D loss: 0.682459, acc: 0.564453]  [A loss: 1.017764, acc: 0.070312]\n",
      "824: [D loss: 0.671541, acc: 0.619141]  [A loss: 0.862010, acc: 0.265625]\n",
      "825: [D loss: 0.671228, acc: 0.587891]  [A loss: 0.965503, acc: 0.093750]\n",
      "826: [D loss: 0.667544, acc: 0.589844]  [A loss: 0.876219, acc: 0.230469]\n",
      "827: [D loss: 0.675125, acc: 0.585938]  [A loss: 1.039258, acc: 0.113281]\n",
      "828: [D loss: 0.667906, acc: 0.605469]  [A loss: 0.869617, acc: 0.238281]\n",
      "829: [D loss: 0.657235, acc: 0.623047]  [A loss: 1.022822, acc: 0.082031]\n",
      "830: [D loss: 0.665407, acc: 0.574219]  [A loss: 0.845242, acc: 0.265625]\n",
      "831: [D loss: 0.671273, acc: 0.566406]  [A loss: 1.062584, acc: 0.085938]\n",
      "832: [D loss: 0.652967, acc: 0.599609]  [A loss: 0.785837, acc: 0.359375]\n",
      "833: [D loss: 0.699754, acc: 0.558594]  [A loss: 1.217773, acc: 0.031250]\n",
      "834: [D loss: 0.642552, acc: 0.636719]  [A loss: 0.695376, acc: 0.511719]\n",
      "835: [D loss: 0.716165, acc: 0.509766]  [A loss: 1.155648, acc: 0.023438]\n",
      "836: [D loss: 0.669843, acc: 0.570312]  [A loss: 0.739161, acc: 0.417969]\n",
      "837: [D loss: 0.732749, acc: 0.503906]  [A loss: 1.098966, acc: 0.050781]\n",
      "838: [D loss: 0.675745, acc: 0.570312]  [A loss: 0.744789, acc: 0.417969]\n",
      "839: [D loss: 0.704689, acc: 0.548828]  [A loss: 0.997862, acc: 0.089844]\n",
      "840: [D loss: 0.671781, acc: 0.593750]  [A loss: 0.798552, acc: 0.328125]\n",
      "841: [D loss: 0.676695, acc: 0.572266]  [A loss: 0.924357, acc: 0.128906]\n",
      "842: [D loss: 0.684891, acc: 0.562500]  [A loss: 0.926681, acc: 0.167969]\n",
      "843: [D loss: 0.660846, acc: 0.626953]  [A loss: 0.903455, acc: 0.167969]\n",
      "844: [D loss: 0.653876, acc: 0.626953]  [A loss: 0.961643, acc: 0.097656]\n",
      "845: [D loss: 0.673723, acc: 0.542969]  [A loss: 0.964221, acc: 0.121094]\n",
      "846: [D loss: 0.667591, acc: 0.564453]  [A loss: 1.021758, acc: 0.062500]\n",
      "847: [D loss: 0.666126, acc: 0.597656]  [A loss: 0.866835, acc: 0.207031]\n",
      "848: [D loss: 0.662574, acc: 0.599609]  [A loss: 1.033762, acc: 0.078125]\n",
      "849: [D loss: 0.651505, acc: 0.621094]  [A loss: 0.894546, acc: 0.203125]\n",
      "850: [D loss: 0.680794, acc: 0.548828]  [A loss: 1.061086, acc: 0.058594]\n",
      "851: [D loss: 0.666427, acc: 0.591797]  [A loss: 0.850126, acc: 0.242188]\n",
      "852: [D loss: 0.676846, acc: 0.585938]  [A loss: 1.088248, acc: 0.085938]\n",
      "853: [D loss: 0.668638, acc: 0.599609]  [A loss: 0.824562, acc: 0.250000]\n",
      "854: [D loss: 0.702171, acc: 0.527344]  [A loss: 1.136285, acc: 0.031250]\n",
      "855: [D loss: 0.663369, acc: 0.621094]  [A loss: 0.750415, acc: 0.417969]\n",
      "856: [D loss: 0.701595, acc: 0.539062]  [A loss: 1.112669, acc: 0.042969]\n",
      "857: [D loss: 0.673253, acc: 0.554688]  [A loss: 0.799565, acc: 0.324219]\n",
      "858: [D loss: 0.691227, acc: 0.548828]  [A loss: 1.086785, acc: 0.058594]\n",
      "859: [D loss: 0.662286, acc: 0.597656]  [A loss: 0.686223, acc: 0.511719]\n",
      "860: [D loss: 0.694853, acc: 0.554688]  [A loss: 1.051879, acc: 0.050781]\n",
      "861: [D loss: 0.663430, acc: 0.589844]  [A loss: 0.815617, acc: 0.277344]\n",
      "862: [D loss: 0.694893, acc: 0.533203]  [A loss: 1.017092, acc: 0.097656]\n",
      "863: [D loss: 0.661391, acc: 0.617188]  [A loss: 0.849661, acc: 0.207031]\n",
      "864: [D loss: 0.672643, acc: 0.603516]  [A loss: 1.014076, acc: 0.078125]\n",
      "865: [D loss: 0.652296, acc: 0.630859]  [A loss: 0.894829, acc: 0.171875]\n",
      "866: [D loss: 0.668808, acc: 0.548828]  [A loss: 0.949686, acc: 0.117188]\n",
      "867: [D loss: 0.655505, acc: 0.578125]  [A loss: 0.867214, acc: 0.195312]\n",
      "868: [D loss: 0.699750, acc: 0.560547]  [A loss: 1.120643, acc: 0.027344]\n",
      "869: [D loss: 0.668480, acc: 0.605469]  [A loss: 0.739373, acc: 0.425781]\n",
      "870: [D loss: 0.709305, acc: 0.531250]  [A loss: 1.141070, acc: 0.019531]\n",
      "871: [D loss: 0.652307, acc: 0.617188]  [A loss: 0.757292, acc: 0.386719]\n",
      "872: [D loss: 0.704955, acc: 0.544922]  [A loss: 1.055303, acc: 0.089844]\n",
      "873: [D loss: 0.695565, acc: 0.533203]  [A loss: 0.880686, acc: 0.160156]\n",
      "874: [D loss: 0.656910, acc: 0.623047]  [A loss: 0.907137, acc: 0.175781]\n",
      "875: [D loss: 0.682655, acc: 0.562500]  [A loss: 1.049865, acc: 0.042969]\n",
      "876: [D loss: 0.679654, acc: 0.552734]  [A loss: 0.806458, acc: 0.265625]\n",
      "877: [D loss: 0.682245, acc: 0.582031]  [A loss: 1.021326, acc: 0.058594]\n",
      "878: [D loss: 0.671129, acc: 0.603516]  [A loss: 0.863452, acc: 0.210938]\n",
      "879: [D loss: 0.674124, acc: 0.568359]  [A loss: 0.957271, acc: 0.117188]\n",
      "880: [D loss: 0.662861, acc: 0.609375]  [A loss: 0.855236, acc: 0.222656]\n",
      "881: [D loss: 0.670656, acc: 0.593750]  [A loss: 0.996068, acc: 0.074219]\n",
      "882: [D loss: 0.657609, acc: 0.601562]  [A loss: 0.897527, acc: 0.195312]\n",
      "883: [D loss: 0.684341, acc: 0.566406]  [A loss: 1.014185, acc: 0.078125]\n",
      "884: [D loss: 0.652681, acc: 0.619141]  [A loss: 0.879266, acc: 0.195312]\n",
      "885: [D loss: 0.690786, acc: 0.542969]  [A loss: 1.051579, acc: 0.050781]\n",
      "886: [D loss: 0.662880, acc: 0.601562]  [A loss: 0.889018, acc: 0.183594]\n",
      "887: [D loss: 0.711012, acc: 0.527344]  [A loss: 1.127318, acc: 0.035156]\n",
      "888: [D loss: 0.687002, acc: 0.558594]  [A loss: 0.710485, acc: 0.472656]\n",
      "889: [D loss: 0.699132, acc: 0.550781]  [A loss: 1.094909, acc: 0.066406]\n",
      "890: [D loss: 0.677293, acc: 0.578125]  [A loss: 0.898223, acc: 0.175781]\n",
      "891: [D loss: 0.687128, acc: 0.564453]  [A loss: 0.931195, acc: 0.140625]\n",
      "892: [D loss: 0.662802, acc: 0.611328]  [A loss: 0.933038, acc: 0.148438]\n",
      "893: [D loss: 0.666477, acc: 0.587891]  [A loss: 0.963384, acc: 0.125000]\n",
      "894: [D loss: 0.665467, acc: 0.611328]  [A loss: 0.899067, acc: 0.203125]\n",
      "895: [D loss: 0.671860, acc: 0.601562]  [A loss: 0.994301, acc: 0.093750]\n",
      "896: [D loss: 0.668822, acc: 0.587891]  [A loss: 0.910023, acc: 0.175781]\n",
      "897: [D loss: 0.663215, acc: 0.585938]  [A loss: 0.967347, acc: 0.078125]\n",
      "898: [D loss: 0.660861, acc: 0.587891]  [A loss: 0.964707, acc: 0.148438]\n",
      "899: [D loss: 0.678005, acc: 0.576172]  [A loss: 0.993973, acc: 0.070312]\n",
      "900: [D loss: 0.661430, acc: 0.572266]  [A loss: 0.893550, acc: 0.187500]\n",
      "901: [D loss: 0.683830, acc: 0.574219]  [A loss: 1.130992, acc: 0.031250]\n",
      "902: [D loss: 0.659810, acc: 0.623047]  [A loss: 0.778949, acc: 0.386719]\n",
      "903: [D loss: 0.691916, acc: 0.544922]  [A loss: 1.185745, acc: 0.031250]\n",
      "904: [D loss: 0.663165, acc: 0.611328]  [A loss: 0.727667, acc: 0.449219]\n",
      "905: [D loss: 0.697341, acc: 0.560547]  [A loss: 1.122477, acc: 0.042969]\n",
      "906: [D loss: 0.656141, acc: 0.587891]  [A loss: 0.783351, acc: 0.335938]\n",
      "907: [D loss: 0.680891, acc: 0.539062]  [A loss: 1.006273, acc: 0.101562]\n",
      "908: [D loss: 0.667376, acc: 0.593750]  [A loss: 0.898410, acc: 0.179688]\n",
      "909: [D loss: 0.665311, acc: 0.597656]  [A loss: 0.883363, acc: 0.187500]\n",
      "910: [D loss: 0.677675, acc: 0.568359]  [A loss: 0.949910, acc: 0.152344]\n",
      "911: [D loss: 0.647982, acc: 0.632812]  [A loss: 0.891812, acc: 0.199219]\n",
      "912: [D loss: 0.690410, acc: 0.546875]  [A loss: 1.033128, acc: 0.101562]\n",
      "913: [D loss: 0.666602, acc: 0.572266]  [A loss: 0.972075, acc: 0.136719]\n",
      "914: [D loss: 0.672690, acc: 0.580078]  [A loss: 0.970232, acc: 0.121094]\n",
      "915: [D loss: 0.672103, acc: 0.585938]  [A loss: 0.943299, acc: 0.136719]\n",
      "916: [D loss: 0.673034, acc: 0.611328]  [A loss: 1.011511, acc: 0.105469]\n",
      "917: [D loss: 0.674086, acc: 0.570312]  [A loss: 1.079014, acc: 0.082031]\n",
      "918: [D loss: 0.649564, acc: 0.638672]  [A loss: 0.794298, acc: 0.320312]\n",
      "919: [D loss: 0.697308, acc: 0.558594]  [A loss: 1.114888, acc: 0.039062]\n",
      "920: [D loss: 0.677052, acc: 0.548828]  [A loss: 0.760778, acc: 0.402344]\n",
      "921: [D loss: 0.684232, acc: 0.570312]  [A loss: 1.160247, acc: 0.035156]\n",
      "922: [D loss: 0.671539, acc: 0.564453]  [A loss: 0.721444, acc: 0.488281]\n",
      "923: [D loss: 0.707092, acc: 0.550781]  [A loss: 1.143606, acc: 0.042969]\n",
      "924: [D loss: 0.672815, acc: 0.587891]  [A loss: 0.690025, acc: 0.582031]\n",
      "925: [D loss: 0.702229, acc: 0.537109]  [A loss: 1.068720, acc: 0.054688]\n",
      "926: [D loss: 0.677563, acc: 0.568359]  [A loss: 0.796395, acc: 0.363281]\n",
      "927: [D loss: 0.673638, acc: 0.566406]  [A loss: 0.938664, acc: 0.113281]\n",
      "928: [D loss: 0.658323, acc: 0.628906]  [A loss: 0.809321, acc: 0.304688]\n",
      "929: [D loss: 0.677643, acc: 0.566406]  [A loss: 1.020125, acc: 0.062500]\n",
      "930: [D loss: 0.655569, acc: 0.589844]  [A loss: 0.833069, acc: 0.269531]\n",
      "931: [D loss: 0.678576, acc: 0.589844]  [A loss: 0.984542, acc: 0.109375]\n",
      "932: [D loss: 0.673148, acc: 0.591797]  [A loss: 0.829765, acc: 0.273438]\n",
      "933: [D loss: 0.673176, acc: 0.564453]  [A loss: 1.069353, acc: 0.054688]\n",
      "934: [D loss: 0.665322, acc: 0.587891]  [A loss: 0.782902, acc: 0.367188]\n",
      "935: [D loss: 0.670205, acc: 0.566406]  [A loss: 1.081417, acc: 0.074219]\n",
      "936: [D loss: 0.654705, acc: 0.609375]  [A loss: 0.793557, acc: 0.339844]\n",
      "937: [D loss: 0.712984, acc: 0.541016]  [A loss: 1.157981, acc: 0.058594]\n",
      "938: [D loss: 0.699636, acc: 0.511719]  [A loss: 0.783186, acc: 0.367188]\n",
      "939: [D loss: 0.695351, acc: 0.552734]  [A loss: 1.047742, acc: 0.074219]\n",
      "940: [D loss: 0.669951, acc: 0.583984]  [A loss: 0.813584, acc: 0.292969]\n",
      "941: [D loss: 0.680040, acc: 0.566406]  [A loss: 0.990607, acc: 0.105469]\n",
      "942: [D loss: 0.678613, acc: 0.570312]  [A loss: 0.866547, acc: 0.187500]\n",
      "943: [D loss: 0.666810, acc: 0.595703]  [A loss: 0.956932, acc: 0.156250]\n",
      "944: [D loss: 0.681038, acc: 0.562500]  [A loss: 0.905060, acc: 0.191406]\n",
      "945: [D loss: 0.679113, acc: 0.580078]  [A loss: 0.873716, acc: 0.242188]\n",
      "946: [D loss: 0.687238, acc: 0.572266]  [A loss: 0.982379, acc: 0.070312]\n",
      "947: [D loss: 0.670989, acc: 0.591797]  [A loss: 0.846777, acc: 0.273438]\n",
      "948: [D loss: 0.703945, acc: 0.531250]  [A loss: 0.995345, acc: 0.085938]\n",
      "949: [D loss: 0.677609, acc: 0.560547]  [A loss: 0.822049, acc: 0.257812]\n",
      "950: [D loss: 0.677294, acc: 0.599609]  [A loss: 1.030828, acc: 0.070312]\n",
      "951: [D loss: 0.660109, acc: 0.585938]  [A loss: 0.862824, acc: 0.242188]\n",
      "952: [D loss: 0.694784, acc: 0.537109]  [A loss: 1.123833, acc: 0.039062]\n",
      "953: [D loss: 0.668951, acc: 0.589844]  [A loss: 0.722863, acc: 0.472656]\n",
      "954: [D loss: 0.705965, acc: 0.550781]  [A loss: 1.176357, acc: 0.027344]\n",
      "955: [D loss: 0.650715, acc: 0.617188]  [A loss: 0.785904, acc: 0.367188]\n",
      "956: [D loss: 0.721582, acc: 0.515625]  [A loss: 1.034627, acc: 0.074219]\n",
      "957: [D loss: 0.666585, acc: 0.591797]  [A loss: 0.849605, acc: 0.210938]\n",
      "958: [D loss: 0.681836, acc: 0.568359]  [A loss: 0.921782, acc: 0.132812]\n",
      "959: [D loss: 0.662120, acc: 0.611328]  [A loss: 0.847599, acc: 0.218750]\n",
      "960: [D loss: 0.674199, acc: 0.595703]  [A loss: 0.898163, acc: 0.195312]\n",
      "961: [D loss: 0.666073, acc: 0.609375]  [A loss: 0.917784, acc: 0.164062]\n",
      "962: [D loss: 0.660476, acc: 0.583984]  [A loss: 0.952280, acc: 0.128906]\n",
      "963: [D loss: 0.668702, acc: 0.576172]  [A loss: 0.951635, acc: 0.175781]\n",
      "964: [D loss: 0.686357, acc: 0.550781]  [A loss: 0.895596, acc: 0.175781]\n",
      "965: [D loss: 0.698358, acc: 0.533203]  [A loss: 1.032225, acc: 0.074219]\n",
      "966: [D loss: 0.663930, acc: 0.609375]  [A loss: 0.806574, acc: 0.316406]\n",
      "967: [D loss: 0.694232, acc: 0.537109]  [A loss: 1.144421, acc: 0.023438]\n",
      "968: [D loss: 0.677428, acc: 0.560547]  [A loss: 0.757148, acc: 0.417969]\n",
      "969: [D loss: 0.714177, acc: 0.529297]  [A loss: 1.101767, acc: 0.050781]\n",
      "970: [D loss: 0.675244, acc: 0.556641]  [A loss: 0.779852, acc: 0.332031]\n",
      "971: [D loss: 0.675821, acc: 0.564453]  [A loss: 1.025231, acc: 0.085938]\n",
      "972: [D loss: 0.656113, acc: 0.599609]  [A loss: 0.832178, acc: 0.253906]\n",
      "973: [D loss: 0.668199, acc: 0.578125]  [A loss: 1.038733, acc: 0.066406]\n",
      "974: [D loss: 0.664715, acc: 0.580078]  [A loss: 0.816254, acc: 0.308594]\n",
      "975: [D loss: 0.668684, acc: 0.603516]  [A loss: 1.078183, acc: 0.066406]\n",
      "976: [D loss: 0.666222, acc: 0.603516]  [A loss: 0.810075, acc: 0.265625]\n",
      "977: [D loss: 0.692540, acc: 0.550781]  [A loss: 1.132442, acc: 0.039062]\n",
      "978: [D loss: 0.662190, acc: 0.587891]  [A loss: 0.776124, acc: 0.386719]\n",
      "979: [D loss: 0.692130, acc: 0.560547]  [A loss: 1.023319, acc: 0.101562]\n",
      "980: [D loss: 0.661668, acc: 0.621094]  [A loss: 0.740718, acc: 0.414062]\n",
      "981: [D loss: 0.684292, acc: 0.552734]  [A loss: 1.075939, acc: 0.066406]\n",
      "982: [D loss: 0.657990, acc: 0.617188]  [A loss: 0.809826, acc: 0.332031]\n",
      "983: [D loss: 0.683860, acc: 0.576172]  [A loss: 0.986019, acc: 0.074219]\n",
      "984: [D loss: 0.678372, acc: 0.572266]  [A loss: 0.899077, acc: 0.179688]\n",
      "985: [D loss: 0.672645, acc: 0.576172]  [A loss: 0.951210, acc: 0.140625]\n",
      "986: [D loss: 0.664200, acc: 0.589844]  [A loss: 0.895864, acc: 0.148438]\n",
      "987: [D loss: 0.672504, acc: 0.599609]  [A loss: 1.003234, acc: 0.101562]\n",
      "988: [D loss: 0.672324, acc: 0.574219]  [A loss: 0.902134, acc: 0.171875]\n",
      "989: [D loss: 0.667872, acc: 0.587891]  [A loss: 0.950199, acc: 0.148438]\n",
      "990: [D loss: 0.677124, acc: 0.560547]  [A loss: 1.011197, acc: 0.101562]\n",
      "991: [D loss: 0.675539, acc: 0.589844]  [A loss: 0.860466, acc: 0.242188]\n",
      "992: [D loss: 0.703637, acc: 0.535156]  [A loss: 1.096630, acc: 0.054688]\n",
      "993: [D loss: 0.680414, acc: 0.548828]  [A loss: 0.730483, acc: 0.449219]\n",
      "994: [D loss: 0.711263, acc: 0.531250]  [A loss: 1.146768, acc: 0.046875]\n",
      "995: [D loss: 0.661922, acc: 0.611328]  [A loss: 0.785977, acc: 0.355469]\n",
      "996: [D loss: 0.701865, acc: 0.548828]  [A loss: 1.076025, acc: 0.054688]\n",
      "997: [D loss: 0.669683, acc: 0.595703]  [A loss: 0.752234, acc: 0.390625]\n",
      "998: [D loss: 0.674374, acc: 0.580078]  [A loss: 1.056184, acc: 0.058594]\n",
      "999: [D loss: 0.672773, acc: 0.593750]  [A loss: 0.818856, acc: 0.269531]\n",
      "Elapsed: 2.6292002255386775 hr \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAALICAYAAACJnL11AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3WmYXmWVL+5dZA4hMwkBIUCAIAYEWhMRmYdWEGRoBQRaryOgDBJEznFqPMoRRZqmFRFRFARxQomgjaCkGQOKEIQIQpMQhkSGJJAEMg/U/5OXf1nrwV15a3qr7vvj79rDQ+p5dy3eq9ZeLa2trRUAABBt1NULAACA7kqxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKCgb2ferKWlxbhAGtba2trSVfe2h2kPXbmHq8o+pn14FtPs6u5h3ywDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAQadO8AP4R1pa4kCl1lbDugA6wiabbJLmU6ZMCdn69etD9vvf/z5kK1eubHxh3YhvlgEAoECxDAAABYplAAAoUCwDAECBYhkAAAq8DaMJbLRR/H+aPn36pMeuXbu2o5cDbda3b3zUHHrooemxO+64Y8guvPDCkHlDBkDjVq9eneYvvPBCyHbeeeeQbbPNNiF74oknQrZu3boNWF334JtlAAAoUCwDAECBYhkAAAoUywAAUNDSmU0yLS0tOnL+f7KxvldffXXITjzxxJC99tpr6TX79+8fsmw8ZTNrbW2N/3CdxB7eMMcdd1zIfvCDH6THvvLKKyEbOXJku6+pK3XlHq6q7rePN9988zQ/7LDDQjZ16tSQzZw5M2SXXnppes37778/ZKXnKW/Ms7j5ZHVHo3Vg3ZcQlGqRrvz81d3DvlkGAIACxTIAABQolgEAoECxDAAABRr8GjBgwICQZX/oXlX5ZL1sUtnDDz9c+5qZ7NieNulMU0n3ljWQPPXUUyEbP358ev7s2bNDtsMOOzS+sG5Eg9/f+/rXv57mJ598csgGDRoUsrY0CLXleVpHqWkpm1rZ03gWdx8DBw4M2eDBg0O2ZMmSkHVWg93QoUPTfNNNNw3Z/PnzQ5bVUY2uXYMfAAA0SLEMAAAFimUAAChQLAMAQEHP70DoQFnjXDZBr6qqql+/fiHbbrvtal2z7r3bcj50lKxZY6uttqp9fjaNjZ5tr732SvOsSW7dunUhmzt3bshKjT/Zc7eRZrxsUllpTdtuu+0G3wf+KmuUO+2000KWNVtffPHFIVu9enX7LOwfKH3OfvrTn4Ys+6z8z//8T8j23HPP9Jrt3bTom2UAAChQLAMAQIFiGQAAChTLAABQoMGvAWvWrAlZqcEu+0P7lStXhqzudKlskg10B5/97GdDlu3/UgPGSSed1O5rons7++yz0/yLX/xiyG666aaQ/cd//EfISvsrm3Q2ceLEkL3nPe8J2U477RSyD3zgA+l9ttxyy5C94x3vCNl9990XMo3avJEpU6aE7NRTTw3ZVVddFbKurB023njjNH/Tm94Usqxx9tFHHw1ZZ31WfLMMAAAFimUAAChQLAMAQIFiGQAAChTLAABQ0NKZXbctLS29tsU3exvARz7ykZBdccUVta539913p/nee+/dtoU1odbW1viP2Ul68x6u69VXXw3ZkCFDQnbzzTen5x9yyCHtvqbupiv3cFV1v31cegvQDjvsELIFCxaEbPHixSEr/W7L7pVl2VjtzHnnnZfm//Zv/xay7A1IW2yxRciWLFlS695dzbO4Y5U+Fw8++GDIxowZE7JstPuKFSsaX1gN2dr/67/+Kz12m222Cdk3v/nNkGX1UaOjuuvuYd8sAwBAgWIZAAAKFMsAAFCgWAYAgALjrjtJ1myy1157bfD1vvrVrzayHGgX/fv3D1k20jQbPZw1uNI7lZrxsj1y//33h+wXv/hFyEpjfbO9WBqNXUdp7evXrw9Z1pjdLM18dL599tknzSdNmhSyn//85yHrrGa+bDR11sx34IEHpudfeOGFIbvssstC1sjntFG+WQYAgALFMgAAFCiWAQCgQLEMAAAFJvh1oVmzZoVs5513rnVu1kRVVZ33B/1dydSo7uMd73hHyH73u9+FLJvqN2LEiPSaWWNUV5owYULIXnjhhfTY7POXPWNN8Pt7AwcOTPOXXnopZM8880zIdt1115CtWbOm8YW9zqGHHhqy7373u+mxL7/8csimTJkSsmXLljW+sC7iWdx++vXrF7Lly5enx/btG9/NMHLkyJB1RPNo1syX3Seb2lqqT7JjO6s2NcEPAAAapFgGAIACxTIAABQolgEAoMAEvy609dZbb/C5K1eubL+FwAYqTWR6vaxRqy0NHBttFP+/ftNNNw3ZsGHDQrbZZpul13zsscdCtnjx4pBljWKl5rHObJjuSbIGn6qqqpaW2HvzyiuvhGzdunUN3f/II48MWTYRLduHJTfeeGPISlMF4cknnwxZ1vRXVflzqrMmQd5+++0hq9ugt80226TXbIbnpm+WAQCgQLEMAAAFimUAAChQLAMAQIFiGQAACrwNowsNHjy41nFZ530zdI/S85122mm1jsvealDq9M7ebDBp0qSQXXrppSEbM2ZMyEpvrjjiiCNCtmjRopDNmzcvPZ/2k3X3V1VVzZw5M2TZW1De/OY3h+zyyy9Pr/mud72rjat7Y6+99lqaZ2Pbsz272267hew///M/Q/bTn/40vU+jbwKh82XPvi233LL2+RMnTmzP5RRlz+26n59Zs2aFbMGCBQ2vqav4ZhkAAAoUywAAUKBYBgCAAsUyAAAUaPDrQn369Kl13BNPPNHBK4F/bOjQoSEbO3ZsrXNfffXVkJVGB2efi0022SRkDz30UMiyBr/p06en93n66adDpnG2a2TNcFVVVZ/5zGdClv08s2aitoymruuBBx4I2fve97702BEjRoTs97//fciyUcHXXnttyM4777z0PrvsskvIli9fnh5L93D11VfXOq7UPJo1IneE3/zmNyHLmv6yJtO3v/3tHbKmruKbZQAAKFAsAwBAgWIZAAAKFMsAAFCgwa+TDBw4cIPP/clPftKOK4ENc/LJJ4esbhPVlVdeGbJVq1alx2ZNdo899ljIpk2bFrJs2t5zzz2X3qfUVEb3UbcxNNszpWbNuXPnhiybALh27do6SyzK9t1WW20VstmzZ4ds1KhRIdtmm23S+zz88MMh23nnnUO2cuXK9Hw61siRI0P2gQ98oNa5pQa/9jZ69Og032+//UKWfS6mTp1a67hm5ptlAAAoUCwDAECBYhkAAAoUywAAUKDBr5Mce+yxG3xuNmkMOkppsuQZZ5yxwdf89re/HbK2TMtbtmxZyJ566qmQLVy4MGQam7q/bCpYVVXVe9/73pBlU8Xe//73h6zUQNqVFi9eHLJs6mTW1P0v//Iv6TWzpsFDDjkkZL/4xS9C1lkNZL1BaQ9nUygbnS656aabhix79mX3yY7Lpk1WVf6Mzppud9hhh1rXXLp0aXqfZtiHvlkGAIACxTIAABQolgEAoECxDAAABS1tabJp+GYtLZ13sy5S+sP9+fPnh2zcuHG1rnnqqaeG7IorrkiP7Q1TyVpbW/NOik7QG/ZwqVElm6I3ceLEkGXPlL59Yy9xW5o6ss9V//79Q7ZmzZqG7tNZunIPV1X328eDBw9O87e97W0hmzFjRsi648+4EYMGDQpZ1lhVVXlD7m233RayrOlv9erVG7C6v/Es/psBAwak+Z133hmyyZMnhyx77pbqs+wzMH78+JBlzZ9tkd0/+6xlxy1atChkpQa/bKLhI488ErJ+/fqFrLP2sG+WAQCgQLEMAAAFimUAAChQLAMAQIFiGQAACoy7bkDW4X/OOeekx2YjTTNr164N2c477xyysWPHpucvWLAgZOvWrat1b6iqcgf2gw8+GLLsbRhZV3ejXczZmrLPSk97K0Jvd9ddd3X1ErpENqq7LW+uykYNZ58X2k/p5/P000+HbPfddw9ZVk+Urrn99tuHbLPNNvsHKywrvUXr1ltvDdmyZctCtvfee4csewPYsGHD0vtsvvnmIcvevpS9+aWz+GYZAAAKFMsAAFCgWAYAgALFMgAAFGjwqykbt/vTn/40ZIcffnh6ftb0lDXeZSMiJ0yYELL/9b/+V3qf7373uyHLmv40QvU+2R5uyz6YN2/eBt/7qKOOCtmPf/zjDb5eVdnDzSobU97oyNqeJvustqW56ayzzgqZz0vHWrNmTZp/+MMfDtlf/vKXkB1wwAEhK70YoDRau44zzjgjZJdddll6bFuaSjtDVzap+mYZAAAKFMsAAFCgWAYAgALFMgAAFGjwq2mXXXYJ2aGHHhqyrDGjqqpq8eLFIbvmmmtCdtttt4Usm+CXNclUVVUNGTIkZC+99FLIsj/c725/zM+GyxpK62Yl2T6qa4sttghZ6bOSNSLZm91f1oB20UUXhSxr9syam6qqqubMmdP4wrq57DOYNWqXPqsrVqwI2d133934wmgX2TTGz372syEbOnRoyD760Y+m1zzmmGNCdsMNN4TspJNOCpln6YbxzTIAABQolgEAoECxDAAABYplAAAoaOnMP/ZuaWlpir8sz5rn7r333pDtvvvuta/56KOPhuzEE08M2Ysvvhiyk08+OWRHHHFEep+lS5eGLJuU9uCDD4bskUceCVlpYs769evTvDO0trbW70prZ82yhzODBg2qlW211Vbp+d/61rdCNmXKlJBlkynHjx8fsueffz69T2/QlXu4qhrfx1mz2cYbbxyymTNnhmzrrbcO2fLly9P77LjjjiHLJpJ2R8OGDQvZtGnTQrbnnnuGrF+/frXvc9xxx4Xsuuuuq31+IzyLO1ZpH2Sfv9IEQd5Y3T3sm2UAAChQLAMAQIFiGQAAChTLAABQoMEvMXz48JDNnj07ZKNHj27oPlmTXPZH+gMHDgxZWyavZT/jlStXhuw73/lOyL7+9a+n13zmmWdq3acjaCrZMNl0x8MOOyxkWUNpVVXV2LFjQzZhwoSQ3XfffSHbb7/9QpZN6ustmr3Br65sSuM+++wTsltvvTU9P5sK2BtkjdXLli1Lj80aJl955ZX2XlLKs5hmp8EPAAAapFgGAIACxTIAABQolgEAoECxDAAABd6GUdPEiRND9tBDD4Use3NFd5S9iWD69OkhO/7449PzFy1a1O5rqksHdtfI3sCSZdkzpTOfM82gt7wNo64//vGPab7rrrt28ko6Vva2oy984Qshmzt3bsje8Y53pNf84he/GLIlS5a0fXEbwLO4Y5XeeuV52n68DQMAABqkWAYAgALFMgAAFCiWAQCgQINfAwYNGhSy008/PT3285//fMiy8cPZz2PVqlUhW7hwYXqfJ598MmRPPfVUyC699NKQ/fnPfw5Z1pDS1TSV0Ow0+P29zTffPM3POeeckC1fvjxkW221VcgOOuig9JpjxowJWfbczZrk7rnnnpB96lOfSu8ze/bskNUd8Z41dk2ZMiU9dsWKFSGbNWtWrfs0yrO4/WQ/89K493Xr1nX0cnoNDX4AANAgxTIAABQolgEAoECxDAAABRr8aDqaSmh2GvxoL3379g1ZZzWAeRbT7DT4AQBAgxTLAABQoFgGAIACxTIAABTEzgAAoCmY5gYdzzfLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAQUtra2tXrwEAALol3ywDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKCgb2ferKWlpbUz70fP1Nra2tJV97aHaQ9duYeryj6mfXgW0+zq7mHfLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgIK+Xb0AAACaU//+/UO2fv36Wlmz8M0yAAAUKJYBAKBAsQwAAAWKZQAAKNDg1wT69o0/pne+853psZ///OdDtt1224Vs0KBBIVu1alXI/ud//ie9z2GHHRay1atXp8cC9ASTJk0K2dSpU9NjP/vZz4Zs4cKF7b4meoaNNorfXba2tqbHlvLOMGTIkJBddtllITv66KNrX/OEE04I2S9+8Yu2LayD+WYZAAAKFMsAAFCgWAYAgALFMgAAFGjwq6mlpSVk2R/kv/baa+n5Y8aMCdmjjz4ashEjRtS6T2fZaqut0vyVV14J2cc+9rGQ/fCHPwzZmjVrGl8YbZbt4WzyUql5ZN26dSEr7XdoJscee2zIsmdXW57FCxYsCNnnPve5ti2MHinbR6eeemrIsrqhqqrq0ksvDdmiRYtCVrcRMPvdUFX5Ovfaa6+QHX744SEbPHhwrXtXVVVdf/31Idtvv/1Cduedd9a+ZnvzzTIAABQolgEAoECxDAAABYplAAAoaOnMSTAtLS1dN3amQdkfwA8YMCBkWRNUVVXVDjvsELI77rgjZMOHDw9ZNsGvJLv/n//855CNGjUqZOPGjQtZnz59Grr3gw8+GLI99tgjPb9us1hra2vejdAJmnkPjx49OmRf+cpXQnbEEUfUvua1114bsnPOOSdk69evr33N7ib77Df63OzKPVxV3W8fb7zxxml+5JFHhizbn295y1tCts0226TXzJ7bdWU/9zlz5qTH7rrrriFbsWLFBt+7O/Is3jDZVN2s4b/0+/fhhx8O2cEHHxyyl19+udY12/J7fueddw7ZFVdcEbK3vvWtISs1EmamT58esoMOOqj2+XXV3cO+WQYAgALFMgAAFCiWAQCgQLEMAAAFimUAACgw7roB2TjHZcuWpcfOnj07ZFtuuWXI1q5dG7JBgwaFrDQyOnujRJZlb9jYZZddQvblL385vc+b3/zmkI0dOzZkWeds9laGqsrHw9J+Jk2aFLJ//dd/DVk2Aruq8jda3HLLLSHriBHYdccMZ28rGDhwYHps9gaGVatWhSz7TK5evbrWeqin9LaUyZMnhyx7Q0ZbxlDXvf95550Xsi996UshM/KdtrrqqqtCVnruZpYvXx6yUk1QR+kNMVmdsGTJkpDNmDEjZNnzdfvtt6+9pt122632sZ3BN8sAAFCgWAYAgALFMgAAFCiWAQCgQINfA7Jxji+++GJ67BNPPBGyuk1C2R/zNyprWpo5c2bI3vOe96TnZ+Mxd9ppp5CdeOKJIeuI/x7+saxhol+/fiHLmtyqKh+R3hHje7ORqFmWNVZlTTLjx49P7/Pkk0+GLPtc0PFKe+7MM88M2QMPPBCy733veyErjda98847Q5aN0dW4R3vIGoz32GOPWudmDctVVVU/+9nPQpb9Xs3Oz5pZV65cmd4ne4lB1kiYjabO6olLLrkkvc+QIUNCNnz48JCNHDkyZNlI747gm2UAAChQLAMAQIFiGQAAChTLAABQoMGvpmxC1Fe+8pWQ7bDDDun52fS05557rvGFdbBSk0uWP/zww7UyusYxxxwTsqzJdNy4cen57d3MV5q6luVZs9awYcNC9slPfjJkjz/+eHqfUk73ds0114Qs25vZRNKqypuMNPPRUbI6IWuQz5Sa1y6//PKQ1d3DWdNfqbF56dKlIcumFGfHZevJpvxWVVWdc845Icua+Y4//viQfeMb30iv2d58swwAAAWKZQAAKFAsAwBAgWIZAAAKNPjVlE3hmTx5cshKU6OeeeaZkE2YMCFkzz777AasDv5etg+HDh0ashkzZoRsyZIlHbKm12tL82jW9HfBBReE7IgjjgjZcccdtwGro5n8+te/DlmpgTSb1nfDDTeELJsqWJqoBn375uXUxz/+8Q2+5tFHH53mXTlpNJsAWJrA+Xo33XRTmh9wwAEh23///UOWTfrrLL5ZBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKPA2jJr69+/f0PlZp+xjjz0Wsj333DNk2choXdm8kWy/Pf/88yGbPn16ZyynYdkI7mOPPTZkCxYsCNnTTz/dEUuiG8nGtp9++unpsXvttdcGX/OUU04J2bXXXpue7xndu3z6059O87qjrbO3AP3ud79raE0dIXvLzLBhw2qdu9NOO6X5mDFjap3/+OOP1zquI/hmGQAAChTLAABQoFgGAIACxTIAABS0dGYTQktLS9N2PGR/1P7Vr341ZB/84AfT80ePHh2yfv36hSwbU5z9jObOnZveZ+LEiSHLxlM2s9bW1nymeCdolj08aNCgkH35y18O2aJFi0J2/vnnd8ia6srGcr/00kshyxpisnHXv/nNb9L7lMZtv17dz2RbdOUerqrm2cd1ZQ3Yy5YtS4/Nnrsd4ZFHHgnZrrvuGrJmfj731mdx9kxYuXJleuyAAQNqXXPmzJkhmzx5cnps3WdXo7Lx0qeeemrI3vKWt4Tshz/8Yci23nrr9D7Z+YcffnjIsn+P7HdYW9Tdw75ZBgCAAsUyAAAUKJYBAKBAsQwAAAUa/DrJtttuG7IHHnggZCNGjGjoPtnPM5t0dt111zV0n67UW5tK2iKbGvWlL30pZKeddlrIrrzyyvSa2YSqbMpZXVmTTOma2X/P29/+9pA9+OCDG7yezqTBr31le+nOO+9Mj607wa8jrFu3LmSbbbZZyLKG1u6otz6LJ02aFLJZs2alx2Z7c82aNSGbOnVqyLImuarK99Emm2wSsqy5MJsSvMsuu6T3OfTQQ0OWTYe98cYbQ/a1r30tZGvXrk3vkzVHDh8+PGTZhNZGmx01+AEAQIMUywAAUKBYBgCAAsUyAAAUaPDrQiNHjgzZM888E7Jsik6j/vmf/zlkv/3tb9v9Ph2htzaVNOqSSy4J2cc//vGGrplNKdt///1Dlk0py5o1qipv5vvEJz4RsqyBpFlo8Ot4pclp48aNC1k2NXLs2LEh+z//5/+EbL/99kvvk+3j7PftRz7ykZBdffXVIeusqW1t0RuexVmD3o9//OOQHX300en5WVPbs88+G7KFCxeGbMqUKek123sKZakOXLp0acjuu+++kJ1zzjkh+/Of/1z7Pp1Zhyb31uAHAACNUCwDAECBYhkAAAoUywAAUKDBr0mVpp9ddNFFITv77LNDlk2IGj16dOML6wS9oamkI2TTIbOmkqwxqS2yZ0ppv2Zuu+22kB1wwAENram70eDXnLJ9fMIJJ6THZs/i5557LmTvete7QrZ8+fINWF3n6w3P4qyZ7owzzgjZ5MmT0/PvuuuukGXT6c4888yQZdMdO0Jpst60adNC9rOf/Sxkt9xyS8h62h72zTIAABQolgEAoECxDAAABYplAAAoUCwDAECBt2H0AtnPePXq1SEbOHBgZyynYb2hA7uzbLRR/P/l0nj1Usf0691www0hO/jgg2uvKXsbR3cc9dsIb8NoX9lbKjrid1u2N88///z02O233z5k2Xj57A0ZzaI3PIuzZ2SWtWWUc/Y2jC996UshO/7449Nrrlq1KmQXXHBBeuzrbbvttiEr/e6fP39+yG666aaQZaOtV6xYUWs9Xc3bMAAAoEGKZQAAKFAsAwBAgWIZAAAKNPj1MFkDyrp160K2YMGCkI0dO7ZD1tTeekNTSTOruwdL2jIau1lp8Ntwffv2DdmoUaNClj3jqqp+41/WxHXSSSfVyqqqqk455ZSQPfTQQ7Xu3Sx6w7O4I5pHsxHa48ePD1mpqXrevHkhy5qgs7W///3vD9nll1+e3if77/zQhz4Usptvvjlk69evT6/Z3WjwAwCABimWAQCgQLEMAAAFimUAACiInRI0tYULF9Y6bv/99+/gldBbNUtjB91f1iw6ceLEkB199NEh+8Y3vpFe85VXXglZ1jR48sknh+yzn/1syK6++ur0PrNmzUpzyBr35syZ0+73yaax/uAHPwhZ//790/OzBr8dd9wxZNlUv57GN8sAAFCgWAYAgALFMgAAFCiWAQCgQINfk3rxxRfTfMSIESHLpkY9+uij7b4mqKr6E/g0AvKPDBo0KGQDBgwI2dChQ0N2+OGHp9fM9t1+++0Xsg984AMhy6b6XX/99el9solqNJ+BAweGLJtIWpq211lGjx4dsmzSX6mZL5Pt4VtuuSVknTkJuqv4ZhkAAAoUywAAUKBYBgCAAsUyAAAUNFWDX79+/UI2efLkkF166aUhy/5If/r06el9vve974Usa5LrCBdccEHIPvWpT9U+f+XKlSHbfffdG1oTtMXmm29e67hnnnmmg1dCs1u2bFnIssl4L7/8csimTJmSXvPQQw8NWdbMlzVCZY1dpYaprNG1NzRCNbNsYuQnP/nJkO29994hu/HGG9NrZtPtssa7rPE0a2atqqr69Kc/HbIvfOEL6bF1vPrqq2k+duzYkGU1Rm/gm2UAAChQLAMAQIFiGQAAChTLAABQoFgGAICCpnobxrBhw0K2xRZbhGz8+PEhGzJkSMh23HHH9D5nnHHGBqyuc5VGqQ4ePLiTVwJ/7+KLL6513JFHHtnBK6Enyt5YMHz48JDtuuuu6fkHHHBAyOqOAM6eu1OnTk2PXbRoUciefvrpkK1Zs6bWvel42c83e0vF/vvvH7KDDjoovWb2dq6udO+994Zsr732So81sv1vfLMMAAAFimUAAChQLAMAQIFiGQAACpqqwS9rmLjuuutC9qtf/Spk2QjeH/7wh+l9shHa2ejSznLttdeG7MQTT+yClcA/tmrVqpBl41QfffTRzlgOTSxr5nvzm98csj333DNkWSNfVVXVyJEjQ5Y1cWX7ePbs2SFbsmRJep+DDz44ZNdcc03INPh1H9k48n//938P2ejRo0P2iU+dh7/PAAAgAElEQVR8Ir1mZ9UO2drPPffckJ1//vmdsZwexzfLAABQoFgGAIACxTIAABQolgEAoKAl+6PwDrtZS0vn3awBG20U/x8ia9b4whe+ELJsemBVVdWcOXNC9u53vztky5cvr7HC3q21tbXLui2bZQ93pVGjRoUsa9RasGBBZyynW+rKPVxVzbOP6zb4HXfccSE7+uij02tus802Icua7H7yk5+E7Jvf/GbInnrqqfQ+69atC9mKFStC1pm/g9ubZ/HfDBgwIM0vueSSkJ1wwgkhy6ZIZhMfq6qqTj/99JDdeuutIWvmvdVZ6u5h3ywDAECBYhkAAAoUywAAUKBYBgCAAg1+NB1NJTQ7DX5dZ+DAgSHLmrpXrlwZMg1Tf8+zmGanwQ8AABqkWAYAgALFMgAAFCiWAQCgQLEMAAAFfbt6AQDQWVatWtXVSwCajG+WAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoKCltbW1q9cAAADdkm+WAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAICCvp15s5aWltbOvB89U2tra0tX3dsepj105R6uKvuY9uFZTLOru4d9swwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgoFOHkgD8VUtL/i74LH/ttdc6ejkAkPLNMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIG3YQBdYqON8v9X9zYMmknprS677bZbyGbNmhWydevWtfua6NmyPTdhwoRa2YwZM0K2fPny9llYD+abZQAAKFAsAwBAgWIZAAAKFMsAAFCgwQ/ocH37xkfNSSedlB47e/bskP33f/93u68J2mrEiBEhe/HFF9Nj+/TpE7JDDz00ZLfcckvjC6NX2XvvvUN22223hSxrol62bFnIdthhh/Q+zz///AasrmfyzTIAABQolgEAoECxDAAABYplAAAo0OBX08Ybbxyyn/zkJyE766yz0vOffPLJdl8TdEdZU8k999wTst133z09/6GHHgrZ5MmTQ9ba2roBq4N6sqbUhQsXhixr5KuqfOpk1rwKbbXPPvuErDQR9fUGDRoUsokTJ6bHavD7G98sAwBAgWIZAAAKFMsAAFCgWAYAgAINfjXNmDEjZLvuumvI3vve96bnr1mzJmSrVq0KWfbH91mjSdY8UlVVde6554bsK1/5SnosdIRsSlnWoFcyf/78kHW3Zr6Wlpbax3a3tRP1798/ZIsXLw5ZqZkvs27dupDtu+++IVuwYEHIsilr9hF/9ba3vW2Dz82aVLM9yN/zzTIAABQolgEAoECxDAAABYplAAAoaOnMpoGWlpam7VDIGnqGDRsWsg996EPp+Vnj3ahRoxpf2OtkTSWnnHJKyK666qp2v3dnaW1trd9d1c6aeQ93lhUrVoQsa1wt2W677ULW0yZgduUerqresY9LE83e9773hWzatGkbfJ/S79CsgfuBBx4I2cUXXxyy3/72tyHLPlddzbO4Y2WNp1WVN+kNHTo0ZNmLAH75y1+G7NRTT03v8+KLL4aspzWa1t3DvlkGAIACxTIAABQolgEAoECxDAAABYplAAAo8DaMTpKNrH7sscdCds8994TstNNOC1n21ouqyjtVs47Y9evXp+c3Ax3Y3ceAAQNClr0FIJONE66qqho5cmRDa2oG3oax4bI3E2VvW7nzzjvT8xsZFZy9keKmm25Kj503b17IHnnkkZDdfPPNIWuWtxB4Fnesr371q2n+v//3/w5Z9rnIxqb/0z/9U8jmzp2b3qdUZ/Qk3oYBAAANUiwDAECBYhkAAAoUywAAUBC7zug0t956a8jOOOOMkGUNetAdHHLIIRt87sknn9yOK6E3O+aYY0LWSCNfVeX788orrwxZ1lhVVVU1YsSIkL3pTW8K2dKlS0PWHZv56FjZfjn77LPTY0t77vW+//3vh2zOnDkha7TGyNbT0/awb5YBAKBAsQwAAAWKZQAAKFAsAwBAgQl+neSEE04I2QsvvBCy6dOnt/u9N9oo/j9R9nNvlj/INzWq+8gmjY0ZMyZk2d7q169fes1mni5Zlwl+G27w4MEhy5qWxo0bV/uaF1xwQcg+85nPtG1hr9O/f/+QDRkyJGRZg1+zfAY8i9vPgQceGLLf/va36bFZQ102bW/LLbcMWVZ3NCrb62vWrGn3+3QEE/wAAKBBimUAAChQLAMAQIFiGQAACkzwa2cf+chH0vxrX/tayD7wgQ+0+/2zZr5tt902ZE8++WS735uebdiwYSEbNWpUrXOzZo9maWKi62SNTFnj3dixY0NWalj+05/+VOuajcqmoi1fvjxkzdJYTceaP39+yEp7o+7EvKyhtNFpe1mNkTXdZg2HzTyN2DfLAABQoFgGAIACxTIAABQolgEAoECxDAAABd6G0YCBAweG7PLLL0+PzbpNt9tuu5ANHz48ZNlbAyZPnpze50Mf+lDILrnkklrrgTey/fbbhyzrrM788pe/bO/l0AtMnDgxZJ/85CdDlnXZz5gxI73mYYcd1vjC/n+ytwNUVVX17Rt/vWbHZmtv5rcGsGGyt0e05fd0duz+++8fsmeffTZkbRlNnT3z+/XrF7I+ffqErJn3tW+WAQCgQLEMAAAFimUAAChQLAMAQIEGv8SAAQNCtmTJkpBlDX5t8fWvfz1kWTNeW2R/QH/VVVc1dE16l1LT3pZbbhmytWvXhqx///4hO++88xpfGL3Ob37zm5Blz925c+eG7IQTTkivmY2cbkTp8zJo0KCQ7bLLLiEbP358yG688caQrVixIr1P9hmk+XzkIx+pfWz2M//LX/4SsuyFAUOHDg3Zyy+/nN4nqyeyRsKsma/U+NqsetZ/DQAAtCPFMgAAFCiWAQCgQLEMAAAFvbrBL/uj9KqqqsceeyxkjTbzZepOP2uL7I/qsz/oh5KsQa+qquqDH/xgyLIpZVkDyJw5cxpfGD1W1ohUVVX1pje9qdb5n/vc50L23HPPpce29/TS0vWy3xnHHntsyA455JCQ7bvvviH72te+lt7nT3/6U+010T2MHDkyZNlkytLP8Rvf+EbIfvCDH4TsqKOOCtm3vvWtWudWVd5gm/1+yD6n2UsRmplvlgEAoECxDAAABYplAAAoUCwDAEBBr27wW79+fZpPmDAhZAcccEDIHnjggZAtXbo0vWbWeJc1R2222WYhO/vss0N2xhln1L7PxIkT02MhU/pcbLHFFiGrO6Up2+vwV3vssUeaZ03Qq1evDtkvf/nLkHVWk1s25ayqquqVV16pdX7WgL399tuHrPQZ0szXfP7v//2/IevXr1/IFi9eXPv87HNxwQUXhOzggw8O2ZFHHpne5+abb661zmx64Ec/+tH0ms3KN8sAAFCgWAYAgALFMgAAFCiWAQCgQNdNImuYmD59ekPXzJqmsuyZZ54J2dSpU0M2b9689D4XXnhhyD796U/XOg6qqqrWrVuX5qeddlrIHnzwwZBlTVnvf//7Q3bVVVdtwOroibIpdlWV76WsiXrVqlXtvqZGbbzxxiE7/PDDQzZs2LCQbbvttiE74YQT0vs8+uijIcuaveg+DjzwwFrH3XXXXWm+YsWKkGV1y3333ReybDpkqXk0e7HBgAEDQrZw4cJa62lmvlkGAIACxTIAABQolgEAoECxDAAABYplAAAoaOnMjsWWlpae1R7ZhTbZZJM0X7RoUcj69OkTsmxkZbN0r7a2tsYW+U7Sm/dwNto6e3NG9gaDF198MWTZaPfeoiv3cFV1v338rW99K80/9rGPhSwbIz169OiQrV27Nr1mtj8zdZ+H/fv3T/M///nPIZswYUKta65ZsyZk2ZuSqip/S83cuXNDts8++4Rs2rRp6TWXLVsWsuztTZ7FG+b5558PWfY8vPvuu9PzszdaZPt10KBBIctGqb/zne9M73PxxReHbODAgSHLPpPZW166o7p72DfLAABQoFgGAIACxTIAABQolgEAoMC46ya1fPnyNH/iiSdC9pa3vCVkY8aMCVnWhAV/9dprr4Usa/rJRqcOHz68Q9ZEzzBz5szaxw4dOjRkN954Y8g+8YlPpOdnTU9ZQ9urr74asqwR6pprrknvM2TIkDR/vewzdPDBB4dsxowZtc/PZCOJS2PCm6XZu1mtXLmy1nFvfetba18z+5llY7EffvjhkM2ZMye95oUXXljr3tnnp6fxzTIAABQolgEAoECxDAAABYplAAAo0ODXpEoNGHfccUfIdtppp5C9+c1vDpkGP95INvmsbiPQCy+80N7LoQf57W9/m+bZ/sr24V577RWy008/Pb3mfffdF7L9998/ZJMnTw7ZpEmT0mvWtXTp0pB1VvNr1rBI13jyySdDts0224Qsa2atqqr64Ac/GLJrr712g9eT1QNVVVWDBw+udX7WSNjT+GYZAAAKFMsAAFCgWAYAgALFMgAAFGjwa0CfPn1qHztw4MCQbb311iEbNWpUyLJmgAULFqT3yaanZROe6k4Qgr/K9uZGG9X7/+2rrrqqvZdDD/LSSy+lefacyp6l2XTJ97znPek1DzzwwJBtv/32IcuepW1x8803h+yQQw5p6Jr0DIceemjIVq9eXfv8H/zgByE74ogjQvbhD384ZIcddljIvv/976f3qft8z/Z6T+ObZQAAKFAsAwBAgWIZAAAKFMsAAFCgwa+mbJLNrFmzQrbtttum52dTpzLZxKrFixeHLPsD/6qqqqOPPrrWNRcuXFhrPfQ+/fr1S/OpU6fWOn/ZsmUhu+SSSxpaEz3bFltskebr1q2rdf6AAQNClk1Eq6q8aamR5/NBBx2UHvvf//3fta5J77NmzZqQZVMkb7vtttrXzH73H3XUUSGru9dL1q5dG7Lvfve7DV2zGfhmGQAAChTLAABQoFgGAIACxTIAABQolgEAoMDbMGpasWJFyB566KGQld6GUVfWqTpy5MiQnXnmmen52djXbNz15ptvHrK5c+fWWSI9XDbWuqqqatdddw1ZNo74Zz/7WcheffXVxhdGj5W9QaWqquqJJ54I2c4771zrmqVRvXXfBpCNH95xxx1D9vTTT9e6HryR22+/PWSlt8Rkey57i1FHvPnihhtuCNljjz3W0H2agW+WAQCgQLEMAAAFimUAAChQLAMAQIEGvwZ8/vOfD9luu+2WHjtu3LiQ9e/fP2SNjGKtqqqaN29eyF544YVax9H7ZHur1Bh17733hmzYsGEhu/rqq0OWNZ7CXz3//PNpvvfee4ds3333DdnEiRND9u53vzu95qabbhqya6+9NmTf+c53QrZ8+fL0mtARnnvuuTTfZJNNQvaHP/whZJMmTQpZNrJ96dKl6X0+/OEPhywb4153LH0z880yAAAUKJYBAKBAsQwAAAWKZQAAKGjJ/ti7w27W0tJ5N2tnWSNU1lSy9dZbp+fff//9IevbN/ZXjh8/PmRZQ8u0adPS+zz55JNp/nqd+XNvb62trY2NJWpAM+/hRtVtBsya+Zp5v3WErtzDVdW79zHtx7OYZld3D/tmGQAAChTLAABQoFgGAIACxTIAABRo8KPpaCqh2WnwoyfwLKbZafADAIAGKZYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQ0NLa2trVawAAgG7JN8sAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAAChTLAABQoFgGAIACxTIAABQolgEAoECxDAAABYplAAAoUCwDAECBYhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQEHfzrxZS0tLa2fej56ptbW1pavubQ/THrpyD1eVfUz78Cym2dXdw75ZBgCAAsUyAAAUKJYBAKBAsQwAAAWKZQAAKFAsAwBAgWIZAAAKFMsAAFCgWAYAgALFMgAAFCiWAQCgQLEMAAAFimUAAChQLAMAQIFiGQAACvp29QIAINPS0hKy1tbWLlhJx9loo3rfWb322msdvBKgxDfLAABQoFgGAIACxTIAABQolgEAoECDXy+QNclkmQaS3qfu3qgq+4P20bdv/LUzbNiw9Nis+W3JkiUhW7t2beMLqyH7bPTr1y899oADDgjZddddF7IhQ4bUuvfKlSvTfMyYMSFbtmxZrWsC9fhmGQAAChTLAABQoFgGAIACxTIAABRo8GtSgwYNSvOzzjorZGPHjg3Z1772tZDNmzcvZKWmrp42Rau3yhqoNtlkk/TYVatW1crgjbz73e8O2dZbb50eO23atJCtW7euoftnTXr9+/cP2UknnRSy7LmZNSx2hFLj7fjx40P26KOPdvRyoFfxzTIAABQolgEAoECxDAAABYplAAAoaOnMRq2WlpYe1RWWNUdtueWW6bE77bRTrfMnTJgQsi9+8YshGz58eJ0lFmWNWX/4wx9C9swzz6Tn33777SH70Y9+FLLVq1dvwOreWGtra97p0gmaeQ/36dMnZFlz0kEHHZSef/TRR4dsr732Clm2r7MmpOy4qsqbR7Pm0+ze8+fPD1l3nDzYlXu4qjpvH2eT+Z5++umQlZ4z++67b8iyCX6NyprnskbXbO0jRoxo6N4vvfRSyJ577rmQXX/99en5l19+ecgWLFgQso74Xe9ZTLOru4d9swwAAAWKZQAAKFAsAwBAgWIZAAAKNPjVtNVWW4Vs+vTpIdt2223T87NmptJEps6Q/dzXr18fstKEtqwBZfLkySFbunTpBqzujWkq+ceyvTV48OCQZY13d911V3rNrFmrs6aX1bVmzZqQbbfddumxWdNgZ+ktDX777bdfyG6++eaQ/b//9//S888///x2X1Mjst8DDzzwQHps1viXNaBedNFFIXvhhRdC9sgjj6T3yZoOO6KxOuNZ3LFKNUK/fv1CNmTIkJCNGTMmZPfcc0/IRo4cuQGr+5u1a9eGLHvZQdZ4WlV5PTJq1KiQZS8hePjhh9NrZg3p2X00+AEAQIMUywAAUKBYBgCAAsUyAAAUKJYBAKCge7Wyd7LSuN2tt946ZDNmzAjZ2LFja1+zK2UdoOvWrat1XP/+/dNrDh06NGRZRyxdI+uiPvfcc0N22mmnhSzrqq6qqlq+fHnIshHa2Z7JjusI2b2zscV0juwZmY22vvDCCztjOQ1bvHhxyJ566qn02AEDBoQse0vFkUceGbK77747ZE8++WR6n+xZTvPJ9kv2zK2qznue1pW9nSN7o0v2xq2qqqply5aFLKsxst9rv/71r9Nrtveb3rpfZQcAAN2EYhkAAAoUywAAUKBYBgCAgh7Z4Jf9EXg2OvGyyy5Lz99jjz1C1lnNfK+99lrIVqxYEbJXX301PT/7A/psBHA2ujhrhCo1EgwaNChkWWNYtnY63ujRo0P2qU99qta5pRHn48aNC1nWmFFXaZRrNkL7wQcfDNmkSZNq3SdrNKFzZKOYv/SlL4WsOzYHZ/sze+6VGuyy/6bseTphwoSQ/e53v6uzxKqq8t8ZdG9Z89rChQtD1hGNfNm+XLlyZXpsNmL9V7/6Vciyz8DHP/7xkG2++ebpfbLaI/v8Zes888wz02u2N98sAwBAgWIZAAAKFMsAAFCgWAYAgIKmavDL/uD7uuuuC9k///M/hyyb7FVq0Mv+WD1romhLY0X2x/unnnpqyLJpNFnTXmkq2Yc//OGQHXXUUSHLGsCyiTelKTjZxJ5dd901ZLfeemvta9J+Hn/88VrHZXsraz6pqvZvwirtg+w+d9xxR8iyBr/smlnDIJ0jm9bXSFNoZ8p+3+y0004h22qrrdLzs985WfPsE088EbLvfe97IVuyZEl6n7qy/x7P4vZTqicOOuigkN188821rnn77ben+RFHHBGyV155pdY1O8uiRYtCduWVV9Y+P6uvTj/99JB11h72zTIAABQolgEAoECxDAAABYplAAAoaKrOl+wP6LM/dK/b0FNqmJgxY0bIsqaUbJrMjTfemF4za3RrZLpdqdnq+OOPD1nWeJf9W2aNjVkDWFVV1dKlS0OWNbpkP4vuOK2rWb31rW9N8xEjRtQ6P2vm6+qfT9aINGXKlFrnZs0e2WTJqqqqBQsWtG1htFnW5DN48OAuWMkby5rxzj777JBl0wfbMmXtLW95S8iyyZhZU1jWCFhVVTVz5syQZc1RAwYMCNnixYvTa/YG2e+miy66KGQ///nPQzZr1qyQnX/++el9sqa07Bl3zDHHhCx7gUGzeOc73xmy0tTWzF/+8peQff/7329kSQ3xzTIAABQolgEAoECxDAAABYplAAAoUCwDAEBBU70NI+t0L42YfL2sO/iWW25Jj806nrM3Z2Rvinj55ZfTa7b3GwY+9alPpfnb3va2kNXtQM3e7rF69er02BdffDFky5cv3+B7s2H++Mc/1j72Rz/6UcgaeSNLR9lll11Cln1+sre3ZGPlx4wZk95n7ty5G7A62qLuG3b69euXnl/3TRPZc6b0c3/f+94Xsq985Ssh64i3dmTrHDVqVMiytzI8+uij6TWzt3Y89thjIetu45C72g477BCyY489NmR77713yE499dSQZW+zqKr8Z569XauZ33yRyd4MdtJJJ6XHZrXd5z73uVrHdRbfLAMAQIFiGQAAChTLAABQoFgGAICCpmrwy5r0srHL2ajfrFHti1/8Ynqfuo0/AwcODFlp1HbW1FL3j9WzsZHnnntuemzdhrrs3llz4rPPPpue/8ADD4QsazbLGnzYMNnPvPTzzn6+2Sj0zpI1cF188cXpsVmjzOzZs0N21FFHheyOO+4IWalJNfu368oGkp4o+/ecP39+yLJx01VVVbvvvnvIzjjjjJBtu+22Idttt93Sa5ae0XWsWbMmZNl/T1XlY6wHDRoUsuzfKDsua8Cuqnz0cnds3O1u5syZE7KswS/bRy+99FLIsqb3qqqqjTfeOGTvete76iyxqf3mN78JWVv+je677752X1MjfLMMAAAFimUAAChQLAMAQIFiGQAACpqqwS+T/ZF+NsUua1R76qmn0mtm08KyppCsgWPKlCnpNbOmwVWrVoVs7NixIfvxj38cskYn42UTCbOmvVKjSLb2rAFFw1T7mTp1ashK/75nnXVWRy+nKPus3H///SGbNGlSen7WyDtt2rSQZRM423tSJu0va/otPc+y50/W9LfllluGrJFGvqqqqo997GMhu+KKK0KW7deqyqfLZlPJ/u3f/i1kbVl7qfGPN5Y1a955550hyxo4s8mSl112WXqfu+++O2SPPPJInSU2texz/h//8R/psVnzeXebOOmbZQAAKFAsAwBAgWIZAAAKFMsAAFDQ9A1+ixYtqnVc1ihSaszIDB48OGRnnnlmyE444YT0/KyBJVvT8OHDQzZgwIA6S6yqKm/4ypr5HnrooZAtWLAgZDvuuGN6nze96U0hy6anzZs3r9Ya+XtZc9Crr74asuxnVlV5Q1xd2QTMqsqbX7beeuuQZdPHsv+e0ucva1jabLPNap9P8ylN+nz66adD9vDDD4dsq622qn2v7PmTPQ/b0syXyY698sorQ5Y1+GWfly222CK9j+dp+8n+LZ977rmQZRN5v/3tb6fXbObnVDZZL2uMrNtYnU1crqqquuGGG0K2ePHiWtfsLL5ZBgCAAsUyAAAUKJYBAKBAsQwAAAVN3+CXNZplzXQTJkwIWTaBr6qq6oUXXgjZkCFDQpb98XvWmFFVVdWvX7+QDRw4MGR1JzeVmjqmT58esvPOOy9kb3/720OWNckMGzYsvc+mm24asj322KPWevjH+vfvH7I//vGPIXvmmWfS83faaaeQff7znw/ZiSeeGLJsX7ZFtjezhpiS7P7ZVM5Gp1jSfbTlubnJJpuELNtzpWdk1qD0mc98JmSNNmZl/0377rtvreMyI0eOTPPsc6Dpr/2Uptj2JKW99c1vfjNk11xzTciyxr2//OUvIdt+++3T+3z5y18OWTYluCv5ZhkAAAoUywAAUKBYBgCAAsUyAAAUKJYBAKCg6d+G8eKLL4Zs0qRJIcveZnHqqaem1zz//PNDlo0VPuOMM0J22WWXpdfcc889Q5aNOS11pb5e1tFdVVV16aWXhiwbGZuNKR46dGjIss7zqsrfBJJdU1d2+/nWt74VsieeeCI9Nuvkf/nll0OWjSnN3kBQVXnH8/HHHx+ybCx3Nn57zJgx6X2yN2dkHdilEcl0H9kz5bjjjgvZ6NGj0/Oz7vnsTTzZ3iy9yefxxx8P2R/+8If02DpKbzDaa6+9QpZ1/ffp06fWfdryLF62bFmta9L77LjjjiHLxr1XVb433/Oe99Q67u677w7Zpz/96fQ+S5YsSfPuxDfLAABQoFgGAIACxTIAABQolgEAoKClMxuwWlpaGrpZNhb04x//eMj+8z//M7t3yEr/7XPmzAnZRz/60ZA9+OCDIRsxYkR6zaxR5frrrw9ZqYnj9Urjg3/+85+HLGt+ydazevXqkGVjhquqql566aWQnXPOOSH79a9/HbJGx8i2trZ22ZzjRvdwXVnT0PDhw0NWaozoyua37LOWNaSWGqPmz58fsmwUezM3j3blHq6qxvdx9vP41a9+FbKs2Tp7jpd+ltlzbtGiRSG76667QlZqVL3ooqLvd8gAAAZtSURBVItC9sADD4Qs27NZI9M73vGO9D5ZQ+4222wTskGDBoUs+zcqPfMPOuigkN1xxx3pse2tNzyLm9m9994bsj322KP2+dnnMmvgzhoEjz766JBln92uVncP+2YZAAAKFMsAAFCgWAYAgALFMgAAFDR9g9/YsWNDNnfu3JANHDiwkVunsia3/v37p8dm9y81oNRRauBatWpVrTVlP/fly5eHrNRAdsopp4QsayopNaU0orc2lWT7v9FmyY6QTW3LmkxLxo8fH7Jnn322oTV1N83S4Jc1a1ZVVV199dUhO+aYY0JWeh62t+w5NXv27PTYW2+9NWRZc+K8efNCNnny5JD967/+a3qf7HdTJmusHjBgQMg233zz9PyskWrmzJm17t2o3vosblT2ucrqgdLP/N///d9D9i//8i8bvJ5sD1ZVVV1xxRUhu/baa0OW7bdmmbCqwQ8AABqkWAYAgALFMgAAFCiWAQCgoKka/OraeOONQ5b9ofpxxx3XGctpk+znkf3x/O23356ev9tuu4Vszz33DFk2nSpr0LvsssvS+2RNMp21lzSVdG+vvPJKyLLJlKWmko5oxu1umqXBr2TYsGEhGzduXMimTZsWsu222y5kjTQ7l5SaX9euXRuyrBnw0ksvDdmKFStC9tRTT6X3efzxx0OWNSJm68yaeUeNGpXe5+WXXw5Z9t/YETyLN0w2vXTMmDEhu//++9PzS41/dWSfyVJzYDNPSa1Lgx8AADRIsQwAAAWKZQAAKFAsAwBAQY9s8Ksrm5JUVVV17733hmz33Xdv9/tn//Z33nlnyI488siQrVy5Mr3mlltuGbLzzz8/ZNlEtAsuuCBkWfNIVXXtH/5rKuk+sillL7zwQq1z29Kw1NM0e4NfXVnj3rvf/e6QZU1HVZU3QjUqmyo6Z86ckGXrzPZ2qVFVc1TH6g3P4m984xtpftppp4Us229Zk+pZZ53V+MJ6EA1+AADQIMUyAAAUKJYBAKBAsQwAAAWKZQAAKGj/VuMmUupi/qd/+qeQtbTEhsl99tknZNdff316zWw87PLly0P2yCOPpOe/XmmUa9atffbZZ4dswYIFIeusEal0H9lY3ZJBgwaFLBsTnMne3tIb3nrR22XPlP/6r/8K2fbbb5+en70JKHsTwBZbbBGyPn36pNdcs2ZNyH70ox+FLHtGZr8zesNbL+gar776appne/imm24K2Sc/+cl2X1Nv5ZtlAAAoUCwDAECBYhkAAAoUywAAUNCrx113hP79+6f5KaecErL3ve99IcsaAb/97W+H7IYbbkjvkzVSZc0A69atS89vBkas/mNZQ2rWoLfzzjuHLGtcraqqOvfcc0M2ZMiQWut561vfGrJZs2bVOrcn6i3jrjvLwIEDQ7b77runx+6yyy4hu/nmm0M2f/78kGWjsnszz+L2s8kmm4Ts/vvvT4/NGloPOuigkP3+979vfGE9nHHXAADQIMUyAAAUKJYBAKBAsQwAAAUa/DpJ1vh3+umnh2zq1Kkhe/HFF0OWNQxWVVX96U9/Cllp2l+z0lSyYQYMGBCyESNGhOw73/lOev573/vekGWNhFlD6eDBg0PWm5ulNPjRE/TWZ/G4ceNCVvqdfOCBB4Zs4sSJIRs5cmTISlMos0mSWdPfSy+9lJ7P32jwAwCABimWAQCgQLEMAAAFimUAACjQ4NeFNtoo/r9KNomqp03ga1RvbSrpalkz4LRp00I2ffr0kJ1//vkdsqZmpcGPnqA3PIuzJualS5eGLJvA11Gypv0DDjggZHfccUcnrKa5afADAIAGKZYBAKBAsQwAAAWKZQAAKFAsAwBAgbdh0HR6Qwd2s8g6xbO3vPTm0dYZb8OgJ+itz+JNN900ZD/60Y/SY4cPHx6yhQsXhmzmzJkhu+eee9Jrzp8/P2SzZ88OWTYWm7/nbRgAANAgxTIAABQolgEAoECxDAAABRr8aDq9tamEnkODHz2BZzHNToMfAAA0SLEMAAAFimXg/2vfDkoAAGAYiPl3PQX3HRQSFUehAEAQywAAEF4PfgAAsMSyDAAAQSwDAEAQywAAEMQyAAAEsQwAAEEsAwBAEMsAABDEMgAABLEMAABBLAMAQBDLAAAQxDIAAASxDAAAQSwDAEAQywAAEMQyAAAEsQwAAEEsAwBAEMsAABDEMgAABLEMAABBLAMAQBDLAAAQDiyCGUiwRRWoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "DCGAN on MNIST using Keras\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "Dependencies: tensorflow 1.0 and keras 2.0\n",
    "Usage: python3 dcgan_mnist.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    # (W−F+2P)/S+1\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "        # In: 28 x 28 x 1, depth = 1\n",
    "        # Out: 14 x 14 x 1, depth=64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.4\n",
    "        depth = 64+64+64+64\n",
    "        dim = 7\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        (x_train, Y_train), (x_test, Y_test) = mnist.load_data()\n",
    "        x_train = x_train.reshape(60000,28,28,1)\n",
    "        x_test = x_test.reshape(10000, 28,28,1)\n",
    "        self.x_train = x_train.astype('float32')\n",
    "        self.x_test = x_test.astype('float32')\n",
    "        # normalize\n",
    "        #\n",
    "        self.x_train /= 255\n",
    "        self.x_test /= 255\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = np_utils.to_categorical(Y_train, 10)\n",
    "        self.y_test = np_utils.to_categorical(Y_test, 10)\n",
    "        \n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "            self.discriminator.trainable = False\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mnist_dcgan = MNIST_DCGAN()\n",
    "    timer = ElapsedTimer()\n",
    "    mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=50)\n",
    "    timer.elapsed_time()\n",
    "    mnist_dcgan.plot_images(fake=True)\n",
    "    mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/GAN_mnist.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First trial, without locking discriminator. Second trial with locking discriminator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.697947, acc: 0.519531]  [A loss: 0.760974, acc: 0.359375]\n",
      "1: [D loss: 0.695951, acc: 0.523438]  [A loss: 0.953906, acc: 0.125000]\n",
      "2: [D loss: 0.677682, acc: 0.556641]  [A loss: 0.753326, acc: 0.371094]\n",
      "3: [D loss: 0.696532, acc: 0.544922]  [A loss: 0.921155, acc: 0.160156]\n",
      "4: [D loss: 0.667644, acc: 0.589844]  [A loss: 0.735313, acc: 0.445312]\n",
      "5: [D loss: 0.707076, acc: 0.515625]  [A loss: 0.959221, acc: 0.121094]\n",
      "6: [D loss: 0.673640, acc: 0.576172]  [A loss: 0.754499, acc: 0.367188]\n",
      "7: [D loss: 0.689126, acc: 0.539062]  [A loss: 0.878590, acc: 0.171875]\n",
      "8: [D loss: 0.686064, acc: 0.554688]  [A loss: 0.813754, acc: 0.257812]\n",
      "9: [D loss: 0.684674, acc: 0.568359]  [A loss: 0.903288, acc: 0.156250]\n",
      "10: [D loss: 0.689662, acc: 0.521484]  [A loss: 0.781430, acc: 0.296875]\n",
      "11: [D loss: 0.699630, acc: 0.521484]  [A loss: 0.935372, acc: 0.105469]\n",
      "12: [D loss: 0.688363, acc: 0.539062]  [A loss: 0.735295, acc: 0.437500]\n",
      "13: [D loss: 0.701864, acc: 0.535156]  [A loss: 0.933866, acc: 0.089844]\n",
      "14: [D loss: 0.673895, acc: 0.576172]  [A loss: 0.774482, acc: 0.355469]\n",
      "15: [D loss: 0.693951, acc: 0.548828]  [A loss: 0.929511, acc: 0.101562]\n",
      "16: [D loss: 0.693174, acc: 0.542969]  [A loss: 0.719141, acc: 0.453125]\n",
      "17: [D loss: 0.709929, acc: 0.529297]  [A loss: 0.971600, acc: 0.070312]\n",
      "18: [D loss: 0.692608, acc: 0.539062]  [A loss: 0.712248, acc: 0.441406]\n",
      "19: [D loss: 0.706533, acc: 0.533203]  [A loss: 0.890011, acc: 0.195312]\n",
      "20: [D loss: 0.690276, acc: 0.554688]  [A loss: 0.767938, acc: 0.359375]\n",
      "21: [D loss: 0.694219, acc: 0.537109]  [A loss: 0.885222, acc: 0.175781]\n",
      "22: [D loss: 0.689241, acc: 0.570312]  [A loss: 0.802083, acc: 0.304688]\n",
      "23: [D loss: 0.678883, acc: 0.574219]  [A loss: 0.875043, acc: 0.164062]\n",
      "24: [D loss: 0.689574, acc: 0.554688]  [A loss: 0.774102, acc: 0.320312]\n",
      "25: [D loss: 0.700573, acc: 0.541016]  [A loss: 0.864093, acc: 0.136719]\n",
      "26: [D loss: 0.681027, acc: 0.587891]  [A loss: 0.787620, acc: 0.308594]\n",
      "27: [D loss: 0.677048, acc: 0.613281]  [A loss: 0.827236, acc: 0.246094]\n",
      "28: [D loss: 0.692546, acc: 0.562500]  [A loss: 0.874279, acc: 0.144531]\n",
      "29: [D loss: 0.698339, acc: 0.531250]  [A loss: 0.831376, acc: 0.250000]\n",
      "30: [D loss: 0.681672, acc: 0.566406]  [A loss: 0.840398, acc: 0.214844]\n",
      "31: [D loss: 0.688226, acc: 0.552734]  [A loss: 0.864015, acc: 0.195312]\n",
      "32: [D loss: 0.697602, acc: 0.505859]  [A loss: 0.801647, acc: 0.277344]\n",
      "33: [D loss: 0.693618, acc: 0.531250]  [A loss: 0.899852, acc: 0.121094]\n",
      "34: [D loss: 0.697531, acc: 0.519531]  [A loss: 0.826530, acc: 0.261719]\n",
      "35: [D loss: 0.703385, acc: 0.527344]  [A loss: 0.881467, acc: 0.156250]\n",
      "36: [D loss: 0.683535, acc: 0.554688]  [A loss: 0.791230, acc: 0.296875]\n",
      "37: [D loss: 0.697167, acc: 0.507812]  [A loss: 0.894030, acc: 0.125000]\n",
      "38: [D loss: 0.679613, acc: 0.568359]  [A loss: 0.759366, acc: 0.382812]\n",
      "39: [D loss: 0.688246, acc: 0.515625]  [A loss: 0.906426, acc: 0.128906]\n",
      "40: [D loss: 0.686175, acc: 0.550781]  [A loss: 0.808057, acc: 0.292969]\n",
      "41: [D loss: 0.682805, acc: 0.542969]  [A loss: 0.931280, acc: 0.156250]\n",
      "42: [D loss: 0.698624, acc: 0.527344]  [A loss: 0.760437, acc: 0.335938]\n",
      "43: [D loss: 0.689633, acc: 0.568359]  [A loss: 0.981026, acc: 0.082031]\n",
      "44: [D loss: 0.707926, acc: 0.517578]  [A loss: 0.677302, acc: 0.570312]\n",
      "45: [D loss: 0.693312, acc: 0.546875]  [A loss: 0.933673, acc: 0.128906]\n",
      "46: [D loss: 0.690692, acc: 0.550781]  [A loss: 0.748056, acc: 0.421875]\n",
      "47: [D loss: 0.695751, acc: 0.500000]  [A loss: 0.924761, acc: 0.117188]\n",
      "48: [D loss: 0.691191, acc: 0.537109]  [A loss: 0.758882, acc: 0.324219]\n",
      "49: [D loss: 0.703115, acc: 0.531250]  [A loss: 0.864455, acc: 0.191406]\n",
      "50: [D loss: 0.709740, acc: 0.509766]  [A loss: 0.856463, acc: 0.160156]\n",
      "51: [D loss: 0.696374, acc: 0.507812]  [A loss: 0.829491, acc: 0.226562]\n",
      "52: [D loss: 0.696262, acc: 0.550781]  [A loss: 0.797040, acc: 0.269531]\n",
      "53: [D loss: 0.697429, acc: 0.533203]  [A loss: 0.847092, acc: 0.195312]\n",
      "54: [D loss: 0.695405, acc: 0.529297]  [A loss: 0.830238, acc: 0.218750]\n",
      "55: [D loss: 0.695678, acc: 0.537109]  [A loss: 0.891245, acc: 0.160156]\n",
      "56: [D loss: 0.694898, acc: 0.535156]  [A loss: 0.800508, acc: 0.292969]\n",
      "57: [D loss: 0.693089, acc: 0.542969]  [A loss: 0.828971, acc: 0.238281]\n",
      "58: [D loss: 0.697568, acc: 0.541016]  [A loss: 0.863255, acc: 0.199219]\n",
      "59: [D loss: 0.699822, acc: 0.527344]  [A loss: 0.816073, acc: 0.277344]\n",
      "60: [D loss: 0.694355, acc: 0.535156]  [A loss: 0.900320, acc: 0.117188]\n",
      "61: [D loss: 0.682917, acc: 0.554688]  [A loss: 0.769286, acc: 0.351562]\n",
      "62: [D loss: 0.708374, acc: 0.511719]  [A loss: 0.977834, acc: 0.066406]\n",
      "63: [D loss: 0.691441, acc: 0.537109]  [A loss: 0.748671, acc: 0.378906]\n",
      "64: [D loss: 0.696123, acc: 0.535156]  [A loss: 0.887058, acc: 0.128906]\n",
      "65: [D loss: 0.682271, acc: 0.560547]  [A loss: 0.807275, acc: 0.238281]\n",
      "66: [D loss: 0.685674, acc: 0.556641]  [A loss: 0.950291, acc: 0.117188]\n",
      "67: [D loss: 0.697304, acc: 0.521484]  [A loss: 0.800074, acc: 0.292969]\n",
      "68: [D loss: 0.688439, acc: 0.542969]  [A loss: 0.876504, acc: 0.218750]\n",
      "69: [D loss: 0.715595, acc: 0.482422]  [A loss: 0.736467, acc: 0.406250]\n",
      "70: [D loss: 0.709023, acc: 0.525391]  [A loss: 0.926983, acc: 0.121094]\n",
      "71: [D loss: 0.676050, acc: 0.570312]  [A loss: 0.791814, acc: 0.296875]\n",
      "72: [D loss: 0.679186, acc: 0.570312]  [A loss: 0.875919, acc: 0.195312]\n",
      "73: [D loss: 0.696192, acc: 0.527344]  [A loss: 0.798286, acc: 0.308594]\n",
      "74: [D loss: 0.689778, acc: 0.544922]  [A loss: 0.853851, acc: 0.191406]\n",
      "75: [D loss: 0.675538, acc: 0.593750]  [A loss: 0.847822, acc: 0.238281]\n",
      "76: [D loss: 0.688396, acc: 0.546875]  [A loss: 0.906929, acc: 0.140625]\n",
      "77: [D loss: 0.680535, acc: 0.556641]  [A loss: 0.844582, acc: 0.222656]\n",
      "78: [D loss: 0.691778, acc: 0.521484]  [A loss: 0.892592, acc: 0.167969]\n",
      "79: [D loss: 0.676134, acc: 0.595703]  [A loss: 0.777848, acc: 0.359375]\n",
      "80: [D loss: 0.689094, acc: 0.542969]  [A loss: 0.962843, acc: 0.125000]\n",
      "81: [D loss: 0.674425, acc: 0.580078]  [A loss: 0.718771, acc: 0.468750]\n",
      "82: [D loss: 0.694902, acc: 0.560547]  [A loss: 0.990561, acc: 0.109375]\n",
      "83: [D loss: 0.688700, acc: 0.546875]  [A loss: 0.735058, acc: 0.437500]\n",
      "84: [D loss: 0.706525, acc: 0.554688]  [A loss: 0.958242, acc: 0.093750]\n",
      "85: [D loss: 0.686638, acc: 0.542969]  [A loss: 0.803861, acc: 0.273438]\n",
      "86: [D loss: 0.690908, acc: 0.533203]  [A loss: 0.923168, acc: 0.144531]\n",
      "87: [D loss: 0.681798, acc: 0.570312]  [A loss: 0.808793, acc: 0.316406]\n",
      "88: [D loss: 0.693400, acc: 0.554688]  [A loss: 0.891957, acc: 0.183594]\n",
      "89: [D loss: 0.703628, acc: 0.521484]  [A loss: 0.834891, acc: 0.238281]\n",
      "90: [D loss: 0.689132, acc: 0.531250]  [A loss: 0.872394, acc: 0.187500]\n",
      "91: [D loss: 0.698231, acc: 0.527344]  [A loss: 0.822721, acc: 0.250000]\n",
      "92: [D loss: 0.695224, acc: 0.523438]  [A loss: 0.877433, acc: 0.175781]\n",
      "93: [D loss: 0.679269, acc: 0.587891]  [A loss: 0.832857, acc: 0.250000]\n",
      "94: [D loss: 0.688060, acc: 0.537109]  [A loss: 0.894874, acc: 0.191406]\n",
      "95: [D loss: 0.693125, acc: 0.552734]  [A loss: 0.833135, acc: 0.214844]\n",
      "96: [D loss: 0.688691, acc: 0.556641]  [A loss: 0.868332, acc: 0.179688]\n",
      "97: [D loss: 0.686061, acc: 0.558594]  [A loss: 0.845516, acc: 0.214844]\n",
      "98: [D loss: 0.673694, acc: 0.574219]  [A loss: 0.859346, acc: 0.265625]\n",
      "99: [D loss: 0.705231, acc: 0.511719]  [A loss: 0.906586, acc: 0.160156]\n",
      "100: [D loss: 0.704379, acc: 0.537109]  [A loss: 0.836066, acc: 0.257812]\n",
      "101: [D loss: 0.702843, acc: 0.548828]  [A loss: 0.943396, acc: 0.117188]\n",
      "102: [D loss: 0.690948, acc: 0.542969]  [A loss: 0.780972, acc: 0.367188]\n",
      "103: [D loss: 0.700447, acc: 0.544922]  [A loss: 0.899172, acc: 0.175781]\n",
      "104: [D loss: 0.691662, acc: 0.548828]  [A loss: 0.826354, acc: 0.234375]\n",
      "105: [D loss: 0.681628, acc: 0.562500]  [A loss: 0.943060, acc: 0.167969]\n",
      "106: [D loss: 0.694371, acc: 0.542969]  [A loss: 0.792489, acc: 0.332031]\n",
      "107: [D loss: 0.706960, acc: 0.507812]  [A loss: 0.866052, acc: 0.199219]\n",
      "108: [D loss: 0.690278, acc: 0.515625]  [A loss: 0.843516, acc: 0.210938]\n",
      "109: [D loss: 0.685994, acc: 0.560547]  [A loss: 0.904977, acc: 0.140625]\n",
      "110: [D loss: 0.689752, acc: 0.552734]  [A loss: 0.806836, acc: 0.308594]\n",
      "111: [D loss: 0.698634, acc: 0.546875]  [A loss: 0.962331, acc: 0.105469]\n",
      "112: [D loss: 0.679217, acc: 0.566406]  [A loss: 0.785386, acc: 0.328125]\n",
      "113: [D loss: 0.695270, acc: 0.521484]  [A loss: 0.950152, acc: 0.117188]\n",
      "114: [D loss: 0.687851, acc: 0.542969]  [A loss: 0.760249, acc: 0.343750]\n",
      "115: [D loss: 0.702347, acc: 0.529297]  [A loss: 0.951628, acc: 0.144531]\n",
      "116: [D loss: 0.705158, acc: 0.533203]  [A loss: 0.765673, acc: 0.335938]\n",
      "117: [D loss: 0.697948, acc: 0.546875]  [A loss: 0.916755, acc: 0.128906]\n",
      "118: [D loss: 0.694075, acc: 0.523438]  [A loss: 0.822019, acc: 0.246094]\n",
      "119: [D loss: 0.692759, acc: 0.548828]  [A loss: 0.862834, acc: 0.195312]\n",
      "120: [D loss: 0.687466, acc: 0.564453]  [A loss: 0.773587, acc: 0.367188]\n",
      "121: [D loss: 0.683820, acc: 0.566406]  [A loss: 0.929161, acc: 0.109375]\n",
      "122: [D loss: 0.709941, acc: 0.496094]  [A loss: 0.829785, acc: 0.273438]\n",
      "123: [D loss: 0.696065, acc: 0.542969]  [A loss: 0.918796, acc: 0.093750]\n",
      "124: [D loss: 0.693346, acc: 0.525391]  [A loss: 0.820947, acc: 0.265625]\n",
      "125: [D loss: 0.694022, acc: 0.539062]  [A loss: 0.886413, acc: 0.148438]\n",
      "126: [D loss: 0.688753, acc: 0.548828]  [A loss: 0.782417, acc: 0.335938]\n",
      "127: [D loss: 0.700590, acc: 0.535156]  [A loss: 0.929905, acc: 0.132812]\n",
      "128: [D loss: 0.689620, acc: 0.542969]  [A loss: 0.744332, acc: 0.406250]\n",
      "129: [D loss: 0.698854, acc: 0.529297]  [A loss: 0.909549, acc: 0.144531]\n",
      "130: [D loss: 0.694094, acc: 0.576172]  [A loss: 0.801965, acc: 0.281250]\n",
      "131: [D loss: 0.698762, acc: 0.503906]  [A loss: 0.825789, acc: 0.265625]\n",
      "132: [D loss: 0.697152, acc: 0.523438]  [A loss: 0.834742, acc: 0.257812]\n",
      "133: [D loss: 0.691122, acc: 0.554688]  [A loss: 0.854836, acc: 0.179688]\n",
      "134: [D loss: 0.698022, acc: 0.529297]  [A loss: 0.838885, acc: 0.214844]\n",
      "135: [D loss: 0.697560, acc: 0.556641]  [A loss: 0.885291, acc: 0.175781]\n",
      "136: [D loss: 0.685701, acc: 0.546875]  [A loss: 0.830609, acc: 0.250000]\n",
      "137: [D loss: 0.688707, acc: 0.583984]  [A loss: 0.862299, acc: 0.230469]\n",
      "138: [D loss: 0.700240, acc: 0.529297]  [A loss: 0.870288, acc: 0.183594]\n",
      "139: [D loss: 0.697234, acc: 0.548828]  [A loss: 0.931088, acc: 0.125000]\n",
      "140: [D loss: 0.683660, acc: 0.564453]  [A loss: 0.765991, acc: 0.367188]\n",
      "141: [D loss: 0.704422, acc: 0.541016]  [A loss: 1.009350, acc: 0.101562]\n",
      "142: [D loss: 0.674270, acc: 0.566406]  [A loss: 0.732453, acc: 0.449219]\n",
      "143: [D loss: 0.718490, acc: 0.519531]  [A loss: 0.930900, acc: 0.156250]\n",
      "144: [D loss: 0.691122, acc: 0.568359]  [A loss: 0.796488, acc: 0.332031]\n",
      "145: [D loss: 0.701633, acc: 0.531250]  [A loss: 0.951290, acc: 0.105469]\n",
      "146: [D loss: 0.681641, acc: 0.572266]  [A loss: 0.775467, acc: 0.328125]\n",
      "147: [D loss: 0.697918, acc: 0.511719]  [A loss: 0.904680, acc: 0.167969]\n",
      "148: [D loss: 0.706619, acc: 0.521484]  [A loss: 0.846739, acc: 0.250000]\n",
      "149: [D loss: 0.701816, acc: 0.533203]  [A loss: 0.829302, acc: 0.269531]\n",
      "150: [D loss: 0.703752, acc: 0.541016]  [A loss: 0.869762, acc: 0.167969]\n",
      "151: [D loss: 0.693672, acc: 0.517578]  [A loss: 0.751291, acc: 0.375000]\n",
      "152: [D loss: 0.697478, acc: 0.490234]  [A loss: 0.863195, acc: 0.160156]\n",
      "153: [D loss: 0.690151, acc: 0.562500]  [A loss: 0.832519, acc: 0.222656]\n",
      "154: [D loss: 0.693144, acc: 0.523438]  [A loss: 0.860490, acc: 0.203125]\n",
      "155: [D loss: 0.708167, acc: 0.503906]  [A loss: 0.832275, acc: 0.250000]\n",
      "156: [D loss: 0.696109, acc: 0.539062]  [A loss: 0.874230, acc: 0.191406]\n",
      "157: [D loss: 0.684461, acc: 0.578125]  [A loss: 0.814274, acc: 0.296875]\n",
      "158: [D loss: 0.705665, acc: 0.531250]  [A loss: 0.847394, acc: 0.171875]\n",
      "159: [D loss: 0.720079, acc: 0.486328]  [A loss: 0.883316, acc: 0.183594]\n",
      "160: [D loss: 0.682036, acc: 0.585938]  [A loss: 0.772966, acc: 0.320312]\n",
      "161: [D loss: 0.682652, acc: 0.576172]  [A loss: 0.949326, acc: 0.148438]\n",
      "162: [D loss: 0.704173, acc: 0.523438]  [A loss: 0.767431, acc: 0.324219]\n",
      "163: [D loss: 0.694674, acc: 0.546875]  [A loss: 0.905839, acc: 0.136719]\n",
      "164: [D loss: 0.685964, acc: 0.568359]  [A loss: 0.814061, acc: 0.289062]\n",
      "165: [D loss: 0.711891, acc: 0.517578]  [A loss: 0.852487, acc: 0.230469]\n",
      "166: [D loss: 0.703237, acc: 0.521484]  [A loss: 0.732081, acc: 0.421875]\n",
      "167: [D loss: 0.703358, acc: 0.517578]  [A loss: 0.887718, acc: 0.156250]\n",
      "168: [D loss: 0.711678, acc: 0.488281]  [A loss: 0.846611, acc: 0.207031]\n",
      "169: [D loss: 0.686431, acc: 0.560547]  [A loss: 0.852499, acc: 0.242188]\n",
      "170: [D loss: 0.697941, acc: 0.519531]  [A loss: 0.878991, acc: 0.187500]\n",
      "171: [D loss: 0.678993, acc: 0.583984]  [A loss: 0.837837, acc: 0.238281]\n",
      "172: [D loss: 0.683194, acc: 0.556641]  [A loss: 0.826816, acc: 0.257812]\n",
      "173: [D loss: 0.677123, acc: 0.607422]  [A loss: 0.839059, acc: 0.214844]\n",
      "174: [D loss: 0.697412, acc: 0.525391]  [A loss: 0.872001, acc: 0.218750]\n",
      "175: [D loss: 0.700304, acc: 0.527344]  [A loss: 0.886452, acc: 0.195312]\n",
      "176: [D loss: 0.692678, acc: 0.527344]  [A loss: 0.832584, acc: 0.261719]\n",
      "177: [D loss: 0.681659, acc: 0.580078]  [A loss: 0.845692, acc: 0.269531]\n",
      "178: [D loss: 0.703702, acc: 0.519531]  [A loss: 0.929046, acc: 0.113281]\n",
      "179: [D loss: 0.689094, acc: 0.580078]  [A loss: 0.743425, acc: 0.402344]\n",
      "180: [D loss: 0.702884, acc: 0.533203]  [A loss: 0.965193, acc: 0.136719]\n",
      "181: [D loss: 0.689979, acc: 0.521484]  [A loss: 0.707631, acc: 0.476562]\n",
      "182: [D loss: 0.724441, acc: 0.517578]  [A loss: 1.035252, acc: 0.070312]\n",
      "183: [D loss: 0.700389, acc: 0.544922]  [A loss: 0.689966, acc: 0.496094]\n",
      "184: [D loss: 0.710127, acc: 0.515625]  [A loss: 0.940424, acc: 0.125000]\n",
      "185: [D loss: 0.684532, acc: 0.544922]  [A loss: 0.744315, acc: 0.371094]\n",
      "186: [D loss: 0.704587, acc: 0.533203]  [A loss: 0.854459, acc: 0.187500]\n",
      "187: [D loss: 0.696302, acc: 0.552734]  [A loss: 0.827654, acc: 0.218750]\n",
      "188: [D loss: 0.703001, acc: 0.515625]  [A loss: 0.844881, acc: 0.214844]\n",
      "189: [D loss: 0.693476, acc: 0.539062]  [A loss: 0.873928, acc: 0.179688]\n",
      "190: [D loss: 0.686076, acc: 0.552734]  [A loss: 0.790410, acc: 0.324219]\n",
      "191: [D loss: 0.707916, acc: 0.511719]  [A loss: 0.825853, acc: 0.238281]\n",
      "192: [D loss: 0.694043, acc: 0.509766]  [A loss: 0.857804, acc: 0.167969]\n",
      "193: [D loss: 0.688355, acc: 0.550781]  [A loss: 0.804018, acc: 0.277344]\n",
      "194: [D loss: 0.704383, acc: 0.507812]  [A loss: 0.907210, acc: 0.160156]\n",
      "195: [D loss: 0.683098, acc: 0.552734]  [A loss: 0.783024, acc: 0.320312]\n",
      "196: [D loss: 0.679845, acc: 0.572266]  [A loss: 0.910904, acc: 0.140625]\n",
      "197: [D loss: 0.695825, acc: 0.542969]  [A loss: 0.769396, acc: 0.335938]\n",
      "198: [D loss: 0.697397, acc: 0.529297]  [A loss: 0.884301, acc: 0.175781]\n",
      "199: [D loss: 0.696441, acc: 0.531250]  [A loss: 0.794112, acc: 0.300781]\n",
      "200: [D loss: 0.700976, acc: 0.519531]  [A loss: 0.918147, acc: 0.128906]\n",
      "201: [D loss: 0.674760, acc: 0.566406]  [A loss: 0.779050, acc: 0.328125]\n",
      "202: [D loss: 0.701174, acc: 0.521484]  [A loss: 0.876762, acc: 0.199219]\n",
      "203: [D loss: 0.680024, acc: 0.578125]  [A loss: 0.861570, acc: 0.238281]\n",
      "204: [D loss: 0.707430, acc: 0.523438]  [A loss: 0.829028, acc: 0.269531]\n",
      "205: [D loss: 0.699429, acc: 0.529297]  [A loss: 0.810948, acc: 0.257812]\n",
      "206: [D loss: 0.699475, acc: 0.531250]  [A loss: 0.851830, acc: 0.195312]\n",
      "207: [D loss: 0.694321, acc: 0.542969]  [A loss: 0.853204, acc: 0.199219]\n",
      "208: [D loss: 0.685725, acc: 0.550781]  [A loss: 0.817380, acc: 0.304688]\n",
      "209: [D loss: 0.690724, acc: 0.552734]  [A loss: 0.917378, acc: 0.132812]\n",
      "210: [D loss: 0.673840, acc: 0.576172]  [A loss: 0.777191, acc: 0.371094]\n",
      "211: [D loss: 0.690044, acc: 0.578125]  [A loss: 0.979605, acc: 0.093750]\n",
      "212: [D loss: 0.689234, acc: 0.550781]  [A loss: 0.737272, acc: 0.394531]\n",
      "213: [D loss: 0.725659, acc: 0.519531]  [A loss: 0.886048, acc: 0.152344]\n",
      "214: [D loss: 0.692732, acc: 0.554688]  [A loss: 0.805298, acc: 0.257812]\n",
      "215: [D loss: 0.690770, acc: 0.560547]  [A loss: 0.890008, acc: 0.183594]\n",
      "216: [D loss: 0.679851, acc: 0.558594]  [A loss: 0.801156, acc: 0.324219]\n",
      "217: [D loss: 0.707928, acc: 0.523438]  [A loss: 0.884258, acc: 0.187500]\n",
      "218: [D loss: 0.676207, acc: 0.589844]  [A loss: 0.888753, acc: 0.164062]\n",
      "219: [D loss: 0.691636, acc: 0.539062]  [A loss: 0.833249, acc: 0.257812]\n",
      "220: [D loss: 0.685241, acc: 0.570312]  [A loss: 0.890013, acc: 0.191406]\n",
      "221: [D loss: 0.712035, acc: 0.525391]  [A loss: 0.872084, acc: 0.183594]\n",
      "222: [D loss: 0.692022, acc: 0.531250]  [A loss: 0.905960, acc: 0.144531]\n",
      "223: [D loss: 0.664932, acc: 0.601562]  [A loss: 0.818718, acc: 0.273438]\n",
      "224: [D loss: 0.701576, acc: 0.527344]  [A loss: 0.982656, acc: 0.125000]\n",
      "225: [D loss: 0.706251, acc: 0.535156]  [A loss: 0.710487, acc: 0.507812]\n",
      "226: [D loss: 0.702941, acc: 0.560547]  [A loss: 0.907665, acc: 0.160156]\n",
      "227: [D loss: 0.692058, acc: 0.550781]  [A loss: 0.820849, acc: 0.300781]\n",
      "228: [D loss: 0.681475, acc: 0.550781]  [A loss: 0.999529, acc: 0.074219]\n",
      "229: [D loss: 0.691245, acc: 0.572266]  [A loss: 0.723583, acc: 0.476562]\n",
      "230: [D loss: 0.688721, acc: 0.554688]  [A loss: 0.932226, acc: 0.148438]\n",
      "231: [D loss: 0.690182, acc: 0.529297]  [A loss: 0.784546, acc: 0.371094]\n",
      "232: [D loss: 0.713204, acc: 0.521484]  [A loss: 0.923447, acc: 0.156250]\n",
      "233: [D loss: 0.703865, acc: 0.533203]  [A loss: 0.797378, acc: 0.316406]\n",
      "234: [D loss: 0.714368, acc: 0.496094]  [A loss: 0.883909, acc: 0.156250]\n",
      "235: [D loss: 0.691610, acc: 0.527344]  [A loss: 0.786378, acc: 0.335938]\n",
      "236: [D loss: 0.692384, acc: 0.527344]  [A loss: 0.860624, acc: 0.207031]\n",
      "237: [D loss: 0.701966, acc: 0.523438]  [A loss: 0.842447, acc: 0.214844]\n",
      "238: [D loss: 0.693281, acc: 0.531250]  [A loss: 0.781730, acc: 0.367188]\n",
      "239: [D loss: 0.695512, acc: 0.548828]  [A loss: 0.851326, acc: 0.203125]\n",
      "240: [D loss: 0.700523, acc: 0.531250]  [A loss: 0.881979, acc: 0.183594]\n",
      "241: [D loss: 0.683043, acc: 0.587891]  [A loss: 0.806140, acc: 0.308594]\n",
      "242: [D loss: 0.693695, acc: 0.529297]  [A loss: 0.853786, acc: 0.218750]\n",
      "243: [D loss: 0.702156, acc: 0.521484]  [A loss: 0.855149, acc: 0.257812]\n",
      "244: [D loss: 0.703772, acc: 0.521484]  [A loss: 0.904731, acc: 0.179688]\n",
      "245: [D loss: 0.689592, acc: 0.546875]  [A loss: 0.878436, acc: 0.195312]\n",
      "246: [D loss: 0.685086, acc: 0.572266]  [A loss: 0.848576, acc: 0.242188]\n",
      "247: [D loss: 0.688750, acc: 0.564453]  [A loss: 0.920936, acc: 0.199219]\n",
      "248: [D loss: 0.702008, acc: 0.531250]  [A loss: 0.795658, acc: 0.316406]\n",
      "249: [D loss: 0.688727, acc: 0.546875]  [A loss: 0.918415, acc: 0.187500]\n",
      "250: [D loss: 0.695614, acc: 0.544922]  [A loss: 0.746323, acc: 0.425781]\n",
      "251: [D loss: 0.696738, acc: 0.542969]  [A loss: 0.925556, acc: 0.109375]\n",
      "252: [D loss: 0.679827, acc: 0.568359]  [A loss: 0.738720, acc: 0.414062]\n",
      "253: [D loss: 0.699688, acc: 0.531250]  [A loss: 0.953886, acc: 0.125000]\n",
      "254: [D loss: 0.683188, acc: 0.562500]  [A loss: 0.749024, acc: 0.398438]\n",
      "255: [D loss: 0.690642, acc: 0.566406]  [A loss: 0.928642, acc: 0.179688]\n",
      "256: [D loss: 0.697080, acc: 0.519531]  [A loss: 0.752291, acc: 0.417969]\n",
      "257: [D loss: 0.699023, acc: 0.531250]  [A loss: 0.931107, acc: 0.152344]\n",
      "258: [D loss: 0.695038, acc: 0.531250]  [A loss: 0.768887, acc: 0.390625]\n",
      "259: [D loss: 0.710685, acc: 0.521484]  [A loss: 0.946475, acc: 0.117188]\n",
      "260: [D loss: 0.704863, acc: 0.494141]  [A loss: 0.814492, acc: 0.316406]\n",
      "261: [D loss: 0.684252, acc: 0.578125]  [A loss: 0.856339, acc: 0.226562]\n",
      "262: [D loss: 0.692083, acc: 0.548828]  [A loss: 0.851509, acc: 0.226562]\n",
      "263: [D loss: 0.686015, acc: 0.556641]  [A loss: 0.847647, acc: 0.253906]\n",
      "264: [D loss: 0.676906, acc: 0.570312]  [A loss: 0.818697, acc: 0.300781]\n",
      "265: [D loss: 0.690769, acc: 0.527344]  [A loss: 0.854028, acc: 0.187500]\n",
      "266: [D loss: 0.678337, acc: 0.564453]  [A loss: 0.869718, acc: 0.265625]\n",
      "267: [D loss: 0.683610, acc: 0.570312]  [A loss: 0.814557, acc: 0.308594]\n",
      "268: [D loss: 0.686984, acc: 0.562500]  [A loss: 0.946955, acc: 0.167969]\n",
      "269: [D loss: 0.691973, acc: 0.541016]  [A loss: 0.765508, acc: 0.343750]\n",
      "270: [D loss: 0.714832, acc: 0.535156]  [A loss: 0.888648, acc: 0.156250]\n",
      "271: [D loss: 0.686651, acc: 0.544922]  [A loss: 0.799839, acc: 0.332031]\n",
      "272: [D loss: 0.689773, acc: 0.550781]  [A loss: 0.887182, acc: 0.179688]\n",
      "273: [D loss: 0.701331, acc: 0.533203]  [A loss: 0.761853, acc: 0.390625]\n",
      "274: [D loss: 0.697165, acc: 0.544922]  [A loss: 0.919187, acc: 0.195312]\n",
      "275: [D loss: 0.696959, acc: 0.542969]  [A loss: 0.759182, acc: 0.351562]\n",
      "276: [D loss: 0.707065, acc: 0.529297]  [A loss: 0.900042, acc: 0.148438]\n",
      "277: [D loss: 0.679774, acc: 0.542969]  [A loss: 0.751576, acc: 0.417969]\n",
      "278: [D loss: 0.700674, acc: 0.529297]  [A loss: 0.892913, acc: 0.210938]\n",
      "279: [D loss: 0.718614, acc: 0.498047]  [A loss: 0.850451, acc: 0.195312]\n",
      "280: [D loss: 0.694779, acc: 0.535156]  [A loss: 0.888952, acc: 0.210938]\n",
      "281: [D loss: 0.691414, acc: 0.552734]  [A loss: 0.849457, acc: 0.238281]\n",
      "282: [D loss: 0.694182, acc: 0.560547]  [A loss: 0.827873, acc: 0.269531]\n",
      "283: [D loss: 0.695965, acc: 0.541016]  [A loss: 0.905445, acc: 0.152344]\n",
      "284: [D loss: 0.670241, acc: 0.558594]  [A loss: 0.819459, acc: 0.316406]\n",
      "285: [D loss: 0.707040, acc: 0.519531]  [A loss: 0.958601, acc: 0.167969]\n",
      "286: [D loss: 0.683014, acc: 0.546875]  [A loss: 0.759254, acc: 0.406250]\n",
      "287: [D loss: 0.691949, acc: 0.539062]  [A loss: 0.930251, acc: 0.125000]\n",
      "288: [D loss: 0.685292, acc: 0.566406]  [A loss: 0.766548, acc: 0.371094]\n",
      "289: [D loss: 0.700417, acc: 0.539062]  [A loss: 0.916715, acc: 0.144531]\n",
      "290: [D loss: 0.690312, acc: 0.552734]  [A loss: 0.796931, acc: 0.339844]\n",
      "291: [D loss: 0.691761, acc: 0.535156]  [A loss: 0.912207, acc: 0.187500]\n",
      "292: [D loss: 0.706428, acc: 0.484375]  [A loss: 0.763534, acc: 0.347656]\n",
      "293: [D loss: 0.692397, acc: 0.560547]  [A loss: 0.911926, acc: 0.160156]\n",
      "294: [D loss: 0.688745, acc: 0.570312]  [A loss: 0.770702, acc: 0.351562]\n",
      "295: [D loss: 0.696399, acc: 0.529297]  [A loss: 0.946193, acc: 0.167969]\n",
      "296: [D loss: 0.689187, acc: 0.562500]  [A loss: 0.794310, acc: 0.289062]\n",
      "297: [D loss: 0.689932, acc: 0.564453]  [A loss: 0.898393, acc: 0.140625]\n",
      "298: [D loss: 0.697282, acc: 0.523438]  [A loss: 0.822653, acc: 0.281250]\n",
      "299: [D loss: 0.681903, acc: 0.585938]  [A loss: 0.852282, acc: 0.234375]\n",
      "300: [D loss: 0.692172, acc: 0.539062]  [A loss: 0.914665, acc: 0.160156]\n",
      "301: [D loss: 0.680970, acc: 0.556641]  [A loss: 0.828113, acc: 0.218750]\n",
      "302: [D loss: 0.692128, acc: 0.546875]  [A loss: 0.886802, acc: 0.175781]\n",
      "303: [D loss: 0.689988, acc: 0.531250]  [A loss: 0.816159, acc: 0.273438]\n",
      "304: [D loss: 0.714066, acc: 0.507812]  [A loss: 0.899872, acc: 0.144531]\n",
      "305: [D loss: 0.707150, acc: 0.505859]  [A loss: 0.877802, acc: 0.183594]\n",
      "306: [D loss: 0.686120, acc: 0.542969]  [A loss: 0.892399, acc: 0.148438]\n",
      "307: [D loss: 0.709717, acc: 0.501953]  [A loss: 0.805831, acc: 0.253906]\n",
      "308: [D loss: 0.695597, acc: 0.556641]  [A loss: 0.947793, acc: 0.160156]\n",
      "309: [D loss: 0.695337, acc: 0.521484]  [A loss: 0.758916, acc: 0.375000]\n",
      "310: [D loss: 0.712099, acc: 0.478516]  [A loss: 0.867860, acc: 0.207031]\n",
      "311: [D loss: 0.690428, acc: 0.537109]  [A loss: 0.798698, acc: 0.300781]\n",
      "312: [D loss: 0.696996, acc: 0.519531]  [A loss: 0.929106, acc: 0.128906]\n",
      "313: [D loss: 0.672789, acc: 0.597656]  [A loss: 0.746882, acc: 0.355469]\n",
      "314: [D loss: 0.712898, acc: 0.533203]  [A loss: 0.959948, acc: 0.125000]\n",
      "315: [D loss: 0.685516, acc: 0.560547]  [A loss: 0.686489, acc: 0.507812]\n",
      "316: [D loss: 0.714001, acc: 0.519531]  [A loss: 0.984306, acc: 0.074219]\n",
      "317: [D loss: 0.705028, acc: 0.505859]  [A loss: 0.726770, acc: 0.433594]\n",
      "318: [D loss: 0.687912, acc: 0.531250]  [A loss: 0.897580, acc: 0.175781]\n",
      "319: [D loss: 0.694561, acc: 0.544922]  [A loss: 0.779514, acc: 0.335938]\n",
      "320: [D loss: 0.711374, acc: 0.515625]  [A loss: 0.950420, acc: 0.136719]\n",
      "321: [D loss: 0.695637, acc: 0.531250]  [A loss: 0.682400, acc: 0.535156]\n",
      "322: [D loss: 0.697940, acc: 0.535156]  [A loss: 0.902879, acc: 0.109375]\n",
      "323: [D loss: 0.692456, acc: 0.531250]  [A loss: 0.738912, acc: 0.425781]\n",
      "324: [D loss: 0.675934, acc: 0.572266]  [A loss: 0.865107, acc: 0.214844]\n",
      "325: [D loss: 0.668392, acc: 0.591797]  [A loss: 0.809719, acc: 0.281250]\n",
      "326: [D loss: 0.703513, acc: 0.535156]  [A loss: 0.894542, acc: 0.156250]\n",
      "327: [D loss: 0.684952, acc: 0.556641]  [A loss: 0.811945, acc: 0.304688]\n",
      "328: [D loss: 0.692364, acc: 0.537109]  [A loss: 0.915142, acc: 0.164062]\n",
      "329: [D loss: 0.690654, acc: 0.525391]  [A loss: 0.805755, acc: 0.320312]\n",
      "330: [D loss: 0.697284, acc: 0.554688]  [A loss: 0.920168, acc: 0.144531]\n",
      "331: [D loss: 0.690735, acc: 0.546875]  [A loss: 0.748123, acc: 0.371094]\n",
      "332: [D loss: 0.720027, acc: 0.496094]  [A loss: 0.988611, acc: 0.089844]\n",
      "333: [D loss: 0.697416, acc: 0.517578]  [A loss: 0.731182, acc: 0.410156]\n",
      "334: [D loss: 0.698730, acc: 0.515625]  [A loss: 0.896677, acc: 0.171875]\n",
      "335: [D loss: 0.696633, acc: 0.544922]  [A loss: 0.806088, acc: 0.281250]\n",
      "336: [D loss: 0.690415, acc: 0.535156]  [A loss: 0.827877, acc: 0.261719]\n",
      "337: [D loss: 0.693340, acc: 0.527344]  [A loss: 0.817518, acc: 0.273438]\n",
      "338: [D loss: 0.692144, acc: 0.544922]  [A loss: 0.883234, acc: 0.187500]\n",
      "339: [D loss: 0.688545, acc: 0.523438]  [A loss: 0.811905, acc: 0.257812]\n",
      "340: [D loss: 0.699029, acc: 0.507812]  [A loss: 0.860416, acc: 0.207031]\n",
      "341: [D loss: 0.686595, acc: 0.535156]  [A loss: 0.801380, acc: 0.289062]\n",
      "342: [D loss: 0.699262, acc: 0.525391]  [A loss: 0.848006, acc: 0.218750]\n",
      "343: [D loss: 0.707817, acc: 0.517578]  [A loss: 0.868987, acc: 0.175781]\n",
      "344: [D loss: 0.686533, acc: 0.550781]  [A loss: 0.893618, acc: 0.148438]\n",
      "345: [D loss: 0.699267, acc: 0.523438]  [A loss: 0.844215, acc: 0.242188]\n",
      "346: [D loss: 0.702992, acc: 0.542969]  [A loss: 0.828599, acc: 0.246094]\n",
      "347: [D loss: 0.700555, acc: 0.509766]  [A loss: 0.960205, acc: 0.101562]\n",
      "348: [D loss: 0.681570, acc: 0.566406]  [A loss: 0.750453, acc: 0.402344]\n",
      "349: [D loss: 0.709515, acc: 0.509766]  [A loss: 0.984669, acc: 0.105469]\n",
      "350: [D loss: 0.682388, acc: 0.564453]  [A loss: 0.700783, acc: 0.484375]\n",
      "351: [D loss: 0.695446, acc: 0.558594]  [A loss: 0.936043, acc: 0.132812]\n",
      "352: [D loss: 0.696777, acc: 0.513672]  [A loss: 0.711075, acc: 0.484375]\n",
      "353: [D loss: 0.708286, acc: 0.546875]  [A loss: 0.877122, acc: 0.187500]\n",
      "354: [D loss: 0.695790, acc: 0.515625]  [A loss: 0.777408, acc: 0.347656]\n",
      "355: [D loss: 0.705926, acc: 0.546875]  [A loss: 0.867531, acc: 0.183594]\n",
      "356: [D loss: 0.704558, acc: 0.546875]  [A loss: 0.811403, acc: 0.289062]\n",
      "357: [D loss: 0.696810, acc: 0.511719]  [A loss: 0.855749, acc: 0.226562]\n",
      "358: [D loss: 0.713984, acc: 0.476562]  [A loss: 0.856588, acc: 0.191406]\n",
      "359: [D loss: 0.700442, acc: 0.521484]  [A loss: 0.818652, acc: 0.269531]\n",
      "360: [D loss: 0.696492, acc: 0.548828]  [A loss: 0.816215, acc: 0.324219]\n",
      "361: [D loss: 0.686813, acc: 0.554688]  [A loss: 0.821826, acc: 0.289062]\n",
      "362: [D loss: 0.695106, acc: 0.546875]  [A loss: 0.840353, acc: 0.222656]\n",
      "363: [D loss: 0.690725, acc: 0.541016]  [A loss: 0.806275, acc: 0.269531]\n",
      "364: [D loss: 0.700861, acc: 0.531250]  [A loss: 0.861058, acc: 0.210938]\n",
      "365: [D loss: 0.688145, acc: 0.554688]  [A loss: 0.779817, acc: 0.335938]\n",
      "366: [D loss: 0.702087, acc: 0.529297]  [A loss: 0.891192, acc: 0.167969]\n",
      "367: [D loss: 0.701184, acc: 0.511719]  [A loss: 0.740099, acc: 0.386719]\n",
      "368: [D loss: 0.685398, acc: 0.572266]  [A loss: 0.895644, acc: 0.148438]\n",
      "369: [D loss: 0.684212, acc: 0.546875]  [A loss: 0.768129, acc: 0.343750]\n",
      "370: [D loss: 0.701633, acc: 0.523438]  [A loss: 0.929441, acc: 0.132812]\n",
      "371: [D loss: 0.695964, acc: 0.539062]  [A loss: 0.868236, acc: 0.222656]\n",
      "372: [D loss: 0.684583, acc: 0.568359]  [A loss: 0.929598, acc: 0.164062]\n",
      "373: [D loss: 0.704803, acc: 0.500000]  [A loss: 0.763008, acc: 0.351562]\n",
      "374: [D loss: 0.691606, acc: 0.521484]  [A loss: 0.908346, acc: 0.140625]\n",
      "375: [D loss: 0.695625, acc: 0.548828]  [A loss: 0.694942, acc: 0.527344]\n",
      "376: [D loss: 0.717602, acc: 0.535156]  [A loss: 1.049725, acc: 0.066406]\n",
      "377: [D loss: 0.697945, acc: 0.525391]  [A loss: 0.644553, acc: 0.644531]\n",
      "378: [D loss: 0.699981, acc: 0.539062]  [A loss: 0.918614, acc: 0.144531]\n",
      "379: [D loss: 0.679168, acc: 0.566406]  [A loss: 0.763194, acc: 0.347656]\n",
      "380: [D loss: 0.710880, acc: 0.509766]  [A loss: 0.863211, acc: 0.203125]\n",
      "381: [D loss: 0.705914, acc: 0.492188]  [A loss: 0.832258, acc: 0.269531]\n",
      "382: [D loss: 0.700268, acc: 0.521484]  [A loss: 0.804989, acc: 0.316406]\n",
      "383: [D loss: 0.696913, acc: 0.525391]  [A loss: 0.834416, acc: 0.250000]\n",
      "384: [D loss: 0.684274, acc: 0.550781]  [A loss: 0.880626, acc: 0.187500]\n",
      "385: [D loss: 0.689437, acc: 0.558594]  [A loss: 0.843599, acc: 0.214844]\n",
      "386: [D loss: 0.705033, acc: 0.523438]  [A loss: 0.858494, acc: 0.187500]\n",
      "387: [D loss: 0.695563, acc: 0.544922]  [A loss: 0.873605, acc: 0.218750]\n",
      "388: [D loss: 0.692719, acc: 0.544922]  [A loss: 0.789462, acc: 0.300781]\n",
      "389: [D loss: 0.705604, acc: 0.498047]  [A loss: 0.866563, acc: 0.203125]\n",
      "390: [D loss: 0.692114, acc: 0.546875]  [A loss: 0.755614, acc: 0.371094]\n",
      "391: [D loss: 0.692322, acc: 0.542969]  [A loss: 0.885722, acc: 0.187500]\n",
      "392: [D loss: 0.682013, acc: 0.542969]  [A loss: 0.780582, acc: 0.308594]\n",
      "393: [D loss: 0.702446, acc: 0.511719]  [A loss: 0.938902, acc: 0.144531]\n",
      "394: [D loss: 0.692604, acc: 0.515625]  [A loss: 0.782809, acc: 0.343750]\n",
      "395: [D loss: 0.683839, acc: 0.546875]  [A loss: 0.933051, acc: 0.125000]\n",
      "396: [D loss: 0.688953, acc: 0.552734]  [A loss: 0.719790, acc: 0.476562]\n",
      "397: [D loss: 0.707436, acc: 0.517578]  [A loss: 0.910975, acc: 0.148438]\n",
      "398: [D loss: 0.690372, acc: 0.521484]  [A loss: 0.817323, acc: 0.289062]\n",
      "399: [D loss: 0.705623, acc: 0.527344]  [A loss: 0.900887, acc: 0.156250]\n",
      "400: [D loss: 0.683012, acc: 0.550781]  [A loss: 0.815726, acc: 0.253906]\n",
      "401: [D loss: 0.694892, acc: 0.556641]  [A loss: 0.909452, acc: 0.175781]\n",
      "402: [D loss: 0.690362, acc: 0.562500]  [A loss: 0.797210, acc: 0.312500]\n",
      "403: [D loss: 0.709469, acc: 0.513672]  [A loss: 0.906530, acc: 0.167969]\n",
      "404: [D loss: 0.694792, acc: 0.554688]  [A loss: 0.817528, acc: 0.273438]\n",
      "405: [D loss: 0.684077, acc: 0.585938]  [A loss: 0.939439, acc: 0.140625]\n",
      "406: [D loss: 0.679857, acc: 0.544922]  [A loss: 0.761907, acc: 0.398438]\n",
      "407: [D loss: 0.709830, acc: 0.500000]  [A loss: 0.950076, acc: 0.109375]\n",
      "408: [D loss: 0.695484, acc: 0.546875]  [A loss: 0.764365, acc: 0.406250]\n",
      "409: [D loss: 0.708908, acc: 0.523438]  [A loss: 0.920481, acc: 0.140625]\n",
      "410: [D loss: 0.692637, acc: 0.550781]  [A loss: 0.744720, acc: 0.406250]\n",
      "411: [D loss: 0.716519, acc: 0.500000]  [A loss: 0.917728, acc: 0.128906]\n",
      "412: [D loss: 0.684690, acc: 0.548828]  [A loss: 0.755541, acc: 0.363281]\n",
      "413: [D loss: 0.704105, acc: 0.548828]  [A loss: 0.925661, acc: 0.152344]\n",
      "414: [D loss: 0.683301, acc: 0.544922]  [A loss: 0.731646, acc: 0.425781]\n",
      "415: [D loss: 0.702236, acc: 0.515625]  [A loss: 0.945660, acc: 0.125000]\n",
      "416: [D loss: 0.701387, acc: 0.515625]  [A loss: 0.746009, acc: 0.398438]\n",
      "417: [D loss: 0.706947, acc: 0.535156]  [A loss: 0.971920, acc: 0.128906]\n",
      "418: [D loss: 0.696554, acc: 0.533203]  [A loss: 0.706893, acc: 0.511719]\n",
      "419: [D loss: 0.701701, acc: 0.511719]  [A loss: 0.924565, acc: 0.128906]\n",
      "420: [D loss: 0.685380, acc: 0.544922]  [A loss: 0.757337, acc: 0.378906]\n",
      "421: [D loss: 0.716620, acc: 0.513672]  [A loss: 0.928176, acc: 0.156250]\n",
      "422: [D loss: 0.689255, acc: 0.548828]  [A loss: 0.708915, acc: 0.503906]\n",
      "423: [D loss: 0.713555, acc: 0.490234]  [A loss: 0.917053, acc: 0.148438]\n",
      "424: [D loss: 0.696279, acc: 0.529297]  [A loss: 0.801738, acc: 0.273438]\n",
      "425: [D loss: 0.701178, acc: 0.541016]  [A loss: 0.853568, acc: 0.218750]\n",
      "426: [D loss: 0.704602, acc: 0.515625]  [A loss: 0.800223, acc: 0.304688]\n",
      "427: [D loss: 0.688669, acc: 0.542969]  [A loss: 0.771071, acc: 0.363281]\n",
      "428: [D loss: 0.691003, acc: 0.548828]  [A loss: 0.824126, acc: 0.253906]\n",
      "429: [D loss: 0.692769, acc: 0.568359]  [A loss: 0.849689, acc: 0.226562]\n",
      "430: [D loss: 0.702970, acc: 0.525391]  [A loss: 0.837394, acc: 0.203125]\n",
      "431: [D loss: 0.686090, acc: 0.552734]  [A loss: 0.794491, acc: 0.347656]\n",
      "432: [D loss: 0.701469, acc: 0.494141]  [A loss: 0.861286, acc: 0.210938]\n",
      "433: [D loss: 0.687463, acc: 0.541016]  [A loss: 0.824962, acc: 0.269531]\n",
      "434: [D loss: 0.693077, acc: 0.556641]  [A loss: 0.848461, acc: 0.191406]\n",
      "435: [D loss: 0.700868, acc: 0.503906]  [A loss: 0.808930, acc: 0.285156]\n",
      "436: [D loss: 0.686026, acc: 0.541016]  [A loss: 0.860237, acc: 0.246094]\n",
      "437: [D loss: 0.698037, acc: 0.521484]  [A loss: 0.780632, acc: 0.351562]\n",
      "438: [D loss: 0.705212, acc: 0.535156]  [A loss: 0.897673, acc: 0.175781]\n",
      "439: [D loss: 0.676023, acc: 0.574219]  [A loss: 0.774720, acc: 0.367188]\n",
      "440: [D loss: 0.711706, acc: 0.525391]  [A loss: 0.943825, acc: 0.132812]\n",
      "441: [D loss: 0.688212, acc: 0.542969]  [A loss: 0.708140, acc: 0.453125]\n",
      "442: [D loss: 0.705645, acc: 0.519531]  [A loss: 0.985526, acc: 0.109375]\n",
      "443: [D loss: 0.696268, acc: 0.546875]  [A loss: 0.727575, acc: 0.402344]\n",
      "444: [D loss: 0.707702, acc: 0.554688]  [A loss: 0.866120, acc: 0.152344]\n",
      "445: [D loss: 0.703574, acc: 0.511719]  [A loss: 0.829903, acc: 0.250000]\n",
      "446: [D loss: 0.707048, acc: 0.496094]  [A loss: 0.847044, acc: 0.222656]\n",
      "447: [D loss: 0.702968, acc: 0.523438]  [A loss: 0.806885, acc: 0.296875]\n",
      "448: [D loss: 0.704735, acc: 0.501953]  [A loss: 0.864035, acc: 0.171875]\n",
      "449: [D loss: 0.687377, acc: 0.550781]  [A loss: 0.774419, acc: 0.347656]\n",
      "450: [D loss: 0.707286, acc: 0.529297]  [A loss: 1.014028, acc: 0.101562]\n",
      "451: [D loss: 0.696756, acc: 0.525391]  [A loss: 0.807287, acc: 0.308594]\n",
      "452: [D loss: 0.705413, acc: 0.529297]  [A loss: 0.811902, acc: 0.292969]\n",
      "453: [D loss: 0.685389, acc: 0.546875]  [A loss: 0.863524, acc: 0.218750]\n",
      "454: [D loss: 0.697702, acc: 0.546875]  [A loss: 0.855344, acc: 0.230469]\n",
      "455: [D loss: 0.682958, acc: 0.580078]  [A loss: 0.824424, acc: 0.261719]\n",
      "456: [D loss: 0.682301, acc: 0.558594]  [A loss: 0.927876, acc: 0.144531]\n",
      "457: [D loss: 0.692061, acc: 0.525391]  [A loss: 0.757179, acc: 0.394531]\n",
      "458: [D loss: 0.707166, acc: 0.519531]  [A loss: 0.939209, acc: 0.113281]\n",
      "459: [D loss: 0.685327, acc: 0.558594]  [A loss: 0.718738, acc: 0.507812]\n",
      "460: [D loss: 0.700108, acc: 0.511719]  [A loss: 0.971925, acc: 0.121094]\n",
      "461: [D loss: 0.678124, acc: 0.562500]  [A loss: 0.729425, acc: 0.472656]\n",
      "462: [D loss: 0.711104, acc: 0.521484]  [A loss: 0.945930, acc: 0.128906]\n",
      "463: [D loss: 0.689300, acc: 0.558594]  [A loss: 0.748056, acc: 0.417969]\n",
      "464: [D loss: 0.722043, acc: 0.486328]  [A loss: 0.995493, acc: 0.105469]\n",
      "465: [D loss: 0.693836, acc: 0.531250]  [A loss: 0.710190, acc: 0.484375]\n",
      "466: [D loss: 0.721096, acc: 0.517578]  [A loss: 0.938207, acc: 0.109375]\n",
      "467: [D loss: 0.695597, acc: 0.537109]  [A loss: 0.755684, acc: 0.343750]\n",
      "468: [D loss: 0.709762, acc: 0.531250]  [A loss: 1.028032, acc: 0.074219]\n",
      "469: [D loss: 0.688099, acc: 0.550781]  [A loss: 0.694344, acc: 0.496094]\n",
      "470: [D loss: 0.715884, acc: 0.505859]  [A loss: 0.908225, acc: 0.164062]\n",
      "471: [D loss: 0.694890, acc: 0.539062]  [A loss: 0.739129, acc: 0.437500]\n",
      "472: [D loss: 0.705703, acc: 0.515625]  [A loss: 0.915138, acc: 0.148438]\n",
      "473: [D loss: 0.690161, acc: 0.541016]  [A loss: 0.801084, acc: 0.308594]\n",
      "474: [D loss: 0.707461, acc: 0.509766]  [A loss: 0.938634, acc: 0.136719]\n",
      "475: [D loss: 0.705730, acc: 0.507812]  [A loss: 0.747776, acc: 0.394531]\n",
      "476: [D loss: 0.709871, acc: 0.521484]  [A loss: 0.854589, acc: 0.218750]\n",
      "477: [D loss: 0.704635, acc: 0.503906]  [A loss: 0.784010, acc: 0.328125]\n",
      "478: [D loss: 0.694690, acc: 0.558594]  [A loss: 0.850303, acc: 0.226562]\n",
      "479: [D loss: 0.694882, acc: 0.548828]  [A loss: 0.855751, acc: 0.191406]\n",
      "480: [D loss: 0.694268, acc: 0.511719]  [A loss: 0.843295, acc: 0.257812]\n",
      "481: [D loss: 0.702356, acc: 0.525391]  [A loss: 0.824998, acc: 0.281250]\n",
      "482: [D loss: 0.689082, acc: 0.525391]  [A loss: 0.863342, acc: 0.207031]\n",
      "483: [D loss: 0.710073, acc: 0.517578]  [A loss: 0.870169, acc: 0.195312]\n",
      "484: [D loss: 0.684508, acc: 0.566406]  [A loss: 0.880818, acc: 0.171875]\n",
      "485: [D loss: 0.699695, acc: 0.515625]  [A loss: 0.867668, acc: 0.226562]\n",
      "486: [D loss: 0.681965, acc: 0.537109]  [A loss: 0.795122, acc: 0.332031]\n",
      "487: [D loss: 0.689507, acc: 0.564453]  [A loss: 0.902463, acc: 0.148438]\n",
      "488: [D loss: 0.694663, acc: 0.580078]  [A loss: 0.810165, acc: 0.296875]\n",
      "489: [D loss: 0.702531, acc: 0.533203]  [A loss: 0.869195, acc: 0.210938]\n",
      "490: [D loss: 0.682139, acc: 0.574219]  [A loss: 0.854739, acc: 0.199219]\n",
      "491: [D loss: 0.704324, acc: 0.501953]  [A loss: 0.815420, acc: 0.281250]\n",
      "492: [D loss: 0.696932, acc: 0.542969]  [A loss: 0.961770, acc: 0.171875]\n",
      "493: [D loss: 0.678854, acc: 0.574219]  [A loss: 0.697693, acc: 0.531250]\n",
      "494: [D loss: 0.697701, acc: 0.552734]  [A loss: 0.947625, acc: 0.144531]\n",
      "495: [D loss: 0.698338, acc: 0.513672]  [A loss: 0.763899, acc: 0.351562]\n",
      "496: [D loss: 0.701794, acc: 0.539062]  [A loss: 0.894423, acc: 0.167969]\n",
      "497: [D loss: 0.704000, acc: 0.513672]  [A loss: 0.814174, acc: 0.265625]\n",
      "498: [D loss: 0.688005, acc: 0.562500]  [A loss: 0.871105, acc: 0.218750]\n",
      "499: [D loss: 0.688437, acc: 0.566406]  [A loss: 0.791675, acc: 0.332031]\n",
      "500: [D loss: 0.701254, acc: 0.535156]  [A loss: 0.881895, acc: 0.160156]\n",
      "501: [D loss: 0.674033, acc: 0.582031]  [A loss: 0.789946, acc: 0.371094]\n",
      "502: [D loss: 0.691398, acc: 0.550781]  [A loss: 0.847273, acc: 0.238281]\n",
      "503: [D loss: 0.697156, acc: 0.552734]  [A loss: 0.983389, acc: 0.105469]\n",
      "504: [D loss: 0.691674, acc: 0.537109]  [A loss: 0.785114, acc: 0.332031]\n",
      "505: [D loss: 0.706196, acc: 0.517578]  [A loss: 0.928550, acc: 0.148438]\n",
      "506: [D loss: 0.696800, acc: 0.525391]  [A loss: 0.847653, acc: 0.261719]\n",
      "507: [D loss: 0.719007, acc: 0.505859]  [A loss: 0.889900, acc: 0.167969]\n",
      "508: [D loss: 0.697520, acc: 0.537109]  [A loss: 0.841094, acc: 0.250000]\n",
      "509: [D loss: 0.695217, acc: 0.550781]  [A loss: 0.893221, acc: 0.152344]\n",
      "510: [D loss: 0.690108, acc: 0.556641]  [A loss: 0.789320, acc: 0.351562]\n",
      "511: [D loss: 0.702899, acc: 0.542969]  [A loss: 0.882833, acc: 0.179688]\n",
      "512: [D loss: 0.698097, acc: 0.517578]  [A loss: 0.802412, acc: 0.285156]\n",
      "513: [D loss: 0.691062, acc: 0.539062]  [A loss: 0.973041, acc: 0.078125]\n",
      "514: [D loss: 0.699041, acc: 0.542969]  [A loss: 0.675535, acc: 0.566406]\n",
      "515: [D loss: 0.723967, acc: 0.521484]  [A loss: 1.046429, acc: 0.078125]\n",
      "516: [D loss: 0.712136, acc: 0.507812]  [A loss: 0.717828, acc: 0.457031]\n",
      "517: [D loss: 0.703479, acc: 0.519531]  [A loss: 0.883317, acc: 0.156250]\n",
      "518: [D loss: 0.691508, acc: 0.554688]  [A loss: 0.799759, acc: 0.304688]\n",
      "519: [D loss: 0.688272, acc: 0.574219]  [A loss: 0.857501, acc: 0.230469]\n",
      "520: [D loss: 0.702000, acc: 0.523438]  [A loss: 0.808947, acc: 0.269531]\n",
      "521: [D loss: 0.696510, acc: 0.525391]  [A loss: 0.890526, acc: 0.140625]\n",
      "522: [D loss: 0.694638, acc: 0.556641]  [A loss: 0.803240, acc: 0.316406]\n",
      "523: [D loss: 0.702664, acc: 0.542969]  [A loss: 0.934724, acc: 0.132812]\n",
      "524: [D loss: 0.682630, acc: 0.537109]  [A loss: 0.734300, acc: 0.468750]\n",
      "525: [D loss: 0.706786, acc: 0.537109]  [A loss: 0.938895, acc: 0.164062]\n",
      "526: [D loss: 0.693348, acc: 0.546875]  [A loss: 0.763485, acc: 0.382812]\n",
      "527: [D loss: 0.687869, acc: 0.556641]  [A loss: 0.902404, acc: 0.187500]\n",
      "528: [D loss: 0.695595, acc: 0.556641]  [A loss: 0.755528, acc: 0.429688]\n",
      "529: [D loss: 0.709897, acc: 0.509766]  [A loss: 0.968226, acc: 0.105469]\n",
      "530: [D loss: 0.693968, acc: 0.541016]  [A loss: 0.747962, acc: 0.367188]\n",
      "531: [D loss: 0.699297, acc: 0.544922]  [A loss: 0.974120, acc: 0.113281]\n",
      "532: [D loss: 0.691473, acc: 0.574219]  [A loss: 0.731262, acc: 0.398438]\n",
      "533: [D loss: 0.709426, acc: 0.539062]  [A loss: 0.886622, acc: 0.179688]\n",
      "534: [D loss: 0.693647, acc: 0.519531]  [A loss: 0.746113, acc: 0.386719]\n",
      "535: [D loss: 0.708211, acc: 0.525391]  [A loss: 0.888339, acc: 0.179688]\n",
      "536: [D loss: 0.684698, acc: 0.562500]  [A loss: 0.766961, acc: 0.359375]\n",
      "537: [D loss: 0.700708, acc: 0.525391]  [A loss: 0.952583, acc: 0.085938]\n",
      "538: [D loss: 0.692742, acc: 0.539062]  [A loss: 0.796744, acc: 0.292969]\n",
      "539: [D loss: 0.688481, acc: 0.566406]  [A loss: 0.849152, acc: 0.218750]\n",
      "540: [D loss: 0.696357, acc: 0.521484]  [A loss: 0.884961, acc: 0.160156]\n",
      "541: [D loss: 0.691255, acc: 0.542969]  [A loss: 0.806220, acc: 0.304688]\n",
      "542: [D loss: 0.696435, acc: 0.523438]  [A loss: 0.875017, acc: 0.246094]\n",
      "543: [D loss: 0.708079, acc: 0.496094]  [A loss: 0.692065, acc: 0.539062]\n",
      "544: [D loss: 0.720106, acc: 0.513672]  [A loss: 0.932394, acc: 0.132812]\n",
      "545: [D loss: 0.701728, acc: 0.492188]  [A loss: 0.797044, acc: 0.320312]\n",
      "546: [D loss: 0.687914, acc: 0.552734]  [A loss: 0.864106, acc: 0.175781]\n",
      "547: [D loss: 0.686635, acc: 0.576172]  [A loss: 0.873784, acc: 0.218750]\n",
      "548: [D loss: 0.692569, acc: 0.552734]  [A loss: 0.817094, acc: 0.265625]\n",
      "549: [D loss: 0.704078, acc: 0.523438]  [A loss: 0.881566, acc: 0.187500]\n",
      "550: [D loss: 0.701362, acc: 0.496094]  [A loss: 0.782941, acc: 0.308594]\n",
      "551: [D loss: 0.687737, acc: 0.542969]  [A loss: 0.885595, acc: 0.121094]\n",
      "552: [D loss: 0.693636, acc: 0.529297]  [A loss: 0.746643, acc: 0.414062]\n",
      "553: [D loss: 0.703037, acc: 0.527344]  [A loss: 0.883652, acc: 0.167969]\n",
      "554: [D loss: 0.694847, acc: 0.521484]  [A loss: 0.796616, acc: 0.320312]\n",
      "555: [D loss: 0.700053, acc: 0.498047]  [A loss: 0.864315, acc: 0.199219]\n",
      "556: [D loss: 0.698355, acc: 0.511719]  [A loss: 0.835537, acc: 0.238281]\n",
      "557: [D loss: 0.692066, acc: 0.531250]  [A loss: 0.950173, acc: 0.136719]\n",
      "558: [D loss: 0.708200, acc: 0.521484]  [A loss: 0.726068, acc: 0.441406]\n",
      "559: [D loss: 0.741334, acc: 0.511719]  [A loss: 1.108774, acc: 0.027344]\n",
      "560: [D loss: 0.691982, acc: 0.542969]  [A loss: 0.650513, acc: 0.613281]\n",
      "561: [D loss: 0.718624, acc: 0.509766]  [A loss: 0.892157, acc: 0.152344]\n",
      "562: [D loss: 0.695753, acc: 0.537109]  [A loss: 0.778420, acc: 0.347656]\n",
      "563: [D loss: 0.699481, acc: 0.517578]  [A loss: 0.855239, acc: 0.207031]\n",
      "564: [D loss: 0.694101, acc: 0.531250]  [A loss: 0.798136, acc: 0.292969]\n",
      "565: [D loss: 0.697662, acc: 0.550781]  [A loss: 0.859349, acc: 0.210938]\n",
      "566: [D loss: 0.701008, acc: 0.500000]  [A loss: 0.791486, acc: 0.300781]\n",
      "567: [D loss: 0.688554, acc: 0.574219]  [A loss: 0.799662, acc: 0.292969]\n",
      "568: [D loss: 0.698766, acc: 0.509766]  [A loss: 0.811964, acc: 0.265625]\n",
      "569: [D loss: 0.683697, acc: 0.564453]  [A loss: 0.849555, acc: 0.265625]\n",
      "570: [D loss: 0.693234, acc: 0.539062]  [A loss: 0.825565, acc: 0.253906]\n",
      "571: [D loss: 0.688648, acc: 0.562500]  [A loss: 0.857820, acc: 0.242188]\n",
      "572: [D loss: 0.705105, acc: 0.505859]  [A loss: 0.817298, acc: 0.246094]\n",
      "573: [D loss: 0.705685, acc: 0.533203]  [A loss: 0.959684, acc: 0.109375]\n",
      "574: [D loss: 0.685046, acc: 0.558594]  [A loss: 0.745764, acc: 0.425781]\n",
      "575: [D loss: 0.705968, acc: 0.541016]  [A loss: 0.936364, acc: 0.128906]\n",
      "576: [D loss: 0.691309, acc: 0.539062]  [A loss: 0.738779, acc: 0.414062]\n",
      "577: [D loss: 0.704731, acc: 0.500000]  [A loss: 0.846806, acc: 0.207031]\n",
      "578: [D loss: 0.705840, acc: 0.511719]  [A loss: 0.799453, acc: 0.281250]\n",
      "579: [D loss: 0.703642, acc: 0.544922]  [A loss: 0.901769, acc: 0.152344]\n",
      "580: [D loss: 0.690203, acc: 0.525391]  [A loss: 0.796354, acc: 0.343750]\n",
      "581: [D loss: 0.699960, acc: 0.525391]  [A loss: 0.871217, acc: 0.218750]\n",
      "582: [D loss: 0.675045, acc: 0.570312]  [A loss: 0.846200, acc: 0.226562]\n",
      "583: [D loss: 0.693591, acc: 0.560547]  [A loss: 0.879365, acc: 0.179688]\n",
      "584: [D loss: 0.686602, acc: 0.566406]  [A loss: 0.780798, acc: 0.296875]\n",
      "585: [D loss: 0.694049, acc: 0.537109]  [A loss: 0.966665, acc: 0.113281]\n",
      "586: [D loss: 0.684301, acc: 0.570312]  [A loss: 0.786839, acc: 0.289062]\n",
      "587: [D loss: 0.714554, acc: 0.498047]  [A loss: 0.984403, acc: 0.082031]\n",
      "588: [D loss: 0.684052, acc: 0.531250]  [A loss: 0.695012, acc: 0.539062]\n",
      "589: [D loss: 0.712972, acc: 0.519531]  [A loss: 1.006076, acc: 0.082031]\n",
      "590: [D loss: 0.693498, acc: 0.552734]  [A loss: 0.702411, acc: 0.507812]\n",
      "591: [D loss: 0.724068, acc: 0.501953]  [A loss: 0.985074, acc: 0.078125]\n",
      "592: [D loss: 0.696082, acc: 0.525391]  [A loss: 0.755575, acc: 0.410156]\n",
      "593: [D loss: 0.697897, acc: 0.539062]  [A loss: 0.892127, acc: 0.210938]\n",
      "594: [D loss: 0.685902, acc: 0.556641]  [A loss: 0.815021, acc: 0.312500]\n",
      "595: [D loss: 0.700979, acc: 0.548828]  [A loss: 0.874493, acc: 0.218750]\n",
      "596: [D loss: 0.695123, acc: 0.535156]  [A loss: 0.788971, acc: 0.324219]\n",
      "597: [D loss: 0.722309, acc: 0.476562]  [A loss: 0.852620, acc: 0.195312]\n",
      "598: [D loss: 0.685414, acc: 0.546875]  [A loss: 0.860684, acc: 0.195312]\n",
      "599: [D loss: 0.700682, acc: 0.544922]  [A loss: 0.831284, acc: 0.261719]\n",
      "600: [D loss: 0.705268, acc: 0.527344]  [A loss: 0.904754, acc: 0.156250]\n",
      "601: [D loss: 0.692569, acc: 0.542969]  [A loss: 0.828330, acc: 0.261719]\n",
      "602: [D loss: 0.701150, acc: 0.542969]  [A loss: 0.850013, acc: 0.210938]\n",
      "603: [D loss: 0.681965, acc: 0.570312]  [A loss: 0.832523, acc: 0.253906]\n",
      "604: [D loss: 0.685582, acc: 0.544922]  [A loss: 0.927796, acc: 0.140625]\n",
      "605: [D loss: 0.690412, acc: 0.525391]  [A loss: 0.720426, acc: 0.484375]\n",
      "606: [D loss: 0.701424, acc: 0.519531]  [A loss: 0.943948, acc: 0.136719]\n",
      "607: [D loss: 0.686217, acc: 0.548828]  [A loss: 0.761953, acc: 0.343750]\n",
      "608: [D loss: 0.713339, acc: 0.505859]  [A loss: 0.925286, acc: 0.144531]\n",
      "609: [D loss: 0.684210, acc: 0.578125]  [A loss: 0.747322, acc: 0.402344]\n",
      "610: [D loss: 0.692643, acc: 0.550781]  [A loss: 1.012054, acc: 0.101562]\n",
      "611: [D loss: 0.683383, acc: 0.580078]  [A loss: 0.751398, acc: 0.417969]\n",
      "612: [D loss: 0.705608, acc: 0.527344]  [A loss: 0.880982, acc: 0.199219]\n",
      "613: [D loss: 0.694563, acc: 0.525391]  [A loss: 0.785817, acc: 0.351562]\n",
      "614: [D loss: 0.700900, acc: 0.550781]  [A loss: 0.922217, acc: 0.167969]\n",
      "615: [D loss: 0.701133, acc: 0.523438]  [A loss: 0.781937, acc: 0.347656]\n",
      "616: [D loss: 0.706079, acc: 0.527344]  [A loss: 0.958635, acc: 0.121094]\n",
      "617: [D loss: 0.682209, acc: 0.527344]  [A loss: 0.720047, acc: 0.449219]\n",
      "618: [D loss: 0.714597, acc: 0.535156]  [A loss: 0.940743, acc: 0.156250]\n",
      "619: [D loss: 0.697060, acc: 0.517578]  [A loss: 0.736247, acc: 0.414062]\n",
      "620: [D loss: 0.698633, acc: 0.541016]  [A loss: 0.959877, acc: 0.117188]\n",
      "621: [D loss: 0.676475, acc: 0.580078]  [A loss: 0.714372, acc: 0.476562]\n",
      "622: [D loss: 0.708860, acc: 0.550781]  [A loss: 0.961067, acc: 0.109375]\n",
      "623: [D loss: 0.679150, acc: 0.558594]  [A loss: 0.793378, acc: 0.335938]\n",
      "624: [D loss: 0.689572, acc: 0.560547]  [A loss: 0.851310, acc: 0.253906]\n",
      "625: [D loss: 0.687937, acc: 0.531250]  [A loss: 0.854105, acc: 0.234375]\n",
      "626: [D loss: 0.692552, acc: 0.548828]  [A loss: 0.836008, acc: 0.289062]\n",
      "627: [D loss: 0.691316, acc: 0.568359]  [A loss: 0.835069, acc: 0.246094]\n",
      "628: [D loss: 0.698301, acc: 0.539062]  [A loss: 0.864771, acc: 0.230469]\n",
      "629: [D loss: 0.685505, acc: 0.533203]  [A loss: 0.810751, acc: 0.292969]\n",
      "630: [D loss: 0.705373, acc: 0.544922]  [A loss: 0.976515, acc: 0.117188]\n",
      "631: [D loss: 0.696059, acc: 0.537109]  [A loss: 0.694649, acc: 0.539062]\n",
      "632: [D loss: 0.718445, acc: 0.542969]  [A loss: 1.087495, acc: 0.027344]\n",
      "633: [D loss: 0.697828, acc: 0.517578]  [A loss: 0.658825, acc: 0.625000]\n",
      "634: [D loss: 0.719222, acc: 0.517578]  [A loss: 0.961162, acc: 0.121094]\n",
      "635: [D loss: 0.693219, acc: 0.558594]  [A loss: 0.752404, acc: 0.367188]\n",
      "636: [D loss: 0.702429, acc: 0.542969]  [A loss: 0.870035, acc: 0.199219]\n",
      "637: [D loss: 0.698326, acc: 0.533203]  [A loss: 0.797491, acc: 0.335938]\n",
      "638: [D loss: 0.704330, acc: 0.505859]  [A loss: 0.840536, acc: 0.261719]\n",
      "639: [D loss: 0.686132, acc: 0.578125]  [A loss: 0.813781, acc: 0.277344]\n",
      "640: [D loss: 0.698328, acc: 0.525391]  [A loss: 0.845466, acc: 0.269531]\n",
      "641: [D loss: 0.720971, acc: 0.498047]  [A loss: 0.876683, acc: 0.191406]\n",
      "642: [D loss: 0.710128, acc: 0.505859]  [A loss: 0.745365, acc: 0.386719]\n",
      "643: [D loss: 0.706981, acc: 0.529297]  [A loss: 0.935532, acc: 0.132812]\n",
      "644: [D loss: 0.690802, acc: 0.541016]  [A loss: 0.714984, acc: 0.468750]\n",
      "645: [D loss: 0.705677, acc: 0.529297]  [A loss: 0.943077, acc: 0.136719]\n",
      "646: [D loss: 0.675206, acc: 0.595703]  [A loss: 0.741969, acc: 0.433594]\n",
      "647: [D loss: 0.706326, acc: 0.527344]  [A loss: 0.951122, acc: 0.132812]\n",
      "648: [D loss: 0.697268, acc: 0.533203]  [A loss: 0.735789, acc: 0.441406]\n",
      "649: [D loss: 0.718329, acc: 0.488281]  [A loss: 0.892823, acc: 0.187500]\n",
      "650: [D loss: 0.689255, acc: 0.535156]  [A loss: 0.744241, acc: 0.445312]\n",
      "651: [D loss: 0.722140, acc: 0.507812]  [A loss: 0.884268, acc: 0.195312]\n",
      "652: [D loss: 0.689589, acc: 0.537109]  [A loss: 0.769592, acc: 0.363281]\n",
      "653: [D loss: 0.697169, acc: 0.542969]  [A loss: 0.893214, acc: 0.175781]\n",
      "654: [D loss: 0.691709, acc: 0.550781]  [A loss: 0.772589, acc: 0.359375]\n",
      "655: [D loss: 0.695481, acc: 0.533203]  [A loss: 0.895728, acc: 0.156250]\n",
      "656: [D loss: 0.710123, acc: 0.501953]  [A loss: 0.770549, acc: 0.359375]\n",
      "657: [D loss: 0.702504, acc: 0.544922]  [A loss: 0.914750, acc: 0.160156]\n",
      "658: [D loss: 0.689324, acc: 0.556641]  [A loss: 0.763062, acc: 0.382812]\n",
      "659: [D loss: 0.691849, acc: 0.541016]  [A loss: 0.888400, acc: 0.203125]\n",
      "660: [D loss: 0.685271, acc: 0.562500]  [A loss: 0.760820, acc: 0.390625]\n",
      "661: [D loss: 0.730482, acc: 0.486328]  [A loss: 0.986469, acc: 0.101562]\n",
      "662: [D loss: 0.708174, acc: 0.478516]  [A loss: 0.739248, acc: 0.445312]\n",
      "663: [D loss: 0.710649, acc: 0.503906]  [A loss: 0.963987, acc: 0.093750]\n",
      "664: [D loss: 0.692266, acc: 0.548828]  [A loss: 0.723208, acc: 0.484375]\n",
      "665: [D loss: 0.708055, acc: 0.529297]  [A loss: 0.873326, acc: 0.195312]\n",
      "666: [D loss: 0.718640, acc: 0.496094]  [A loss: 0.816912, acc: 0.253906]\n",
      "667: [D loss: 0.689054, acc: 0.533203]  [A loss: 0.849056, acc: 0.226562]\n",
      "668: [D loss: 0.698796, acc: 0.511719]  [A loss: 0.869911, acc: 0.183594]\n",
      "669: [D loss: 0.698519, acc: 0.533203]  [A loss: 0.789005, acc: 0.312500]\n",
      "670: [D loss: 0.695686, acc: 0.550781]  [A loss: 0.887267, acc: 0.179688]\n",
      "671: [D loss: 0.688456, acc: 0.554688]  [A loss: 0.776088, acc: 0.343750]\n",
      "672: [D loss: 0.693940, acc: 0.552734]  [A loss: 0.906264, acc: 0.175781]\n",
      "673: [D loss: 0.689569, acc: 0.578125]  [A loss: 0.786999, acc: 0.359375]\n",
      "674: [D loss: 0.693215, acc: 0.564453]  [A loss: 0.884055, acc: 0.187500]\n",
      "675: [D loss: 0.694184, acc: 0.533203]  [A loss: 0.774895, acc: 0.335938]\n",
      "676: [D loss: 0.715189, acc: 0.517578]  [A loss: 0.848255, acc: 0.218750]\n",
      "677: [D loss: 0.700256, acc: 0.539062]  [A loss: 0.797343, acc: 0.285156]\n",
      "678: [D loss: 0.707330, acc: 0.511719]  [A loss: 0.810605, acc: 0.250000]\n",
      "679: [D loss: 0.694391, acc: 0.535156]  [A loss: 0.823435, acc: 0.277344]\n",
      "680: [D loss: 0.701203, acc: 0.544922]  [A loss: 0.906876, acc: 0.164062]\n",
      "681: [D loss: 0.683236, acc: 0.546875]  [A loss: 0.780247, acc: 0.343750]\n",
      "682: [D loss: 0.691779, acc: 0.550781]  [A loss: 0.883760, acc: 0.187500]\n",
      "683: [D loss: 0.686694, acc: 0.535156]  [A loss: 0.748322, acc: 0.414062]\n",
      "684: [D loss: 0.708066, acc: 0.537109]  [A loss: 0.986278, acc: 0.101562]\n",
      "685: [D loss: 0.685388, acc: 0.552734]  [A loss: 0.704638, acc: 0.507812]\n",
      "686: [D loss: 0.728085, acc: 0.521484]  [A loss: 0.947507, acc: 0.105469]\n",
      "687: [D loss: 0.698430, acc: 0.519531]  [A loss: 0.768083, acc: 0.390625]\n",
      "688: [D loss: 0.720472, acc: 0.519531]  [A loss: 0.949588, acc: 0.125000]\n",
      "689: [D loss: 0.693708, acc: 0.539062]  [A loss: 0.747896, acc: 0.445312]\n",
      "690: [D loss: 0.710399, acc: 0.519531]  [A loss: 0.932395, acc: 0.144531]\n",
      "691: [D loss: 0.687654, acc: 0.537109]  [A loss: 0.704925, acc: 0.527344]\n",
      "692: [D loss: 0.732776, acc: 0.509766]  [A loss: 0.986713, acc: 0.082031]\n",
      "693: [D loss: 0.688481, acc: 0.558594]  [A loss: 0.743721, acc: 0.375000]\n",
      "694: [D loss: 0.706440, acc: 0.525391]  [A loss: 0.844917, acc: 0.238281]\n",
      "695: [D loss: 0.701527, acc: 0.529297]  [A loss: 0.786782, acc: 0.289062]\n",
      "696: [D loss: 0.700395, acc: 0.529297]  [A loss: 0.849327, acc: 0.261719]\n",
      "697: [D loss: 0.686550, acc: 0.523438]  [A loss: 0.832108, acc: 0.238281]\n",
      "698: [D loss: 0.684927, acc: 0.535156]  [A loss: 0.848294, acc: 0.261719]\n",
      "699: [D loss: 0.708444, acc: 0.519531]  [A loss: 0.871111, acc: 0.195312]\n",
      "700: [D loss: 0.692554, acc: 0.537109]  [A loss: 0.843338, acc: 0.214844]\n",
      "701: [D loss: 0.688490, acc: 0.552734]  [A loss: 0.830909, acc: 0.257812]\n",
      "702: [D loss: 0.684936, acc: 0.572266]  [A loss: 0.819472, acc: 0.277344]\n",
      "703: [D loss: 0.695859, acc: 0.560547]  [A loss: 0.848526, acc: 0.234375]\n",
      "704: [D loss: 0.692775, acc: 0.560547]  [A loss: 0.812715, acc: 0.281250]\n",
      "705: [D loss: 0.693983, acc: 0.554688]  [A loss: 0.764959, acc: 0.386719]\n",
      "706: [D loss: 0.694401, acc: 0.548828]  [A loss: 0.961754, acc: 0.128906]\n",
      "707: [D loss: 0.716101, acc: 0.478516]  [A loss: 0.764467, acc: 0.367188]\n",
      "708: [D loss: 0.708485, acc: 0.533203]  [A loss: 0.959271, acc: 0.121094]\n",
      "709: [D loss: 0.681810, acc: 0.570312]  [A loss: 0.702037, acc: 0.500000]\n",
      "710: [D loss: 0.707722, acc: 0.507812]  [A loss: 0.956342, acc: 0.140625]\n",
      "711: [D loss: 0.699004, acc: 0.537109]  [A loss: 0.764540, acc: 0.339844]\n",
      "712: [D loss: 0.705070, acc: 0.515625]  [A loss: 0.879799, acc: 0.230469]\n",
      "713: [D loss: 0.690232, acc: 0.550781]  [A loss: 0.769394, acc: 0.363281]\n",
      "714: [D loss: 0.694756, acc: 0.539062]  [A loss: 0.844277, acc: 0.277344]\n",
      "715: [D loss: 0.696027, acc: 0.542969]  [A loss: 0.771073, acc: 0.359375]\n",
      "716: [D loss: 0.702844, acc: 0.541016]  [A loss: 0.963157, acc: 0.136719]\n",
      "717: [D loss: 0.698592, acc: 0.525391]  [A loss: 0.674043, acc: 0.558594]\n",
      "718: [D loss: 0.720167, acc: 0.527344]  [A loss: 0.983361, acc: 0.105469]\n",
      "719: [D loss: 0.690241, acc: 0.556641]  [A loss: 0.700026, acc: 0.519531]\n",
      "720: [D loss: 0.714362, acc: 0.519531]  [A loss: 0.940435, acc: 0.164062]\n",
      "721: [D loss: 0.691344, acc: 0.529297]  [A loss: 0.702428, acc: 0.503906]\n",
      "722: [D loss: 0.697952, acc: 0.544922]  [A loss: 0.854189, acc: 0.203125]\n",
      "723: [D loss: 0.695791, acc: 0.529297]  [A loss: 0.764206, acc: 0.355469]\n",
      "724: [D loss: 0.711836, acc: 0.517578]  [A loss: 0.913056, acc: 0.171875]\n",
      "725: [D loss: 0.702754, acc: 0.494141]  [A loss: 0.733038, acc: 0.445312]\n",
      "726: [D loss: 0.704243, acc: 0.535156]  [A loss: 0.883420, acc: 0.199219]\n",
      "727: [D loss: 0.708959, acc: 0.539062]  [A loss: 0.799531, acc: 0.300781]\n",
      "728: [D loss: 0.691616, acc: 0.539062]  [A loss: 0.859029, acc: 0.195312]\n",
      "729: [D loss: 0.690848, acc: 0.533203]  [A loss: 0.815112, acc: 0.277344]\n",
      "730: [D loss: 0.701452, acc: 0.541016]  [A loss: 0.883236, acc: 0.199219]\n",
      "731: [D loss: 0.678377, acc: 0.564453]  [A loss: 0.815101, acc: 0.273438]\n",
      "732: [D loss: 0.706063, acc: 0.511719]  [A loss: 0.842867, acc: 0.218750]\n",
      "733: [D loss: 0.690713, acc: 0.541016]  [A loss: 0.879513, acc: 0.187500]\n",
      "734: [D loss: 0.699839, acc: 0.531250]  [A loss: 0.819329, acc: 0.253906]\n",
      "735: [D loss: 0.695013, acc: 0.562500]  [A loss: 0.832238, acc: 0.242188]\n",
      "736: [D loss: 0.691105, acc: 0.544922]  [A loss: 0.827565, acc: 0.238281]\n",
      "737: [D loss: 0.700626, acc: 0.548828]  [A loss: 0.864816, acc: 0.199219]\n",
      "738: [D loss: 0.686002, acc: 0.548828]  [A loss: 0.840134, acc: 0.230469]\n",
      "739: [D loss: 0.697490, acc: 0.525391]  [A loss: 0.847287, acc: 0.242188]\n",
      "740: [D loss: 0.685209, acc: 0.560547]  [A loss: 0.884161, acc: 0.203125]\n",
      "741: [D loss: 0.694413, acc: 0.535156]  [A loss: 0.723429, acc: 0.449219]\n",
      "742: [D loss: 0.702618, acc: 0.533203]  [A loss: 0.984956, acc: 0.093750]\n",
      "743: [D loss: 0.701584, acc: 0.523438]  [A loss: 0.672771, acc: 0.550781]\n",
      "744: [D loss: 0.728148, acc: 0.492188]  [A loss: 0.942747, acc: 0.105469]\n",
      "745: [D loss: 0.685135, acc: 0.568359]  [A loss: 0.760031, acc: 0.394531]\n",
      "746: [D loss: 0.692454, acc: 0.544922]  [A loss: 0.867634, acc: 0.160156]\n",
      "747: [D loss: 0.692589, acc: 0.531250]  [A loss: 0.794664, acc: 0.292969]\n",
      "748: [D loss: 0.698539, acc: 0.544922]  [A loss: 0.834175, acc: 0.261719]\n",
      "749: [D loss: 0.689119, acc: 0.556641]  [A loss: 0.814278, acc: 0.277344]\n",
      "750: [D loss: 0.693730, acc: 0.541016]  [A loss: 0.832129, acc: 0.281250]\n",
      "751: [D loss: 0.682029, acc: 0.583984]  [A loss: 0.881903, acc: 0.210938]\n",
      "752: [D loss: 0.703780, acc: 0.513672]  [A loss: 0.714581, acc: 0.480469]\n",
      "753: [D loss: 0.712149, acc: 0.539062]  [A loss: 0.986250, acc: 0.082031]\n",
      "754: [D loss: 0.694306, acc: 0.533203]  [A loss: 0.728884, acc: 0.449219]\n",
      "755: [D loss: 0.704898, acc: 0.515625]  [A loss: 0.897761, acc: 0.160156]\n",
      "756: [D loss: 0.686417, acc: 0.539062]  [A loss: 0.767287, acc: 0.371094]\n",
      "757: [D loss: 0.713232, acc: 0.529297]  [A loss: 0.903423, acc: 0.164062]\n",
      "758: [D loss: 0.688123, acc: 0.554688]  [A loss: 0.737092, acc: 0.445312]\n",
      "759: [D loss: 0.693982, acc: 0.568359]  [A loss: 0.922629, acc: 0.140625]\n",
      "760: [D loss: 0.702359, acc: 0.501953]  [A loss: 0.778019, acc: 0.367188]\n",
      "761: [D loss: 0.718329, acc: 0.503906]  [A loss: 0.852142, acc: 0.203125]\n",
      "762: [D loss: 0.715177, acc: 0.466797]  [A loss: 0.773160, acc: 0.371094]\n",
      "763: [D loss: 0.694320, acc: 0.533203]  [A loss: 0.809929, acc: 0.289062]\n",
      "764: [D loss: 0.695109, acc: 0.517578]  [A loss: 0.811700, acc: 0.312500]\n",
      "765: [D loss: 0.702714, acc: 0.531250]  [A loss: 0.816148, acc: 0.250000]\n",
      "766: [D loss: 0.702164, acc: 0.531250]  [A loss: 0.787643, acc: 0.363281]\n",
      "767: [D loss: 0.699639, acc: 0.548828]  [A loss: 0.965045, acc: 0.101562]\n",
      "768: [D loss: 0.692547, acc: 0.537109]  [A loss: 0.752033, acc: 0.390625]\n",
      "769: [D loss: 0.718850, acc: 0.513672]  [A loss: 1.022090, acc: 0.054688]\n",
      "770: [D loss: 0.687040, acc: 0.542969]  [A loss: 0.735431, acc: 0.476562]\n",
      "771: [D loss: 0.703576, acc: 0.533203]  [A loss: 0.909125, acc: 0.117188]\n",
      "772: [D loss: 0.690052, acc: 0.558594]  [A loss: 0.758614, acc: 0.367188]\n",
      "773: [D loss: 0.711386, acc: 0.527344]  [A loss: 0.929449, acc: 0.125000]\n",
      "774: [D loss: 0.702543, acc: 0.525391]  [A loss: 0.706936, acc: 0.484375]\n",
      "775: [D loss: 0.701340, acc: 0.525391]  [A loss: 0.933532, acc: 0.113281]\n",
      "776: [D loss: 0.694375, acc: 0.535156]  [A loss: 0.689687, acc: 0.531250]\n",
      "777: [D loss: 0.708982, acc: 0.529297]  [A loss: 0.958650, acc: 0.125000]\n",
      "778: [D loss: 0.688274, acc: 0.566406]  [A loss: 0.786111, acc: 0.308594]\n",
      "779: [D loss: 0.699598, acc: 0.533203]  [A loss: 0.878900, acc: 0.191406]\n",
      "780: [D loss: 0.686674, acc: 0.529297]  [A loss: 0.794471, acc: 0.335938]\n",
      "781: [D loss: 0.697210, acc: 0.544922]  [A loss: 0.933668, acc: 0.121094]\n",
      "782: [D loss: 0.701311, acc: 0.505859]  [A loss: 0.719358, acc: 0.488281]\n",
      "783: [D loss: 0.710437, acc: 0.541016]  [A loss: 0.930118, acc: 0.152344]\n",
      "784: [D loss: 0.697323, acc: 0.537109]  [A loss: 0.693152, acc: 0.519531]\n",
      "785: [D loss: 0.708819, acc: 0.527344]  [A loss: 0.878397, acc: 0.214844]\n",
      "786: [D loss: 0.696282, acc: 0.521484]  [A loss: 0.730938, acc: 0.421875]\n",
      "787: [D loss: 0.705072, acc: 0.537109]  [A loss: 0.826206, acc: 0.234375]\n",
      "788: [D loss: 0.702076, acc: 0.513672]  [A loss: 0.814227, acc: 0.269531]\n",
      "789: [D loss: 0.695461, acc: 0.554688]  [A loss: 0.809001, acc: 0.265625]\n",
      "790: [D loss: 0.684289, acc: 0.568359]  [A loss: 0.845131, acc: 0.242188]\n",
      "791: [D loss: 0.680735, acc: 0.578125]  [A loss: 0.791125, acc: 0.339844]\n",
      "792: [D loss: 0.700455, acc: 0.521484]  [A loss: 0.883367, acc: 0.191406]\n",
      "793: [D loss: 0.688251, acc: 0.550781]  [A loss: 0.718259, acc: 0.500000]\n",
      "794: [D loss: 0.696299, acc: 0.521484]  [A loss: 0.897395, acc: 0.187500]\n",
      "795: [D loss: 0.693665, acc: 0.546875]  [A loss: 0.747561, acc: 0.386719]\n",
      "796: [D loss: 0.705068, acc: 0.542969]  [A loss: 0.883140, acc: 0.160156]\n",
      "797: [D loss: 0.703726, acc: 0.523438]  [A loss: 0.787799, acc: 0.332031]\n",
      "798: [D loss: 0.720272, acc: 0.501953]  [A loss: 0.941434, acc: 0.101562]\n",
      "799: [D loss: 0.683174, acc: 0.560547]  [A loss: 0.715724, acc: 0.445312]\n",
      "800: [D loss: 0.727057, acc: 0.498047]  [A loss: 0.956230, acc: 0.089844]\n",
      "801: [D loss: 0.695261, acc: 0.541016]  [A loss: 0.768479, acc: 0.328125]\n",
      "802: [D loss: 0.700967, acc: 0.527344]  [A loss: 0.838759, acc: 0.246094]\n",
      "803: [D loss: 0.679651, acc: 0.566406]  [A loss: 0.849928, acc: 0.191406]\n",
      "804: [D loss: 0.695934, acc: 0.501953]  [A loss: 0.828249, acc: 0.265625]\n",
      "805: [D loss: 0.705538, acc: 0.515625]  [A loss: 0.820484, acc: 0.281250]\n",
      "806: [D loss: 0.712001, acc: 0.496094]  [A loss: 0.867705, acc: 0.214844]\n",
      "807: [D loss: 0.689584, acc: 0.556641]  [A loss: 0.753246, acc: 0.351562]\n",
      "808: [D loss: 0.702010, acc: 0.519531]  [A loss: 0.931645, acc: 0.140625]\n",
      "809: [D loss: 0.692589, acc: 0.541016]  [A loss: 0.743982, acc: 0.414062]\n",
      "810: [D loss: 0.699738, acc: 0.515625]  [A loss: 0.952671, acc: 0.140625]\n",
      "811: [D loss: 0.696560, acc: 0.533203]  [A loss: 0.708591, acc: 0.496094]\n",
      "812: [D loss: 0.713597, acc: 0.519531]  [A loss: 0.890292, acc: 0.187500]\n",
      "813: [D loss: 0.709027, acc: 0.513672]  [A loss: 0.755796, acc: 0.359375]\n",
      "814: [D loss: 0.701961, acc: 0.535156]  [A loss: 0.837780, acc: 0.218750]\n",
      "815: [D loss: 0.702825, acc: 0.509766]  [A loss: 0.797065, acc: 0.308594]\n",
      "816: [D loss: 0.692041, acc: 0.544922]  [A loss: 0.799399, acc: 0.300781]\n",
      "817: [D loss: 0.706533, acc: 0.515625]  [A loss: 0.791281, acc: 0.343750]\n",
      "818: [D loss: 0.693947, acc: 0.550781]  [A loss: 0.886592, acc: 0.187500]\n",
      "819: [D loss: 0.701703, acc: 0.509766]  [A loss: 0.860310, acc: 0.261719]\n",
      "820: [D loss: 0.704533, acc: 0.542969]  [A loss: 0.850458, acc: 0.230469]\n",
      "821: [D loss: 0.698888, acc: 0.525391]  [A loss: 0.864731, acc: 0.222656]\n",
      "822: [D loss: 0.704515, acc: 0.511719]  [A loss: 0.840830, acc: 0.257812]\n",
      "823: [D loss: 0.703461, acc: 0.523438]  [A loss: 0.810649, acc: 0.304688]\n",
      "824: [D loss: 0.697734, acc: 0.550781]  [A loss: 0.847804, acc: 0.253906]\n",
      "825: [D loss: 0.703044, acc: 0.541016]  [A loss: 0.804662, acc: 0.320312]\n",
      "826: [D loss: 0.702928, acc: 0.521484]  [A loss: 0.914185, acc: 0.140625]\n",
      "827: [D loss: 0.701769, acc: 0.511719]  [A loss: 0.725974, acc: 0.449219]\n",
      "828: [D loss: 0.700529, acc: 0.556641]  [A loss: 0.951287, acc: 0.128906]\n",
      "829: [D loss: 0.678579, acc: 0.562500]  [A loss: 0.754950, acc: 0.375000]\n",
      "830: [D loss: 0.706789, acc: 0.554688]  [A loss: 0.884789, acc: 0.214844]\n",
      "831: [D loss: 0.689312, acc: 0.558594]  [A loss: 0.755413, acc: 0.414062]\n",
      "832: [D loss: 0.708156, acc: 0.541016]  [A loss: 0.859207, acc: 0.210938]\n",
      "833: [D loss: 0.690659, acc: 0.541016]  [A loss: 0.812272, acc: 0.320312]\n",
      "834: [D loss: 0.689315, acc: 0.525391]  [A loss: 0.932144, acc: 0.121094]\n",
      "835: [D loss: 0.697760, acc: 0.525391]  [A loss: 0.783044, acc: 0.343750]\n",
      "836: [D loss: 0.701503, acc: 0.517578]  [A loss: 0.884557, acc: 0.210938]\n",
      "837: [D loss: 0.695530, acc: 0.539062]  [A loss: 0.786521, acc: 0.343750]\n",
      "838: [D loss: 0.703970, acc: 0.525391]  [A loss: 0.871714, acc: 0.183594]\n",
      "839: [D loss: 0.694802, acc: 0.539062]  [A loss: 0.757815, acc: 0.410156]\n",
      "840: [D loss: 0.711151, acc: 0.498047]  [A loss: 0.964215, acc: 0.136719]\n",
      "841: [D loss: 0.699544, acc: 0.531250]  [A loss: 0.759282, acc: 0.351562]\n",
      "842: [D loss: 0.695537, acc: 0.539062]  [A loss: 0.880247, acc: 0.164062]\n",
      "843: [D loss: 0.692703, acc: 0.527344]  [A loss: 0.777845, acc: 0.328125]\n",
      "844: [D loss: 0.708801, acc: 0.533203]  [A loss: 0.869331, acc: 0.218750]\n",
      "845: [D loss: 0.692969, acc: 0.550781]  [A loss: 0.819129, acc: 0.238281]\n",
      "846: [D loss: 0.703352, acc: 0.517578]  [A loss: 0.850384, acc: 0.222656]\n",
      "847: [D loss: 0.686230, acc: 0.560547]  [A loss: 0.790001, acc: 0.312500]\n",
      "848: [D loss: 0.698252, acc: 0.521484]  [A loss: 0.887678, acc: 0.167969]\n",
      "849: [D loss: 0.701968, acc: 0.525391]  [A loss: 0.890063, acc: 0.179688]\n",
      "850: [D loss: 0.696529, acc: 0.531250]  [A loss: 0.847671, acc: 0.222656]\n",
      "851: [D loss: 0.702923, acc: 0.513672]  [A loss: 0.844061, acc: 0.277344]\n",
      "852: [D loss: 0.693127, acc: 0.554688]  [A loss: 0.857695, acc: 0.218750]\n",
      "853: [D loss: 0.692799, acc: 0.558594]  [A loss: 0.774627, acc: 0.339844]\n",
      "854: [D loss: 0.711854, acc: 0.523438]  [A loss: 0.874328, acc: 0.214844]\n",
      "855: [D loss: 0.694128, acc: 0.554688]  [A loss: 0.814643, acc: 0.265625]\n",
      "856: [D loss: 0.707100, acc: 0.505859]  [A loss: 0.858094, acc: 0.218750]\n",
      "857: [D loss: 0.711499, acc: 0.513672]  [A loss: 0.803424, acc: 0.335938]\n",
      "858: [D loss: 0.703624, acc: 0.513672]  [A loss: 0.875038, acc: 0.203125]\n",
      "859: [D loss: 0.692753, acc: 0.542969]  [A loss: 0.817421, acc: 0.269531]\n",
      "860: [D loss: 0.704150, acc: 0.517578]  [A loss: 0.903811, acc: 0.152344]\n",
      "861: [D loss: 0.693673, acc: 0.525391]  [A loss: 0.714381, acc: 0.460938]\n",
      "862: [D loss: 0.711257, acc: 0.523438]  [A loss: 0.967377, acc: 0.140625]\n",
      "863: [D loss: 0.700070, acc: 0.511719]  [A loss: 0.673461, acc: 0.570312]\n",
      "864: [D loss: 0.727704, acc: 0.500000]  [A loss: 0.990012, acc: 0.070312]\n",
      "865: [D loss: 0.709107, acc: 0.503906]  [A loss: 0.707632, acc: 0.503906]\n",
      "866: [D loss: 0.723355, acc: 0.496094]  [A loss: 0.836812, acc: 0.253906]\n",
      "867: [D loss: 0.703839, acc: 0.505859]  [A loss: 0.820906, acc: 0.269531]\n",
      "868: [D loss: 0.707160, acc: 0.496094]  [A loss: 0.791569, acc: 0.335938]\n",
      "869: [D loss: 0.712633, acc: 0.521484]  [A loss: 0.782592, acc: 0.324219]\n",
      "870: [D loss: 0.685124, acc: 0.554688]  [A loss: 0.807693, acc: 0.273438]\n",
      "871: [D loss: 0.682632, acc: 0.578125]  [A loss: 0.810517, acc: 0.261719]\n",
      "872: [D loss: 0.689797, acc: 0.541016]  [A loss: 0.798954, acc: 0.308594]\n",
      "873: [D loss: 0.705038, acc: 0.527344]  [A loss: 0.965062, acc: 0.132812]\n",
      "874: [D loss: 0.680781, acc: 0.572266]  [A loss: 0.770371, acc: 0.351562]\n",
      "875: [D loss: 0.722071, acc: 0.462891]  [A loss: 0.913946, acc: 0.179688]\n",
      "876: [D loss: 0.694697, acc: 0.535156]  [A loss: 0.753626, acc: 0.406250]\n",
      "877: [D loss: 0.707239, acc: 0.503906]  [A loss: 0.864361, acc: 0.195312]\n",
      "878: [D loss: 0.698458, acc: 0.535156]  [A loss: 0.769507, acc: 0.347656]\n",
      "879: [D loss: 0.710274, acc: 0.513672]  [A loss: 0.889109, acc: 0.171875]\n",
      "880: [D loss: 0.695677, acc: 0.515625]  [A loss: 0.785731, acc: 0.328125]\n",
      "881: [D loss: 0.699077, acc: 0.537109]  [A loss: 0.926228, acc: 0.156250]\n",
      "882: [D loss: 0.693454, acc: 0.537109]  [A loss: 0.724745, acc: 0.437500]\n",
      "883: [D loss: 0.711459, acc: 0.519531]  [A loss: 0.892688, acc: 0.148438]\n",
      "884: [D loss: 0.700449, acc: 0.535156]  [A loss: 0.816630, acc: 0.300781]\n",
      "885: [D loss: 0.688486, acc: 0.550781]  [A loss: 0.752180, acc: 0.371094]\n",
      "886: [D loss: 0.704888, acc: 0.517578]  [A loss: 0.862392, acc: 0.187500]\n",
      "887: [D loss: 0.700712, acc: 0.531250]  [A loss: 0.740031, acc: 0.390625]\n",
      "888: [D loss: 0.709019, acc: 0.517578]  [A loss: 0.954688, acc: 0.097656]\n",
      "889: [D loss: 0.690058, acc: 0.550781]  [A loss: 0.743144, acc: 0.394531]\n",
      "890: [D loss: 0.710989, acc: 0.523438]  [A loss: 0.905333, acc: 0.140625]\n",
      "891: [D loss: 0.696763, acc: 0.546875]  [A loss: 0.765424, acc: 0.398438]\n",
      "892: [D loss: 0.692123, acc: 0.554688]  [A loss: 0.883896, acc: 0.191406]\n",
      "893: [D loss: 0.692882, acc: 0.556641]  [A loss: 0.800603, acc: 0.316406]\n",
      "894: [D loss: 0.695944, acc: 0.562500]  [A loss: 0.848233, acc: 0.234375]\n",
      "895: [D loss: 0.709485, acc: 0.509766]  [A loss: 0.821582, acc: 0.273438]\n",
      "896: [D loss: 0.703115, acc: 0.507812]  [A loss: 0.869179, acc: 0.214844]\n",
      "897: [D loss: 0.682076, acc: 0.568359]  [A loss: 0.783880, acc: 0.347656]\n",
      "898: [D loss: 0.716833, acc: 0.513672]  [A loss: 1.006842, acc: 0.089844]\n",
      "899: [D loss: 0.693121, acc: 0.529297]  [A loss: 0.667605, acc: 0.613281]\n",
      "900: [D loss: 0.723796, acc: 0.517578]  [A loss: 0.978174, acc: 0.128906]\n",
      "901: [D loss: 0.695012, acc: 0.535156]  [A loss: 0.695871, acc: 0.550781]\n",
      "902: [D loss: 0.706980, acc: 0.496094]  [A loss: 0.877626, acc: 0.191406]\n",
      "903: [D loss: 0.683936, acc: 0.531250]  [A loss: 0.761324, acc: 0.378906]\n",
      "904: [D loss: 0.702249, acc: 0.505859]  [A loss: 0.838391, acc: 0.226562]\n",
      "905: [D loss: 0.702473, acc: 0.509766]  [A loss: 0.798849, acc: 0.312500]\n",
      "906: [D loss: 0.700226, acc: 0.511719]  [A loss: 0.816786, acc: 0.285156]\n",
      "907: [D loss: 0.694615, acc: 0.544922]  [A loss: 0.806645, acc: 0.285156]\n",
      "908: [D loss: 0.706771, acc: 0.513672]  [A loss: 0.865992, acc: 0.246094]\n",
      "909: [D loss: 0.697791, acc: 0.546875]  [A loss: 0.753871, acc: 0.371094]\n",
      "910: [D loss: 0.699865, acc: 0.546875]  [A loss: 0.831388, acc: 0.230469]\n",
      "911: [D loss: 0.695849, acc: 0.544922]  [A loss: 0.797532, acc: 0.312500]\n",
      "912: [D loss: 0.689178, acc: 0.535156]  [A loss: 0.884431, acc: 0.183594]\n",
      "913: [D loss: 0.690756, acc: 0.546875]  [A loss: 0.803278, acc: 0.289062]\n",
      "914: [D loss: 0.704711, acc: 0.541016]  [A loss: 0.859188, acc: 0.210938]\n",
      "915: [D loss: 0.709913, acc: 0.492188]  [A loss: 0.776672, acc: 0.378906]\n",
      "916: [D loss: 0.702293, acc: 0.505859]  [A loss: 0.883315, acc: 0.246094]\n",
      "917: [D loss: 0.715756, acc: 0.474609]  [A loss: 0.798403, acc: 0.324219]\n",
      "918: [D loss: 0.709536, acc: 0.521484]  [A loss: 0.893126, acc: 0.152344]\n",
      "919: [D loss: 0.693473, acc: 0.541016]  [A loss: 0.775286, acc: 0.300781]\n",
      "920: [D loss: 0.705121, acc: 0.544922]  [A loss: 0.928330, acc: 0.121094]\n",
      "921: [D loss: 0.696240, acc: 0.537109]  [A loss: 0.725797, acc: 0.480469]\n",
      "922: [D loss: 0.700476, acc: 0.535156]  [A loss: 0.937379, acc: 0.140625]\n",
      "923: [D loss: 0.687298, acc: 0.533203]  [A loss: 0.697224, acc: 0.503906]\n",
      "924: [D loss: 0.704228, acc: 0.537109]  [A loss: 0.984049, acc: 0.101562]\n",
      "925: [D loss: 0.691847, acc: 0.544922]  [A loss: 0.695612, acc: 0.492188]\n",
      "926: [D loss: 0.713233, acc: 0.523438]  [A loss: 0.920253, acc: 0.164062]\n",
      "927: [D loss: 0.697426, acc: 0.535156]  [A loss: 0.754455, acc: 0.375000]\n",
      "928: [D loss: 0.697763, acc: 0.531250]  [A loss: 0.807215, acc: 0.277344]\n",
      "929: [D loss: 0.693758, acc: 0.542969]  [A loss: 0.803988, acc: 0.277344]\n",
      "930: [D loss: 0.695127, acc: 0.544922]  [A loss: 0.846258, acc: 0.234375]\n",
      "931: [D loss: 0.692091, acc: 0.515625]  [A loss: 0.822053, acc: 0.210938]\n",
      "932: [D loss: 0.699800, acc: 0.521484]  [A loss: 0.876009, acc: 0.183594]\n",
      "933: [D loss: 0.698224, acc: 0.552734]  [A loss: 0.898070, acc: 0.156250]\n",
      "934: [D loss: 0.692799, acc: 0.564453]  [A loss: 0.746924, acc: 0.457031]\n",
      "935: [D loss: 0.698058, acc: 0.519531]  [A loss: 0.909956, acc: 0.164062]\n",
      "936: [D loss: 0.699376, acc: 0.513672]  [A loss: 0.703042, acc: 0.488281]\n",
      "937: [D loss: 0.703010, acc: 0.515625]  [A loss: 0.968033, acc: 0.105469]\n",
      "938: [D loss: 0.684985, acc: 0.560547]  [A loss: 0.737676, acc: 0.441406]\n",
      "939: [D loss: 0.704220, acc: 0.505859]  [A loss: 0.861789, acc: 0.203125]\n",
      "940: [D loss: 0.694453, acc: 0.550781]  [A loss: 0.799344, acc: 0.289062]\n",
      "941: [D loss: 0.701246, acc: 0.523438]  [A loss: 0.779355, acc: 0.320312]\n",
      "942: [D loss: 0.699056, acc: 0.542969]  [A loss: 0.919461, acc: 0.160156]\n",
      "943: [D loss: 0.698839, acc: 0.517578]  [A loss: 0.706806, acc: 0.515625]\n",
      "944: [D loss: 0.726781, acc: 0.500000]  [A loss: 1.051612, acc: 0.035156]\n",
      "945: [D loss: 0.689489, acc: 0.554688]  [A loss: 0.686200, acc: 0.566406]\n",
      "946: [D loss: 0.701020, acc: 0.550781]  [A loss: 0.872649, acc: 0.179688]\n",
      "947: [D loss: 0.685652, acc: 0.544922]  [A loss: 0.749412, acc: 0.394531]\n",
      "948: [D loss: 0.704608, acc: 0.529297]  [A loss: 0.848930, acc: 0.210938]\n",
      "949: [D loss: 0.703229, acc: 0.519531]  [A loss: 0.765480, acc: 0.398438]\n",
      "950: [D loss: 0.706411, acc: 0.542969]  [A loss: 0.898248, acc: 0.191406]\n",
      "951: [D loss: 0.689716, acc: 0.535156]  [A loss: 0.830735, acc: 0.222656]\n",
      "952: [D loss: 0.693293, acc: 0.529297]  [A loss: 0.810332, acc: 0.328125]\n",
      "953: [D loss: 0.692530, acc: 0.560547]  [A loss: 0.835059, acc: 0.238281]\n",
      "954: [D loss: 0.694879, acc: 0.544922]  [A loss: 0.733584, acc: 0.464844]\n",
      "955: [D loss: 0.730095, acc: 0.527344]  [A loss: 0.943420, acc: 0.136719]\n",
      "956: [D loss: 0.696127, acc: 0.515625]  [A loss: 0.744024, acc: 0.386719]\n",
      "957: [D loss: 0.701086, acc: 0.527344]  [A loss: 0.846548, acc: 0.226562]\n",
      "958: [D loss: 0.716156, acc: 0.496094]  [A loss: 0.801674, acc: 0.316406]\n",
      "959: [D loss: 0.689712, acc: 0.537109]  [A loss: 0.852765, acc: 0.218750]\n",
      "960: [D loss: 0.705078, acc: 0.496094]  [A loss: 0.800463, acc: 0.347656]\n",
      "961: [D loss: 0.702320, acc: 0.515625]  [A loss: 0.852528, acc: 0.214844]\n",
      "962: [D loss: 0.686945, acc: 0.566406]  [A loss: 0.738249, acc: 0.402344]\n",
      "963: [D loss: 0.703487, acc: 0.533203]  [A loss: 0.854047, acc: 0.230469]\n",
      "964: [D loss: 0.706939, acc: 0.496094]  [A loss: 0.753013, acc: 0.363281]\n",
      "965: [D loss: 0.708370, acc: 0.505859]  [A loss: 0.884599, acc: 0.179688]\n",
      "966: [D loss: 0.708302, acc: 0.498047]  [A loss: 0.794861, acc: 0.300781]\n",
      "967: [D loss: 0.695934, acc: 0.556641]  [A loss: 0.872320, acc: 0.187500]\n",
      "968: [D loss: 0.684610, acc: 0.552734]  [A loss: 0.738728, acc: 0.406250]\n",
      "969: [D loss: 0.699595, acc: 0.539062]  [A loss: 0.860421, acc: 0.167969]\n",
      "970: [D loss: 0.689631, acc: 0.515625]  [A loss: 0.824377, acc: 0.281250]\n",
      "971: [D loss: 0.715024, acc: 0.511719]  [A loss: 0.824007, acc: 0.281250]\n",
      "972: [D loss: 0.699177, acc: 0.533203]  [A loss: 0.825254, acc: 0.218750]\n",
      "973: [D loss: 0.701914, acc: 0.542969]  [A loss: 0.892622, acc: 0.210938]\n",
      "974: [D loss: 0.692977, acc: 0.515625]  [A loss: 0.678890, acc: 0.542969]\n",
      "975: [D loss: 0.719926, acc: 0.494141]  [A loss: 0.932368, acc: 0.117188]\n",
      "976: [D loss: 0.690922, acc: 0.539062]  [A loss: 0.674591, acc: 0.597656]\n",
      "977: [D loss: 0.745436, acc: 0.496094]  [A loss: 0.993903, acc: 0.109375]\n",
      "978: [D loss: 0.689796, acc: 0.531250]  [A loss: 0.665504, acc: 0.605469]\n",
      "979: [D loss: 0.703576, acc: 0.531250]  [A loss: 0.816958, acc: 0.253906]\n",
      "980: [D loss: 0.689877, acc: 0.542969]  [A loss: 0.765219, acc: 0.375000]\n",
      "981: [D loss: 0.700722, acc: 0.539062]  [A loss: 0.857260, acc: 0.164062]\n",
      "982: [D loss: 0.698402, acc: 0.511719]  [A loss: 0.802753, acc: 0.320312]\n",
      "983: [D loss: 0.709034, acc: 0.535156]  [A loss: 0.907158, acc: 0.132812]\n",
      "984: [D loss: 0.685608, acc: 0.585938]  [A loss: 0.770105, acc: 0.343750]\n",
      "985: [D loss: 0.690483, acc: 0.529297]  [A loss: 0.942579, acc: 0.101562]\n",
      "986: [D loss: 0.692111, acc: 0.537109]  [A loss: 0.699336, acc: 0.503906]\n",
      "987: [D loss: 0.706483, acc: 0.535156]  [A loss: 0.887521, acc: 0.164062]\n",
      "988: [D loss: 0.696272, acc: 0.537109]  [A loss: 0.766213, acc: 0.367188]\n",
      "989: [D loss: 0.687393, acc: 0.552734]  [A loss: 0.848590, acc: 0.234375]\n",
      "990: [D loss: 0.689275, acc: 0.566406]  [A loss: 0.813481, acc: 0.265625]\n",
      "991: [D loss: 0.701746, acc: 0.521484]  [A loss: 0.875893, acc: 0.191406]\n",
      "992: [D loss: 0.702166, acc: 0.517578]  [A loss: 0.784493, acc: 0.312500]\n",
      "993: [D loss: 0.721588, acc: 0.484375]  [A loss: 0.868812, acc: 0.195312]\n",
      "994: [D loss: 0.678467, acc: 0.546875]  [A loss: 0.769135, acc: 0.339844]\n",
      "995: [D loss: 0.710663, acc: 0.517578]  [A loss: 0.907790, acc: 0.179688]\n",
      "996: [D loss: 0.700457, acc: 0.515625]  [A loss: 0.746480, acc: 0.382812]\n",
      "997: [D loss: 0.695139, acc: 0.544922]  [A loss: 0.897165, acc: 0.199219]\n",
      "998: [D loss: 0.690312, acc: 0.544922]  [A loss: 0.793703, acc: 0.332031]\n",
      "999: [D loss: 0.695564, acc: 0.537109]  [A loss: 0.807372, acc: 0.312500]\n"
     ]
    }
   ],
   "source": [
    "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
