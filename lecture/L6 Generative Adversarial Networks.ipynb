{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "We have so far exclusively treated neural networks as classifiers and regressors, but neglected the high dimensional information that the network is creating. Convolution reduces spatial information (2D images or 1D time series) to components by means of filters. The filters have usually been trained based on a large number of examples, describing features that these examples have in common. The result of the convolutional layers is then a representation that describes how much of each feature has been in the original image. This process could also be used the other way round to generate images using \"deconvolution\". Instead of reducing high-dimensional information such as an image to low-dimensional one, such as a distribution over classes, such networks could be generate high-dimensional images from a few numbers that define the desired content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling and \"Deconvolving\"\n",
    "The label \"deconvolution\" is unfortunately confusing here, however, as it has established <a href=\"https://en.wikipedia.org/wiki/Deconvolution\">mathematical meaning</a>, which is usually not implemented in a neural network context. Rather, we can achieve the desired effect by first upsampling the input and performing a convolution then. In this example, a simple (2,2) image will be turned into a (4,4) image. What exactly happens is random, as the initial weights are random, and running the model below a couple of times will give the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[-0.19158459  0.3958907  -0.38316917 -1.5581198 ]\n",
      " [-0.9374951  -0.3500198  -1.1290797  -2.5667803 ]\n",
      " [-0.57475376  0.01272154 -0.76633835 -3.1162395 ]\n",
      " [ 0.5441121   1.0002123   0.7254827  -1.0989184 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15cce17f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code inspired by https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import UpSampling2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define input data\n",
    "X = asarray([[1, 2],\n",
    "            [3, 4]])\n",
    "# show input data for context\n",
    "print(X)\n",
    "# reshape input data into one sample a sample with a channel\n",
    "X = X.reshape((1, 2, 2, 1))\n",
    "model = Sequential()\n",
    "model.add(UpSampling2D(input_shape=(2, 2, 1)))\n",
    "model.add(Conv2D(1, (2,2), padding='same'))\n",
    "#model.summary()\n",
    "yhat = model.predict(X)\n",
    "# reshape output to remove channel to make printing easier\n",
    "yhat = yhat.reshape((4, 4))\n",
    "# summarize output\n",
    "print(yhat)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X.reshape((2,2)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a function to combine convolution and upsampling <code>Conv2DTranspose</code> that pads new rows and columns during upsampling with zeros. This can be seen when using (1,1) as a convolution kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[ 0.3130663   0.81237286  0.6261326   1.6247457 ]\n",
      " [-0.43699604  0.4756996  -0.8739921   0.9513992 ]\n",
      " [ 0.9391989   2.4371185   1.2522652   3.2494915 ]\n",
      " [-1.3109882   1.4270988  -1.7479842   1.9027984 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15d8d9128>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code inspired by https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/\n",
    "\n",
    "# example of using the transpose convolutional layer\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2DTranspose\n",
    "# define input data\n",
    "X = asarray([[1, 2],\n",
    "            [3, 4]])\n",
    "# show input data for context\n",
    "print(X)\n",
    "# reshape input data into one sample a sample with a channel\n",
    "X = X.reshape((1, 2, 2, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "# Use different kernels (1,1), (2,2) etc to see how Conv2DTranspose operates\n",
    "model.add(Conv2DTranspose(1, (2,2), strides=(2,2), input_shape=(2, 2, 1)))\n",
    "#model.summary()\n",
    "# make a prediction with the model\n",
    "yhat = model.predict(X)\n",
    "# reshape output to remove channel to make printing easier\n",
    "yhat = yhat.reshape((4, 4))\n",
    "# summarize output\n",
    "print(yhat)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X.reshape((2,2)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the right kernels to get meaningful output\n",
    "\n",
    "Lets assume a training set consists of single pixel images, which color is drawn from a 3D normal distribution centered around the color orange. The \"generator\" G() does not know this distribution, but turns uniform random input drawn from a distribution z into other random colors. The \"discriminator\" has learned the original distribution and can say \"hot\" (yes) or \"cold\" (no) and anything in between. The network can now use this feedback to improve the generator to better match the true distribution. Once gradient information is available, the generator can change its weights to get closer and closer to the desired distribution. This is known as backpropagation. \n",
    "\n",
    "At the same time, we can use knowledge of the fact that generated images are fake to improve the discriminator. Instead of training the discriminator with samples of shades of orange and totally random colors, we are training it with whatever the generator currently produces as examples of \"fake\" data. This will let the discriminator become more subtle over time.\n",
    "\n",
    "Training both generator and discriminator sounds like a chicken-egg problem, but is usually bootstrapped by implementing a generating network that is believed to have a sufficiently complex structure to create a variety of output that we are interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "A generator is using upsampling and convolution operations to turn a seed of noise into an image. Initially, the generated images are just noise, but will eventually show similar distributions as the training set. The generator itself is never trained as a stand-alone network, which is indicated by all parameters colored in light gray. Once this network is trained, the generator gets better and better, here showing the output after zero and 2000 training iterations, which will be explained further below. \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_generator.svg\" width=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "We use a discriminator to decide whether an image is following the desired distribution, i.e. like the training set, or a generated image. We can train this discriminator by using two batches of equal size, one of which contains training images, the other generated images. The generated images are labeled by a zero, the training images by a one.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_discriminator.svg\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "As the generator gets better, the discriminator will get presented with better and better generated images, becoming more and more sophisticated. Here, the training set at 0 and after 2000 iterations of training the generator are shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Generator and Discriminator into an Adversial Network\n",
    "A GAN consists of a generator and a discriminator network that are connected in series. When training the GAN, the generator pretends that images it generates are real images, which leads to a loss if the discriminator detects the fake image. In order to prevent this loss from backpropagating into the discriminator, all discriminator parameters are locked during training, which is indicated by all parameters of the discriminator shown in light gray.  \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/GAN_adversarial.svg\" width=\"75%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GAN\n",
    "\n",
    "The implementation below has been inspired by <a href=\"https://github.com/roatienza/Deep-Learning-Experiments\">Rowel Atienza's code</a>, and adapted to Keras 2.0. The architecture below follows the guidelines from <a href=\"https://arxiv.org/pdf/1511.06434.pdf%C3%AF%C2%BC%E2%80%B0\">UNSUPERVISED REPRESENTATION LEARNING\n",
    "WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</a>\n",
    "\n",
    "- Architecture guidelines for stable Deep Convolutional GANs\n",
    "- Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
    "- Use batchnorm in both the generator and the discriminator.\n",
    "- Remove fully connected hidden layers for deeper architectures.\n",
    "- Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "- Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential()\n",
    "depth = 64\n",
    "dropout = 0.4\n",
    "# In: 28 x 28 x 1, depth = 1\n",
    "# Out: 14 x 14 x 1, depth=64\n",
    "input_shape = (28, 28, 1)\n",
    "discriminator.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "discriminator.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(dropout))\n",
    "\n",
    "# Out: 1-dim probability\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1))\n",
    "discriminator.add(Activation('sigmoid'))\n",
    "discriminator.summary()\n",
    "\n",
    "optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Sequential()\n",
    "dropout = 0.4\n",
    "depth = 64+64+64+64\n",
    "dim = 7\n",
    "# In: 100\n",
    "# Out: dim x dim x depth\n",
    "generator.add(Dense(dim*dim*depth, input_dim=100))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(Reshape((dim, dim, depth)))\n",
    "generator.add(Dropout(dropout))\n",
    "\n",
    "# In: dim x dim x depth\n",
    "# Out: 2*dim x 2*dim x depth/2\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "generator.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.9))\n",
    "generator.add(Activation('relu'))\n",
    "\n",
    "# Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "generator.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "generator.add(Activation('sigmoid'))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 28, 28, 1)         2394241   \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 4311553   \n",
      "=================================================================\n",
      "Total params: 6,705,794\n",
      "Trainable params: 6,680,258\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "adversarial = Sequential()\n",
    "adversarial.add(generator)\n",
    "adversarial.add(discriminator)\n",
    "adversarial.summary()\n",
    "adversarial.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, Y_train), (x_test, Y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(10000, 28,28,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalize\n",
    "#\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(Y_train, 10)\n",
    "y_test = np_utils.to_categorical(Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.690452, acc: 0.601562]  [A loss: 0.826891, acc: 0.000000]\n",
      "1: [D loss: 0.621099, acc: 0.689453]  [A loss: 1.077610, acc: 0.000000]\n",
      "2: [D loss: 0.496320, acc: 0.761719]  [A loss: 0.886917, acc: 0.019531]\n",
      "3: [D loss: 0.321745, acc: 0.992188]  [A loss: 0.499974, acc: 0.917969]\n",
      "4: [D loss: 0.155056, acc: 1.000000]  [A loss: 0.228547, acc: 0.988281]\n",
      "5: [D loss: 0.081347, acc: 0.992188]  [A loss: 0.000018, acc: 1.000000]\n",
      "6: [D loss: 0.442354, acc: 0.541016]  [A loss: 1.266462, acc: 0.238281]\n",
      "7: [D loss: 0.142876, acc: 0.962891]  [A loss: 0.000003, acc: 1.000000]\n",
      "8: [D loss: 0.313033, acc: 0.894531]  [A loss: 0.270790, acc: 0.937500]\n",
      "9: [D loss: 0.101248, acc: 0.974609]  [A loss: 0.000000, acc: 1.000000]\n",
      "10: [D loss: 0.266075, acc: 0.986328]  [A loss: 0.060513, acc: 1.000000]\n",
      "11: [D loss: 0.066087, acc: 0.984375]  [A loss: 0.000000, acc: 1.000000]\n",
      "12: [D loss: 0.188892, acc: 1.000000]  [A loss: 0.017347, acc: 1.000000]\n",
      "13: [D loss: 0.067840, acc: 0.986328]  [A loss: 0.000000, acc: 1.000000]\n",
      "14: [D loss: 0.167386, acc: 1.000000]  [A loss: 0.004706, acc: 1.000000]\n",
      "15: [D loss: 0.032144, acc: 0.996094]  [A loss: 0.000000, acc: 1.000000]\n",
      "16: [D loss: 0.100391, acc: 1.000000]  [A loss: 0.000400, acc: 1.000000]\n",
      "17: [D loss: 0.057089, acc: 0.992188]  [A loss: 0.000000, acc: 1.000000]\n",
      "18: [D loss: 0.089070, acc: 1.000000]  [A loss: 0.000107, acc: 1.000000]\n",
      "19: [D loss: 0.021168, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "20: [D loss: 0.039409, acc: 1.000000]  [A loss: 0.000212, acc: 1.000000]\n",
      "21: [D loss: 0.031879, acc: 0.994141]  [A loss: 0.000000, acc: 1.000000]\n",
      "22: [D loss: 0.039668, acc: 1.000000]  [A loss: 0.000002, acc: 1.000000]\n",
      "23: [D loss: 0.016984, acc: 0.994141]  [A loss: 0.000000, acc: 1.000000]\n",
      "24: [D loss: 0.033528, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "25: [D loss: 0.010734, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "26: [D loss: 0.016673, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "27: [D loss: 0.011549, acc: 0.996094]  [A loss: 0.000000, acc: 1.000000]\n",
      "28: [D loss: 0.014766, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "29: [D loss: 0.005573, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "30: [D loss: 0.011002, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "31: [D loss: 0.011501, acc: 0.996094]  [A loss: 0.000000, acc: 1.000000]\n",
      "32: [D loss: 0.007751, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "33: [D loss: 0.013686, acc: 0.992188]  [A loss: 0.000000, acc: 1.000000]\n",
      "34: [D loss: 0.010831, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "35: [D loss: 0.016572, acc: 0.994141]  [A loss: 0.000000, acc: 1.000000]\n",
      "36: [D loss: 0.016075, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "37: [D loss: 0.035874, acc: 0.986328]  [A loss: 0.000000, acc: 1.000000]\n",
      "38: [D loss: 0.035804, acc: 0.998047]  [A loss: 0.000033, acc: 1.000000]\n",
      "39: [D loss: 0.056055, acc: 0.972656]  [A loss: 0.000000, acc: 1.000000]\n",
      "40: [D loss: 0.180461, acc: 0.923828]  [A loss: 0.000003, acc: 1.000000]\n",
      "41: [D loss: 0.033752, acc: 0.986328]  [A loss: 0.000000, acc: 1.000000]\n",
      "42: [D loss: 0.419016, acc: 0.822266]  [A loss: 0.000052, acc: 1.000000]\n",
      "43: [D loss: 0.064530, acc: 0.976562]  [A loss: 0.000000, acc: 1.000000]\n",
      "44: [D loss: 1.286767, acc: 0.626953]  [A loss: 0.000209, acc: 1.000000]\n",
      "45: [D loss: 0.101543, acc: 0.960938]  [A loss: 0.000000, acc: 1.000000]\n",
      "46: [D loss: 2.156466, acc: 0.556641]  [A loss: 0.008652, acc: 0.996094]\n",
      "47: [D loss: 0.252114, acc: 0.896484]  [A loss: 0.000000, acc: 1.000000]\n",
      "48: [D loss: 2.944007, acc: 0.525391]  [A loss: 1.892028, acc: 0.457031]\n",
      "49: [D loss: 0.674717, acc: 0.732422]  [A loss: 0.000000, acc: 1.000000]\n",
      "50: [D loss: 6.488783, acc: 0.500000]  [A loss: 0.795363, acc: 0.703125]\n",
      "51: [D loss: 0.523435, acc: 0.781250]  [A loss: 0.000000, acc: 1.000000]\n",
      "52: [D loss: 9.471931, acc: 0.500000]  [A loss: 0.580433, acc: 0.757812]\n",
      "53: [D loss: 0.352795, acc: 0.847656]  [A loss: 0.000000, acc: 1.000000]\n",
      "54: [D loss: 7.723857, acc: 0.500000]  [A loss: 0.484977, acc: 0.773438]\n",
      "55: [D loss: 0.314424, acc: 0.853516]  [A loss: 1.997771, acc: 0.375000]\n",
      "56: [D loss: 0.154866, acc: 0.933594]  [A loss: 0.000000, acc: 1.000000]\n",
      "57: [D loss: 7.933053, acc: 0.500000]  [A loss: 0.957496, acc: 0.605469]\n",
      "58: [D loss: 0.098504, acc: 0.982422]  [A loss: 0.000000, acc: 1.000000]\n",
      "59: [D loss: 7.581344, acc: 0.500000]  [A loss: 5.057250, acc: 0.035156]\n",
      "60: [D loss: 0.192435, acc: 0.925781]  [A loss: 0.000000, acc: 1.000000]\n",
      "61: [D loss: 6.801283, acc: 0.500000]  [A loss: 3.813290, acc: 0.062500]\n",
      "62: [D loss: 0.123083, acc: 0.962891]  [A loss: 0.000000, acc: 1.000000]\n",
      "63: [D loss: 6.889018, acc: 0.500000]  [A loss: 3.128296, acc: 0.074219]\n",
      "64: [D loss: 0.104106, acc: 0.972656]  [A loss: 0.000001, acc: 1.000000]\n",
      "65: [D loss: 6.077152, acc: 0.500000]  [A loss: 2.480580, acc: 0.117188]\n",
      "66: [D loss: 0.072074, acc: 0.990234]  [A loss: 0.000001, acc: 1.000000]\n",
      "67: [D loss: 5.882027, acc: 0.500000]  [A loss: 2.284632, acc: 0.125000]\n",
      "68: [D loss: 0.056928, acc: 0.996094]  [A loss: 0.000002, acc: 1.000000]\n",
      "69: [D loss: 4.974988, acc: 0.500000]  [A loss: 2.359481, acc: 0.117188]\n",
      "70: [D loss: 0.062603, acc: 0.996094]  [A loss: 0.000004, acc: 1.000000]\n",
      "71: [D loss: 4.616942, acc: 0.500000]  [A loss: 2.629089, acc: 0.078125]\n",
      "72: [D loss: 0.060086, acc: 0.998047]  [A loss: 0.000010, acc: 1.000000]\n",
      "73: [D loss: 3.920311, acc: 0.500000]  [A loss: 2.649379, acc: 0.074219]\n",
      "74: [D loss: 0.066328, acc: 0.992188]  [A loss: 0.000032, acc: 1.000000]\n",
      "75: [D loss: 3.171740, acc: 0.500000]  [A loss: 4.565923, acc: 0.007812]\n",
      "76: [D loss: 0.098657, acc: 0.988281]  [A loss: 0.000090, acc: 1.000000]\n",
      "77: [D loss: 2.036896, acc: 0.500000]  [A loss: 4.295989, acc: 0.019531]\n",
      "78: [D loss: 0.140746, acc: 0.990234]  [A loss: 0.000149, acc: 1.000000]\n",
      "79: [D loss: 1.076190, acc: 0.507812]  [A loss: 4.578475, acc: 0.015625]\n",
      "80: [D loss: 0.159125, acc: 0.980469]  [A loss: 0.000160, acc: 1.000000]\n",
      "81: [D loss: 1.336917, acc: 0.500000]  [A loss: 3.856180, acc: 0.042969]\n",
      "82: [D loss: 0.165460, acc: 0.970703]  [A loss: 0.000270, acc: 1.000000]\n",
      "83: [D loss: 0.951867, acc: 0.509766]  [A loss: 3.628810, acc: 0.050781]\n",
      "84: [D loss: 0.142643, acc: 0.982422]  [A loss: 0.000881, acc: 1.000000]\n",
      "85: [D loss: 0.932195, acc: 0.515625]  [A loss: 3.689966, acc: 0.050781]\n",
      "86: [D loss: 0.128729, acc: 0.982422]  [A loss: 0.001433, acc: 1.000000]\n",
      "87: [D loss: 0.818410, acc: 0.535156]  [A loss: 3.229122, acc: 0.105469]\n",
      "88: [D loss: 0.110954, acc: 0.986328]  [A loss: 0.002030, acc: 1.000000]\n",
      "89: [D loss: 1.308803, acc: 0.507812]  [A loss: 2.970454, acc: 0.109375]\n",
      "90: [D loss: 0.111792, acc: 0.988281]  [A loss: 0.001669, acc: 1.000000]\n",
      "91: [D loss: 2.203074, acc: 0.500000]  [A loss: 3.418059, acc: 0.062500]\n",
      "92: [D loss: 0.103828, acc: 0.988281]  [A loss: 0.002447, acc: 1.000000]\n",
      "93: [D loss: 2.139535, acc: 0.500000]  [A loss: 3.160659, acc: 0.105469]\n",
      "94: [D loss: 0.099383, acc: 0.994141]  [A loss: 0.003506, acc: 1.000000]\n",
      "95: [D loss: 1.847648, acc: 0.500000]  [A loss: 2.939614, acc: 0.089844]\n",
      "96: [D loss: 0.090853, acc: 0.994141]  [A loss: 0.005198, acc: 1.000000]\n",
      "97: [D loss: 1.913531, acc: 0.500000]  [A loss: 3.234680, acc: 0.062500]\n",
      "98: [D loss: 0.101817, acc: 0.982422]  [A loss: 0.003285, acc: 1.000000]\n",
      "99: [D loss: 1.926270, acc: 0.503906]  [A loss: 3.196251, acc: 0.070312]\n",
      "100: [D loss: 0.089607, acc: 0.990234]  [A loss: 0.006035, acc: 1.000000]\n",
      "101: [D loss: 1.736390, acc: 0.505859]  [A loss: 3.091022, acc: 0.070312]\n",
      "102: [D loss: 0.101237, acc: 0.978516]  [A loss: 0.005570, acc: 1.000000]\n",
      "103: [D loss: 1.639165, acc: 0.507812]  [A loss: 2.984463, acc: 0.066406]\n",
      "104: [D loss: 0.085823, acc: 0.992188]  [A loss: 0.006790, acc: 1.000000]\n",
      "105: [D loss: 1.503496, acc: 0.507812]  [A loss: 3.011649, acc: 0.062500]\n",
      "106: [D loss: 0.098589, acc: 0.974609]  [A loss: 0.008378, acc: 1.000000]\n",
      "107: [D loss: 1.356409, acc: 0.529297]  [A loss: 2.867087, acc: 0.089844]\n",
      "108: [D loss: 0.085680, acc: 0.990234]  [A loss: 0.008229, acc: 1.000000]\n",
      "109: [D loss: 1.201134, acc: 0.562500]  [A loss: 2.967800, acc: 0.070312]\n",
      "110: [D loss: 0.079794, acc: 0.992188]  [A loss: 0.012504, acc: 1.000000]\n",
      "111: [D loss: 1.119462, acc: 0.535156]  [A loss: 2.898712, acc: 0.078125]\n",
      "112: [D loss: 0.085369, acc: 0.992188]  [A loss: 0.011987, acc: 1.000000]\n",
      "113: [D loss: 1.060954, acc: 0.556641]  [A loss: 3.006339, acc: 0.070312]\n",
      "114: [D loss: 0.096805, acc: 0.982422]  [A loss: 0.016286, acc: 1.000000]\n",
      "115: [D loss: 0.958721, acc: 0.560547]  [A loss: 2.887978, acc: 0.078125]\n",
      "116: [D loss: 0.092215, acc: 0.984375]  [A loss: 0.019576, acc: 0.996094]\n",
      "117: [D loss: 0.867181, acc: 0.585938]  [A loss: 2.738385, acc: 0.085938]\n",
      "118: [D loss: 0.082279, acc: 0.996094]  [A loss: 0.026531, acc: 1.000000]\n",
      "119: [D loss: 0.761755, acc: 0.611328]  [A loss: 2.705613, acc: 0.085938]\n",
      "120: [D loss: 0.080368, acc: 0.990234]  [A loss: 0.032360, acc: 1.000000]\n",
      "121: [D loss: 0.669168, acc: 0.638672]  [A loss: 2.753386, acc: 0.109375]\n",
      "122: [D loss: 0.071247, acc: 0.998047]  [A loss: 0.039202, acc: 0.996094]\n",
      "123: [D loss: 0.588772, acc: 0.675781]  [A loss: 2.845310, acc: 0.089844]\n",
      "124: [D loss: 0.063616, acc: 0.996094]  [A loss: 0.035887, acc: 0.996094]\n",
      "125: [D loss: 0.512853, acc: 0.707031]  [A loss: 2.946119, acc: 0.101562]\n",
      "126: [D loss: 0.061395, acc: 0.996094]  [A loss: 0.039031, acc: 1.000000]\n",
      "127: [D loss: 0.499748, acc: 0.726562]  [A loss: 2.912222, acc: 0.105469]\n",
      "128: [D loss: 0.053901, acc: 0.998047]  [A loss: 0.055223, acc: 0.988281]\n",
      "129: [D loss: 0.423376, acc: 0.767578]  [A loss: 2.867585, acc: 0.128906]\n",
      "130: [D loss: 0.049735, acc: 1.000000]  [A loss: 0.060926, acc: 0.992188]\n",
      "131: [D loss: 0.334179, acc: 0.837891]  [A loss: 3.001547, acc: 0.109375]\n",
      "132: [D loss: 0.047039, acc: 0.998047]  [A loss: 0.062621, acc: 0.976562]\n",
      "133: [D loss: 0.285023, acc: 0.849609]  [A loss: 3.107871, acc: 0.109375]\n",
      "134: [D loss: 0.041723, acc: 0.998047]  [A loss: 0.069918, acc: 0.988281]\n",
      "135: [D loss: 0.248006, acc: 0.884766]  [A loss: 3.280462, acc: 0.109375]\n",
      "136: [D loss: 0.031699, acc: 0.998047]  [A loss: 0.077401, acc: 0.976562]\n",
      "137: [D loss: 0.199933, acc: 0.900391]  [A loss: 3.341032, acc: 0.082031]\n",
      "138: [D loss: 0.034091, acc: 1.000000]  [A loss: 0.097093, acc: 0.957031]\n",
      "139: [D loss: 0.155809, acc: 0.933594]  [A loss: 3.459124, acc: 0.101562]\n",
      "140: [D loss: 0.028508, acc: 1.000000]  [A loss: 0.076341, acc: 0.980469]\n",
      "141: [D loss: 0.127658, acc: 0.962891]  [A loss: 3.577816, acc: 0.082031]\n",
      "142: [D loss: 0.025388, acc: 1.000000]  [A loss: 0.098040, acc: 0.968750]\n",
      "143: [D loss: 0.103988, acc: 0.974609]  [A loss: 3.693886, acc: 0.105469]\n",
      "144: [D loss: 0.020221, acc: 0.996094]  [A loss: 0.097921, acc: 0.976562]\n",
      "145: [D loss: 0.062864, acc: 0.988281]  [A loss: 4.071709, acc: 0.078125]\n",
      "146: [D loss: 0.021579, acc: 0.996094]  [A loss: 0.113111, acc: 0.957031]\n",
      "147: [D loss: 0.048124, acc: 0.988281]  [A loss: 4.173351, acc: 0.082031]\n",
      "148: [D loss: 0.016182, acc: 1.000000]  [A loss: 0.127063, acc: 0.941406]\n",
      "149: [D loss: 0.040799, acc: 0.998047]  [A loss: 4.261703, acc: 0.085938]\n",
      "150: [D loss: 0.015367, acc: 1.000000]  [A loss: 0.144536, acc: 0.957031]\n",
      "151: [D loss: 0.027018, acc: 0.998047]  [A loss: 4.582593, acc: 0.050781]\n",
      "152: [D loss: 0.012695, acc: 1.000000]  [A loss: 0.169148, acc: 0.933594]\n",
      "153: [D loss: 0.022597, acc: 1.000000]  [A loss: 4.659174, acc: 0.054688]\n",
      "154: [D loss: 0.014321, acc: 0.998047]  [A loss: 0.204031, acc: 0.906250]\n",
      "155: [D loss: 0.016305, acc: 0.998047]  [A loss: 4.919548, acc: 0.050781]\n",
      "156: [D loss: 0.010110, acc: 1.000000]  [A loss: 0.227757, acc: 0.902344]\n",
      "157: [D loss: 0.008413, acc: 1.000000]  [A loss: 5.230143, acc: 0.050781]\n",
      "158: [D loss: 0.007430, acc: 1.000000]  [A loss: 0.332815, acc: 0.875000]\n",
      "159: [D loss: 0.002565, acc: 1.000000]  [A loss: 6.061223, acc: 0.039062]\n",
      "160: [D loss: 0.019304, acc: 1.000000]  [A loss: 0.392175, acc: 0.816406]\n",
      "161: [D loss: 0.002828, acc: 1.000000]  [A loss: 6.034304, acc: 0.039062]\n",
      "162: [D loss: 0.007861, acc: 1.000000]  [A loss: 0.420499, acc: 0.808594]\n",
      "163: [D loss: 0.002287, acc: 1.000000]  [A loss: 6.034970, acc: 0.042969]\n",
      "164: [D loss: 0.006497, acc: 1.000000]  [A loss: 0.471571, acc: 0.765625]\n",
      "165: [D loss: 0.001423, acc: 1.000000]  [A loss: 6.125994, acc: 0.054688]\n",
      "166: [D loss: 0.003244, acc: 1.000000]  [A loss: 0.463106, acc: 0.781250]\n",
      "167: [D loss: 0.001038, acc: 1.000000]  [A loss: 6.633300, acc: 0.042969]\n",
      "168: [D loss: 0.003741, acc: 0.998047]  [A loss: 1.133360, acc: 0.570312]\n",
      "169: [D loss: 0.000276, acc: 1.000000]  [A loss: 7.454979, acc: 0.015625]\n",
      "170: [D loss: 0.012980, acc: 0.998047]  [A loss: 0.911131, acc: 0.578125]\n",
      "171: [D loss: 0.000398, acc: 1.000000]  [A loss: 7.361412, acc: 0.027344]\n",
      "172: [D loss: 0.008176, acc: 0.998047]  [A loss: 1.144461, acc: 0.527344]\n",
      "173: [D loss: 0.000313, acc: 1.000000]  [A loss: 7.999195, acc: 0.023438]\n",
      "174: [D loss: 0.004247, acc: 1.000000]  [A loss: 1.217095, acc: 0.503906]\n",
      "175: [D loss: 0.000144, acc: 1.000000]  [A loss: 7.002911, acc: 0.042969]\n",
      "176: [D loss: 0.001487, acc: 1.000000]  [A loss: 1.157619, acc: 0.500000]\n",
      "177: [D loss: 0.000095, acc: 1.000000]  [A loss: 7.067374, acc: 0.054688]\n",
      "178: [D loss: 0.000466, acc: 1.000000]  [A loss: 1.103474, acc: 0.566406]\n",
      "179: [D loss: 0.000088, acc: 1.000000]  [A loss: 6.664607, acc: 0.058594]\n",
      "180: [D loss: 0.000193, acc: 1.000000]  [A loss: 1.790141, acc: 0.371094]\n",
      "181: [D loss: 0.000035, acc: 1.000000]  [A loss: 6.111037, acc: 0.074219]\n",
      "182: [D loss: 0.000142, acc: 1.000000]  [A loss: 1.374305, acc: 0.437500]\n",
      "183: [D loss: 0.000051, acc: 1.000000]  [A loss: 6.123498, acc: 0.070312]\n",
      "184: [D loss: 0.000052, acc: 1.000000]  [A loss: 2.100417, acc: 0.320312]\n",
      "185: [D loss: 0.000006, acc: 1.000000]  [A loss: 4.385512, acc: 0.167969]\n",
      "186: [D loss: 0.000006, acc: 1.000000]  [A loss: 4.612042, acc: 0.097656]\n",
      "187: [D loss: 0.000005, acc: 1.000000]  [A loss: 4.479396, acc: 0.074219]\n",
      "188: [D loss: 0.000001, acc: 1.000000]  [A loss: 5.100787, acc: 0.109375]\n",
      "189: [D loss: 0.000001, acc: 1.000000]  [A loss: 5.973586, acc: 0.046875]\n",
      "190: [D loss: 0.000002, acc: 1.000000]  [A loss: 5.024084, acc: 0.078125]\n",
      "191: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.806228, acc: 0.246094]\n",
      "192: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.672353, acc: 0.328125]\n",
      "193: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.798544, acc: 0.328125]\n",
      "194: [D loss: 0.000000, acc: 1.000000]  [A loss: 4.244873, acc: 0.285156]\n",
      "195: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.365017, acc: 0.394531]\n",
      "196: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.209593, acc: 0.445312]\n",
      "197: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.465729, acc: 0.390625]\n",
      "198: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.378182, acc: 0.425781]\n",
      "199: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.560061, acc: 0.402344]\n",
      "200: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.310472, acc: 0.433594]\n",
      "201: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.295827, acc: 0.417969]\n",
      "202: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.198122, acc: 0.433594]\n",
      "203: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.959518, acc: 0.519531]\n",
      "204: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.229562, acc: 0.449219]\n",
      "205: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.022704, acc: 0.511719]\n",
      "206: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.400331, acc: 0.410156]\n",
      "207: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.291605, acc: 0.519531]\n",
      "208: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.605535, acc: 0.421875]\n",
      "209: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.790667, acc: 0.417969]\n",
      "210: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.856771, acc: 0.382812]\n",
      "211: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.634478, acc: 0.453125]\n",
      "212: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.430793, acc: 0.425781]\n",
      "213: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.104452, acc: 0.542969]\n",
      "214: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.133693, acc: 0.449219]\n",
      "215: [D loss: 0.000000, acc: 1.000000]  [A loss: 3.108331, acc: 0.566406]\n",
      "216: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.780880, acc: 0.511719]\n",
      "217: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.790476, acc: 0.558594]\n",
      "218: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.614069, acc: 0.546875]\n",
      "219: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.707785, acc: 0.601562]\n",
      "220: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.656432, acc: 0.535156]\n",
      "221: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.423086, acc: 0.621094]\n",
      "222: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.538307, acc: 0.566406]\n",
      "223: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.492311, acc: 0.621094]\n",
      "224: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.492031, acc: 0.523438]\n",
      "225: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.366159, acc: 0.628906]\n",
      "226: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.344213, acc: 0.605469]\n",
      "227: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.434909, acc: 0.625000]\n",
      "228: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.317301, acc: 0.582031]\n",
      "229: [D loss: 0.000000, acc: 1.000000]  [A loss: 2.376590, acc: 0.656250]\n"
     ]
    }
   ],
   "source": [
    "train_steps=1000\n",
    "batch_size=256\n",
    "save_interval=100\n",
    "\n",
    "def plot_images(save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "    filename = 'mnist.png'\n",
    "    if fake:\n",
    "        if noise is None:\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "        else:\n",
    "            filename = \"mnist_%d.png\" % step\n",
    "        images = generator.predict(noise)\n",
    "    else:\n",
    "        i = np.random.randint(0, x_train.shape[0], samples)\n",
    "        images = x_train[i, :, :, :]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [28,28])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save2file:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "noise_input = None\n",
    "if save_interval>0:\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "    \n",
    "a_loss = np.zeros((train_steps,2))\n",
    "d_loss = np.zeros((train_steps,2))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    images_train = x_train[np.random.randint(0,x_train.shape[0], size=batch_size), :, :, :]\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "    images_fake = generator.predict(noise)\n",
    "    x = np.concatenate((images_train, images_fake))\n",
    "    y = np.ones([2*batch_size, 1])\n",
    "    y[batch_size:, :] = 0\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    d_loss[i] = discriminator.train_on_batch(x, y)\n",
    "    discriminator.trainable = False\n",
    "    adversarial.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    y = np.ones([batch_size, 1])\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "    a_loss[i] = adversarial.train_on_batch(noise, y)\n",
    "    log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[i,0], d_loss[i,1])\n",
    "    log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[i,0], a_loss[i,1])\n",
    "    print(log_mesg)\n",
    "    if save_interval>0:\n",
    "        if (i+1)%save_interval==0:\n",
    "            plot_images(save2file=True, samples=noise_input.shape[0],noise=noise_input, step=(i+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14dc79fd0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title('Discriminator network')\n",
    "plt.plot(d_loss[:,:])\n",
    "plt.legend(discriminator.metrics_names)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Adversarial network')\n",
    "plt.plot(a_loss[:,:])\n",
    "plt.legend(adversarial.metrics_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    # (WF+2P)/S+1\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "        # In: 28 x 28 x 1, depth = 1\n",
    "        # Out: 14 x 14 x 1, depth=64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.4\n",
    "        depth = 64+64+64+64\n",
    "        dim = 7\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        (x_train, Y_train), (x_test, Y_test) = mnist.load_data()\n",
    "        x_train = x_train.reshape(60000,28,28,1)\n",
    "        x_test = x_test.reshape(10000, 28,28,1)\n",
    "        self.x_train = x_train.astype('float32')\n",
    "        self.x_test = x_test.astype('float32')\n",
    "        # normalize\n",
    "        #\n",
    "        self.x_train /= 255\n",
    "        self.x_test /= 255\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = np_utils.to_categorical(Y_train, 10)\n",
    "        self.y_test = np_utils.to_categorical(Y_test, 10)\n",
    "        \n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "            self.discriminator.trainable = False\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mnist_dcgan = MNIST_DCGAN()\n",
    "    timer = ElapsedTimer()\n",
    "    mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=50)\n",
    "    timer.elapsed_time()\n",
    "    mnist_dcgan.plot_images(fake=True)\n",
    "    mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/GAN_mnist.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First trial, without locking discriminator. Second trial with locking discriminator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAN architecture not set in stone. Key idea: use \"discriminator\" to generate self-supervised training examples and use backpropagation to train the \"generator\" to trick the discriminator. \n",
    "\n",
    "- Other example: derive generator structure to generate camouflage patterns. Use detector to detect object. Backpropagation will change generator to minimize detection.\n",
    "- Games: generator to generate move, discriminator to decide whether move was good\n",
    "- Fine balance between improving generator and discriminator is needed and the approach does not necessarily need to succeed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
