{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and training multi-layer artificial neural networks in Keras\n",
    "\n",
    "Fortunately, the past decade has resulted in a large number of tools to efficiently work with very large, so-called deep neural networks. Among the more prominent tools are <a href=\"https://en.wikipedia.org/wiki/Theano_(software)\">theano</a> and <a href=\"https://en.wikipedia.org/wiki/TensorFlow\">Tensorflow</a>, from the University of Montreal and Google Inc., respectively, and the <a href=\"https://en.wikipedia.org/wiki/Keras\">Keras</a> library to conveniently interface with them. We will be using Keras in this class to study the most important deep neural network architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a series of standard models, the simplest being a sequential model that allows the user to stack layers of artifical neurons together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2,input_dim=3,kernel_initializer='random_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example creates a sequential neural network with one \"dense\" layer with 2 artificial neurons, input dimension of 3, and uniformly random initial weights. Keras offers the following options <code>random_uniform</code>, <code>random_normal</code>, and <code>zero</code>, which are self-explanatory. Using a <i>dense</i> layer means that all inputs to a layer are connected to every neuron, and all outputs of a layer connect to all neurons of the next layer. The multi-layer perceptron shown above is a dense network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a first ANN\n",
    "\n",
    "Example from Antonio Gulli, “Deep Learning with Keras.” The MNIST data set is a collection of 70000 hand-written digits from 0 to 9 that are provided in a 28x28 matrix. The MNIST data set is part of the ```keras.datasets``` <a href=\"https://keras.io/datasets/\">library</a>. We will further import a simple ```Sequential``` model, creating a ```Dense``` model with custom ```Activation```, the ```SGD``` optimizer, which will be discussed later, and ```np_utils``` for One-Hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "The following script will load the MNIST dataset (directly from the keras.datasets library) and turn the 28x28 training images into a series of one-dimensional vectors of length 784. We will also normalize the data to fall into the range from 0 to 1, and turn the class labels into one-hot encoded categorical labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# network and training\n",
    "\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "#\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the dataset using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for I in range(4):\n",
    "    for J in range(4):\n",
    "        plt.subplot(4,4,I*4+J+1)\n",
    "        plt.imshow(X_train[I*4+J,:].reshape(28,28),cmap='gray')\n",
    "        plt.axis('off')\n",
    "#plt.tight_layout()        \n",
    "plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "In a first step, we define a neural network with two layers: 10 neurons with 784 inputs and a softmax activation layer. As expected, this network has 7850 parameters, 10 times 784 for the weights and 10 for the bias values.\n",
    "\n",
    "In keras, a neural network model consists both of the architecture as well as a choice of an objective function, an optimizer, and a metrics. Note, that we are using <code>categorical_crossentropy</code>, which is more suited than <code>mean_squared_error</code> when calculating hte error between model prediction and training data. We are using stochastic gradient descent (SGD) as an optimizer, which is using data to estimate gradients vs. explicitely calculating the gradient as we have done before. The \"metrics\" parameter is similar to the objective function (\"loss\"), but is used when evaluating the model, not during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(784,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "After defining the model, the optimizer, the loss function and the metric for testing, training the model is a simple call to keras' <code>fit()</code> function. We are presenting the network with 200 instances of the entire dataset (\"epochs\") and perform a an update of the weights every 128 training samples (\"batch size\"). Setting <code>verbose</code> to 1 lets us see the progress, and using 0.2 (20%) of the dataset as <code>validation_split</code> lets us observe not only the model fit, but already how the model would perform on unknown data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 2.5532 - categorical_accuracy: 0.1022 - val_loss: 2.4359 - val_categorical_accuracy: 0.1120\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.5254 - categorical_accuracy: 0.1049 - val_loss: 2.4191 - val_categorical_accuracy: 0.1198\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.5021 - categorical_accuracy: 0.1097 - val_loss: 2.4031 - val_categorical_accuracy: 0.1260\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.4875 - categorical_accuracy: 0.1106 - val_loss: 2.3879 - val_categorical_accuracy: 0.1330\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.4615 - categorical_accuracy: 0.1164 - val_loss: 2.3735 - val_categorical_accuracy: 0.1392\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.4407 - categorical_accuracy: 0.1200 - val_loss: 2.3596 - val_categorical_accuracy: 0.1452\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.4286 - categorical_accuracy: 0.1249 - val_loss: 2.3463 - val_categorical_accuracy: 0.1521\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.4105 - categorical_accuracy: 0.1290 - val_loss: 2.3333 - val_categorical_accuracy: 0.1586\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.3968 - categorical_accuracy: 0.1327 - val_loss: 2.3208 - val_categorical_accuracy: 0.1656\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.3832 - categorical_accuracy: 0.1343 - val_loss: 2.3087 - val_categorical_accuracy: 0.1724\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.3668 - categorical_accuracy: 0.1373 - val_loss: 2.2969 - val_categorical_accuracy: 0.1791\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.3548 - categorical_accuracy: 0.1433 - val_loss: 2.2855 - val_categorical_accuracy: 0.1860\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.3429 - categorical_accuracy: 0.1465 - val_loss: 2.2743 - val_categorical_accuracy: 0.1936\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.3324 - categorical_accuracy: 0.1488 - val_loss: 2.2634 - val_categorical_accuracy: 0.2009\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.3195 - categorical_accuracy: 0.1541 - val_loss: 2.2527 - val_categorical_accuracy: 0.2078\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.3092 - categorical_accuracy: 0.1575 - val_loss: 2.2421 - val_categorical_accuracy: 0.2138\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.2968 - categorical_accuracy: 0.1603 - val_loss: 2.2318 - val_categorical_accuracy: 0.2209\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.2882 - categorical_accuracy: 0.1659 - val_loss: 2.2217 - val_categorical_accuracy: 0.2273\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.2775 - categorical_accuracy: 0.1684 - val_loss: 2.2119 - val_categorical_accuracy: 0.2322\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.2663 - categorical_accuracy: 0.1725 - val_loss: 2.2021 - val_categorical_accuracy: 0.2377\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.2578 - categorical_accuracy: 0.1783 - val_loss: 2.1924 - val_categorical_accuracy: 0.2440\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.2447 - categorical_accuracy: 0.1845 - val_loss: 2.1829 - val_categorical_accuracy: 0.2517\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.2379 - categorical_accuracy: 0.1862 - val_loss: 2.1736 - val_categorical_accuracy: 0.2579\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.2295 - categorical_accuracy: 0.1874 - val_loss: 2.1645 - val_categorical_accuracy: 0.2641\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.2222 - categorical_accuracy: 0.1938 - val_loss: 2.1554 - val_categorical_accuracy: 0.2718\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.2107 - categorical_accuracy: 0.2001 - val_loss: 2.1464 - val_categorical_accuracy: 0.2788\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.2028 - categorical_accuracy: 0.2015 - val_loss: 2.1375 - val_categorical_accuracy: 0.2848\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.1967 - categorical_accuracy: 0.2039 - val_loss: 2.1288 - val_categorical_accuracy: 0.2907\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1876 - categorical_accuracy: 0.2104 - val_loss: 2.1201 - val_categorical_accuracy: 0.2977\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1791 - categorical_accuracy: 0.2126 - val_loss: 2.1116 - val_categorical_accuracy: 0.3047\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1697 - categorical_accuracy: 0.2178 - val_loss: 2.1032 - val_categorical_accuracy: 0.3119\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1664 - categorical_accuracy: 0.2200 - val_loss: 2.0948 - val_categorical_accuracy: 0.3173\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1590 - categorical_accuracy: 0.2248 - val_loss: 2.0865 - val_categorical_accuracy: 0.3236\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1481 - categorical_accuracy: 0.2304 - val_loss: 2.0782 - val_categorical_accuracy: 0.3293\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1405 - categorical_accuracy: 0.2316 - val_loss: 2.0701 - val_categorical_accuracy: 0.3363\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1305 - categorical_accuracy: 0.2384 - val_loss: 2.0620 - val_categorical_accuracy: 0.3423\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1272 - categorical_accuracy: 0.2388 - val_loss: 2.0541 - val_categorical_accuracy: 0.3480\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1206 - categorical_accuracy: 0.2426 - val_loss: 2.0463 - val_categorical_accuracy: 0.3563\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1147 - categorical_accuracy: 0.2450 - val_loss: 2.0386 - val_categorical_accuracy: 0.3634\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.1042 - categorical_accuracy: 0.2524 - val_loss: 2.0309 - val_categorical_accuracy: 0.3693\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0952 - categorical_accuracy: 0.2545 - val_loss: 2.0232 - val_categorical_accuracy: 0.3743\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.0921 - categorical_accuracy: 0.2588 - val_loss: 2.0156 - val_categorical_accuracy: 0.3821\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.0844 - categorical_accuracy: 0.2606 - val_loss: 2.0082 - val_categorical_accuracy: 0.3892\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0766 - categorical_accuracy: 0.2639 - val_loss: 2.0008 - val_categorical_accuracy: 0.3966\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0699 - categorical_accuracy: 0.2690 - val_loss: 1.9935 - val_categorical_accuracy: 0.4034\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.0631 - categorical_accuracy: 0.2738 - val_loss: 1.9863 - val_categorical_accuracy: 0.4109\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0569 - categorical_accuracy: 0.2750 - val_loss: 1.9791 - val_categorical_accuracy: 0.4178\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0536 - categorical_accuracy: 0.2786 - val_loss: 1.9720 - val_categorical_accuracy: 0.4245\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0492 - categorical_accuracy: 0.2796 - val_loss: 1.9650 - val_categorical_accuracy: 0.4308\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0400 - categorical_accuracy: 0.2840 - val_loss: 1.9580 - val_categorical_accuracy: 0.4382\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0311 - categorical_accuracy: 0.2899 - val_loss: 1.9511 - val_categorical_accuracy: 0.4442\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0243 - categorical_accuracy: 0.2943 - val_loss: 1.9442 - val_categorical_accuracy: 0.4507\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0218 - categorical_accuracy: 0.2946 - val_loss: 1.9374 - val_categorical_accuracy: 0.4571\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.0171 - categorical_accuracy: 0.2973 - val_loss: 1.9307 - val_categorical_accuracy: 0.4627\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 2.0108 - categorical_accuracy: 0.2985 - val_loss: 1.9241 - val_categorical_accuracy: 0.4684\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 2.0034 - categorical_accuracy: 0.3047 - val_loss: 1.9175 - val_categorical_accuracy: 0.4747\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9977 - categorical_accuracy: 0.3078 - val_loss: 1.9110 - val_categorical_accuracy: 0.4820\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9912 - categorical_accuracy: 0.3092 - val_loss: 1.9045 - val_categorical_accuracy: 0.4884\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.9850 - categorical_accuracy: 0.3134 - val_loss: 1.8980 - val_categorical_accuracy: 0.4952\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9818 - categorical_accuracy: 0.3130 - val_loss: 1.8916 - val_categorical_accuracy: 0.5015\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9789 - categorical_accuracy: 0.3181 - val_loss: 1.8854 - val_categorical_accuracy: 0.5073\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9691 - categorical_accuracy: 0.3200 - val_loss: 1.8791 - val_categorical_accuracy: 0.5132\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9650 - categorical_accuracy: 0.3274 - val_loss: 1.8729 - val_categorical_accuracy: 0.5182\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9652 - categorical_accuracy: 0.3212 - val_loss: 1.8668 - val_categorical_accuracy: 0.5232\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9548 - categorical_accuracy: 0.3292 - val_loss: 1.8608 - val_categorical_accuracy: 0.5288\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9515 - categorical_accuracy: 0.3265 - val_loss: 1.8548 - val_categorical_accuracy: 0.5352\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9443 - categorical_accuracy: 0.3316 - val_loss: 1.8489 - val_categorical_accuracy: 0.5414\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9451 - categorical_accuracy: 0.3305 - val_loss: 1.8431 - val_categorical_accuracy: 0.5461\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9409 - categorical_accuracy: 0.3314 - val_loss: 1.8373 - val_categorical_accuracy: 0.5512\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.9340 - categorical_accuracy: 0.3368 - val_loss: 1.8315 - val_categorical_accuracy: 0.5558\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9256 - categorical_accuracy: 0.3394 - val_loss: 1.8258 - val_categorical_accuracy: 0.5601\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9231 - categorical_accuracy: 0.3427 - val_loss: 1.8201 - val_categorical_accuracy: 0.5637\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9170 - categorical_accuracy: 0.3453 - val_loss: 1.8145 - val_categorical_accuracy: 0.5667\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.9104 - categorical_accuracy: 0.3479 - val_loss: 1.8089 - val_categorical_accuracy: 0.5708\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.9070 - categorical_accuracy: 0.3510 - val_loss: 1.8034 - val_categorical_accuracy: 0.5755\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.9024 - categorical_accuracy: 0.3505 - val_loss: 1.7979 - val_categorical_accuracy: 0.5798\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.9065 - categorical_accuracy: 0.3481 - val_loss: 1.7925 - val_categorical_accuracy: 0.5828\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8972 - categorical_accuracy: 0.3543 - val_loss: 1.7872 - val_categorical_accuracy: 0.5870\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8878 - categorical_accuracy: 0.3586 - val_loss: 1.7818 - val_categorical_accuracy: 0.5904\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8882 - categorical_accuracy: 0.3548 - val_loss: 1.7766 - val_categorical_accuracy: 0.5939\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8805 - categorical_accuracy: 0.3602 - val_loss: 1.7713 - val_categorical_accuracy: 0.5975\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8750 - categorical_accuracy: 0.3641 - val_loss: 1.7661 - val_categorical_accuracy: 0.6004\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.8741 - categorical_accuracy: 0.3644 - val_loss: 1.7609 - val_categorical_accuracy: 0.6028\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8693 - categorical_accuracy: 0.3648 - val_loss: 1.7558 - val_categorical_accuracy: 0.6062\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8705 - categorical_accuracy: 0.3634 - val_loss: 1.7508 - val_categorical_accuracy: 0.6102\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.8606 - categorical_accuracy: 0.3693 - val_loss: 1.7458 - val_categorical_accuracy: 0.6133\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.8580 - categorical_accuracy: 0.3684 - val_loss: 1.7408 - val_categorical_accuracy: 0.6167\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8511 - categorical_accuracy: 0.3736 - val_loss: 1.7359 - val_categorical_accuracy: 0.6196\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8492 - categorical_accuracy: 0.3750 - val_loss: 1.7310 - val_categorical_accuracy: 0.6238\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8505 - categorical_accuracy: 0.3733 - val_loss: 1.7262 - val_categorical_accuracy: 0.6267\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8421 - categorical_accuracy: 0.3753 - val_loss: 1.7214 - val_categorical_accuracy: 0.6288\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8411 - categorical_accuracy: 0.3724 - val_loss: 1.7166 - val_categorical_accuracy: 0.6317\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8405 - categorical_accuracy: 0.3735 - val_loss: 1.7119 - val_categorical_accuracy: 0.6343\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8396 - categorical_accuracy: 0.3711 - val_loss: 1.7073 - val_categorical_accuracy: 0.6367\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8285 - categorical_accuracy: 0.3788 - val_loss: 1.7027 - val_categorical_accuracy: 0.6403\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8297 - categorical_accuracy: 0.3779 - val_loss: 1.6982 - val_categorical_accuracy: 0.6418\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8227 - categorical_accuracy: 0.3809 - val_loss: 1.6937 - val_categorical_accuracy: 0.6443\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.8151 - categorical_accuracy: 0.3859 - val_loss: 1.6892 - val_categorical_accuracy: 0.6468\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.8128 - categorical_accuracy: 0.3854 - val_loss: 1.6846 - val_categorical_accuracy: 0.6500\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.8122 - categorical_accuracy: 0.3848 - val_loss: 1.6802 - val_categorical_accuracy: 0.6528\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.8074 - categorical_accuracy: 0.3871 - val_loss: 1.6758 - val_categorical_accuracy: 0.6557\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8067 - categorical_accuracy: 0.3880 - val_loss: 1.6714 - val_categorical_accuracy: 0.6580\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.8024 - categorical_accuracy: 0.3879 - val_loss: 1.6671 - val_categorical_accuracy: 0.6605\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7950 - categorical_accuracy: 0.3918 - val_loss: 1.6628 - val_categorical_accuracy: 0.6628\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7937 - categorical_accuracy: 0.3909 - val_loss: 1.6586 - val_categorical_accuracy: 0.6653\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7946 - categorical_accuracy: 0.3898 - val_loss: 1.6544 - val_categorical_accuracy: 0.6672\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7889 - categorical_accuracy: 0.3907 - val_loss: 1.6502 - val_categorical_accuracy: 0.6694\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7873 - categorical_accuracy: 0.3924 - val_loss: 1.6461 - val_categorical_accuracy: 0.6708\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7836 - categorical_accuracy: 0.3960 - val_loss: 1.6420 - val_categorical_accuracy: 0.6724\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7797 - categorical_accuracy: 0.3956 - val_loss: 1.6379 - val_categorical_accuracy: 0.6747\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7784 - categorical_accuracy: 0.3968 - val_loss: 1.6338 - val_categorical_accuracy: 0.6768\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7761 - categorical_accuracy: 0.3972 - val_loss: 1.6298 - val_categorical_accuracy: 0.6788\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7662 - categorical_accuracy: 0.3993 - val_loss: 1.6258 - val_categorical_accuracy: 0.6816\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7738 - categorical_accuracy: 0.3948 - val_loss: 1.6219 - val_categorical_accuracy: 0.6831\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7604 - categorical_accuracy: 0.4037 - val_loss: 1.6179 - val_categorical_accuracy: 0.6850\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7639 - categorical_accuracy: 0.4001 - val_loss: 1.6140 - val_categorical_accuracy: 0.6865\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7581 - categorical_accuracy: 0.4031 - val_loss: 1.6102 - val_categorical_accuracy: 0.6879\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7597 - categorical_accuracy: 0.4031 - val_loss: 1.6064 - val_categorical_accuracy: 0.6892\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7604 - categorical_accuracy: 0.3993 - val_loss: 1.6026 - val_categorical_accuracy: 0.6908\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7510 - categorical_accuracy: 0.4044 - val_loss: 1.5988 - val_categorical_accuracy: 0.6927\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7468 - categorical_accuracy: 0.4062 - val_loss: 1.5951 - val_categorical_accuracy: 0.6948\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7445 - categorical_accuracy: 0.4059 - val_loss: 1.5913 - val_categorical_accuracy: 0.6968\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7453 - categorical_accuracy: 0.4048 - val_loss: 1.5877 - val_categorical_accuracy: 0.6989\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7412 - categorical_accuracy: 0.4074 - val_loss: 1.5840 - val_categorical_accuracy: 0.7004\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7418 - categorical_accuracy: 0.4064 - val_loss: 1.5804 - val_categorical_accuracy: 0.7024\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7361 - categorical_accuracy: 0.4067 - val_loss: 1.5769 - val_categorical_accuracy: 0.7041\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7293 - categorical_accuracy: 0.4129 - val_loss: 1.5733 - val_categorical_accuracy: 0.7057\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7312 - categorical_accuracy: 0.4108 - val_loss: 1.5697 - val_categorical_accuracy: 0.7074\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7298 - categorical_accuracy: 0.4105 - val_loss: 1.5663 - val_categorical_accuracy: 0.7095\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7248 - categorical_accuracy: 0.4120 - val_loss: 1.5628 - val_categorical_accuracy: 0.7105\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7247 - categorical_accuracy: 0.4125 - val_loss: 1.5593 - val_categorical_accuracy: 0.7115\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7228 - categorical_accuracy: 0.4102 - val_loss: 1.5559 - val_categorical_accuracy: 0.7129\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7200 - categorical_accuracy: 0.4116 - val_loss: 1.5525 - val_categorical_accuracy: 0.7143\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7179 - categorical_accuracy: 0.4122 - val_loss: 1.5492 - val_categorical_accuracy: 0.7153\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7134 - categorical_accuracy: 0.4160 - val_loss: 1.5458 - val_categorical_accuracy: 0.7168\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7126 - categorical_accuracy: 0.4153 - val_loss: 1.5424 - val_categorical_accuracy: 0.7179\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7097 - categorical_accuracy: 0.4177 - val_loss: 1.5391 - val_categorical_accuracy: 0.7190\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7080 - categorical_accuracy: 0.4167 - val_loss: 1.5358 - val_categorical_accuracy: 0.7206\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6976 - categorical_accuracy: 0.4207 - val_loss: 1.5326 - val_categorical_accuracy: 0.7221\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.7026 - categorical_accuracy: 0.4200 - val_loss: 1.5293 - val_categorical_accuracy: 0.7233\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.7036 - categorical_accuracy: 0.4165 - val_loss: 1.5261 - val_categorical_accuracy: 0.7250\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6950 - categorical_accuracy: 0.4206 - val_loss: 1.5229 - val_categorical_accuracy: 0.7266\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6960 - categorical_accuracy: 0.4235 - val_loss: 1.5197 - val_categorical_accuracy: 0.7280\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6856 - categorical_accuracy: 0.4256 - val_loss: 1.5165 - val_categorical_accuracy: 0.7285\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6908 - categorical_accuracy: 0.4223 - val_loss: 1.5134 - val_categorical_accuracy: 0.7296\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6861 - categorical_accuracy: 0.4251 - val_loss: 1.5103 - val_categorical_accuracy: 0.7303\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6848 - categorical_accuracy: 0.4222 - val_loss: 1.5072 - val_categorical_accuracy: 0.7313\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6872 - categorical_accuracy: 0.4215 - val_loss: 1.5041 - val_categorical_accuracy: 0.7326\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6874 - categorical_accuracy: 0.4219 - val_loss: 1.5011 - val_categorical_accuracy: 0.7339\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6810 - categorical_accuracy: 0.4248 - val_loss: 1.4981 - val_categorical_accuracy: 0.7351\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6800 - categorical_accuracy: 0.4251 - val_loss: 1.4951 - val_categorical_accuracy: 0.7358\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6784 - categorical_accuracy: 0.4255 - val_loss: 1.4922 - val_categorical_accuracy: 0.7372\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6768 - categorical_accuracy: 0.4256 - val_loss: 1.4892 - val_categorical_accuracy: 0.7383\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6801 - categorical_accuracy: 0.4225 - val_loss: 1.4863 - val_categorical_accuracy: 0.7398\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6726 - categorical_accuracy: 0.4265 - val_loss: 1.4834 - val_categorical_accuracy: 0.7408\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6682 - categorical_accuracy: 0.4294 - val_loss: 1.4805 - val_categorical_accuracy: 0.7419\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6625 - categorical_accuracy: 0.4311 - val_loss: 1.4776 - val_categorical_accuracy: 0.7426\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6648 - categorical_accuracy: 0.4276 - val_loss: 1.4747 - val_categorical_accuracy: 0.7431\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6651 - categorical_accuracy: 0.4280 - val_loss: 1.4719 - val_categorical_accuracy: 0.7440\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6615 - categorical_accuracy: 0.4294 - val_loss: 1.4691 - val_categorical_accuracy: 0.7448\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6598 - categorical_accuracy: 0.4324 - val_loss: 1.4663 - val_categorical_accuracy: 0.7458\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6579 - categorical_accuracy: 0.4301 - val_loss: 1.4635 - val_categorical_accuracy: 0.7473\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6560 - categorical_accuracy: 0.4320 - val_loss: 1.4607 - val_categorical_accuracy: 0.7482\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6517 - categorical_accuracy: 0.4332 - val_loss: 1.4579 - val_categorical_accuracy: 0.7494\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6523 - categorical_accuracy: 0.4338 - val_loss: 1.4552 - val_categorical_accuracy: 0.7505\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6483 - categorical_accuracy: 0.4336 - val_loss: 1.4525 - val_categorical_accuracy: 0.7513\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6461 - categorical_accuracy: 0.4366 - val_loss: 1.4498 - val_categorical_accuracy: 0.7527\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6449 - categorical_accuracy: 0.4358 - val_loss: 1.4471 - val_categorical_accuracy: 0.7538\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6438 - categorical_accuracy: 0.4358 - val_loss: 1.4445 - val_categorical_accuracy: 0.7548\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6406 - categorical_accuracy: 0.4372 - val_loss: 1.4418 - val_categorical_accuracy: 0.7563\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6457 - categorical_accuracy: 0.4325 - val_loss: 1.4392 - val_categorical_accuracy: 0.7568\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6353 - categorical_accuracy: 0.4392 - val_loss: 1.4366 - val_categorical_accuracy: 0.7577\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6455 - categorical_accuracy: 0.4311 - val_loss: 1.4341 - val_categorical_accuracy: 0.7585\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6354 - categorical_accuracy: 0.4358 - val_loss: 1.4315 - val_categorical_accuracy: 0.7598\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6287 - categorical_accuracy: 0.4407 - val_loss: 1.4289 - val_categorical_accuracy: 0.7608\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6296 - categorical_accuracy: 0.4392 - val_loss: 1.4263 - val_categorical_accuracy: 0.7615\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6323 - categorical_accuracy: 0.4371 - val_loss: 1.4238 - val_categorical_accuracy: 0.7621\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6317 - categorical_accuracy: 0.4404 - val_loss: 1.4213 - val_categorical_accuracy: 0.7639\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6311 - categorical_accuracy: 0.4364 - val_loss: 1.4189 - val_categorical_accuracy: 0.7642\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6302 - categorical_accuracy: 0.4367 - val_loss: 1.4164 - val_categorical_accuracy: 0.7657\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6293 - categorical_accuracy: 0.4371 - val_loss: 1.4140 - val_categorical_accuracy: 0.7665\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6228 - categorical_accuracy: 0.4390 - val_loss: 1.4115 - val_categorical_accuracy: 0.7674\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6214 - categorical_accuracy: 0.4410 - val_loss: 1.4091 - val_categorical_accuracy: 0.7679\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6218 - categorical_accuracy: 0.4399 - val_loss: 1.4067 - val_categorical_accuracy: 0.7684\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6236 - categorical_accuracy: 0.4377 - val_loss: 1.4043 - val_categorical_accuracy: 0.7695\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 0s 7us/step - loss: 1.6216 - categorical_accuracy: 0.4389 - val_loss: 1.4020 - val_categorical_accuracy: 0.7702\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6143 - categorical_accuracy: 0.4441 - val_loss: 1.3996 - val_categorical_accuracy: 0.7711\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6221 - categorical_accuracy: 0.4385 - val_loss: 1.3973 - val_categorical_accuracy: 0.7718\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6140 - categorical_accuracy: 0.4436 - val_loss: 1.3949 - val_categorical_accuracy: 0.7728\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6153 - categorical_accuracy: 0.4436 - val_loss: 1.3926 - val_categorical_accuracy: 0.7741\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6127 - categorical_accuracy: 0.4433 - val_loss: 1.3903 - val_categorical_accuracy: 0.7743\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6075 - categorical_accuracy: 0.4450 - val_loss: 1.3880 - val_categorical_accuracy: 0.7750\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6022 - categorical_accuracy: 0.4466 - val_loss: 1.3857 - val_categorical_accuracy: 0.7758\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6059 - categorical_accuracy: 0.4443 - val_loss: 1.3834 - val_categorical_accuracy: 0.7763\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6105 - categorical_accuracy: 0.4412 - val_loss: 1.3812 - val_categorical_accuracy: 0.7768\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6114 - categorical_accuracy: 0.4405 - val_loss: 1.3790 - val_categorical_accuracy: 0.7774\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.6029 - categorical_accuracy: 0.4452 - val_loss: 1.3767 - val_categorical_accuracy: 0.7782\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.5987 - categorical_accuracy: 0.4477 - val_loss: 1.3745 - val_categorical_accuracy: 0.7787\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.5935 - categorical_accuracy: 0.4496 - val_loss: 1.3723 - val_categorical_accuracy: 0.7794\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 0s 6us/step - loss: 1.5979 - categorical_accuracy: 0.4455 - val_loss: 1.3701 - val_categorical_accuracy: 0.7802\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=48000, epochs=200, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 13us/step\n",
      "Test score: 0.2772996456950903\n",
      "Test accuracy: 0.9206\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 92.27% on the test set sounds great, and that this can be achieved using only 7850 parameters - which after all encode all the knowledge about the dataset - is impressive. Yet, misclassifying every tenth digit is of little practical relevance. In fact, even 99.9% would mean that one out of a thousand digits is misread. When using such a network to identify the zip code on a letter, every 200th letter, on average, would be subject to a wrong reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing model complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the complexity of the model by adding hidden layers. For example, adding two hidden layers with 128 neurons each, increases the number of parameters to 118,282. In this case, we can obtain a test accuracy of 97.61%. You can further increase model complexity, for example by employing 1280 hidden nodes, resulting in more than 2.6M parameters. This will allow you to exceed 99% on training accuracy, but not much more than 98% on the test. At some point, you will learn an exact representation of the training data, even 100% accuracy on the training set, but not be able to train a network that is able to truly understand what to look for in your data. This is known as <i>overfitting</i>. As a rule of thumb, a network should always be trained so that the accuracy of training exceeds that of the test set, but not too much further as the reason is likely overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH=200\n",
    "BATCH_SIZE=128\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['categorical_accuracy'])\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes='true', to_file='figs/model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to render the neural network graphically using Keras' <code>plot_model</code> module (see above)\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/model.png\" width=\"30%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 1.9860 - categorical_accuracy: 0.3219 - val_loss: 1.3298 - val_categorical_accuracy: 0.7560\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 1.3170 - categorical_accuracy: 0.5770 - val_loss: 0.7410 - val_categorical_accuracy: 0.8403\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.9840 - categorical_accuracy: 0.6848 - val_loss: 0.5470 - val_categorical_accuracy: 0.8676\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.8279 - categorical_accuracy: 0.7381 - val_loss: 0.4554 - val_categorical_accuracy: 0.8858\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.7354 - categorical_accuracy: 0.7711 - val_loss: 0.3996 - val_categorical_accuracy: 0.8933\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.6640 - categorical_accuracy: 0.7949 - val_loss: 0.3656 - val_categorical_accuracy: 0.8987\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.6208 - categorical_accuracy: 0.8111 - val_loss: 0.3421 - val_categorical_accuracy: 0.9045\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.5812 - categorical_accuracy: 0.8265 - val_loss: 0.3223 - val_categorical_accuracy: 0.9084\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.5517 - categorical_accuracy: 0.8339 - val_loss: 0.3070 - val_categorical_accuracy: 0.9120\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.5208 - categorical_accuracy: 0.8431 - val_loss: 0.2935 - val_categorical_accuracy: 0.9154\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.5072 - categorical_accuracy: 0.8497 - val_loss: 0.2833 - val_categorical_accuracy: 0.9181\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4857 - categorical_accuracy: 0.8564 - val_loss: 0.2729 - val_categorical_accuracy: 0.9202\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4661 - categorical_accuracy: 0.8628 - val_loss: 0.2639 - val_categorical_accuracy: 0.9232\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4534 - categorical_accuracy: 0.8671 - val_loss: 0.2579 - val_categorical_accuracy: 0.9249\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4439 - categorical_accuracy: 0.8704 - val_loss: 0.2503 - val_categorical_accuracy: 0.9271\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4281 - categorical_accuracy: 0.8758 - val_loss: 0.2443 - val_categorical_accuracy: 0.9291\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4189 - categorical_accuracy: 0.8784 - val_loss: 0.2393 - val_categorical_accuracy: 0.9314\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.4085 - categorical_accuracy: 0.8810 - val_loss: 0.2328 - val_categorical_accuracy: 0.9327\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3994 - categorical_accuracy: 0.8842 - val_loss: 0.2285 - val_categorical_accuracy: 0.9341\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3929 - categorical_accuracy: 0.8880 - val_loss: 0.2228 - val_categorical_accuracy: 0.9353\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3844 - categorical_accuracy: 0.8884 - val_loss: 0.2197 - val_categorical_accuracy: 0.9377\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3801 - categorical_accuracy: 0.8891 - val_loss: 0.2160 - val_categorical_accuracy: 0.9382\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3711 - categorical_accuracy: 0.8946 - val_loss: 0.2119 - val_categorical_accuracy: 0.9389\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3629 - categorical_accuracy: 0.8962 - val_loss: 0.2080 - val_categorical_accuracy: 0.9412\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3577 - categorical_accuracy: 0.8970 - val_loss: 0.2052 - val_categorical_accuracy: 0.9412\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3531 - categorical_accuracy: 0.8991 - val_loss: 0.2001 - val_categorical_accuracy: 0.9427\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3444 - categorical_accuracy: 0.9016 - val_loss: 0.1981 - val_categorical_accuracy: 0.9430\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3396 - categorical_accuracy: 0.9022 - val_loss: 0.1950 - val_categorical_accuracy: 0.9437\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3345 - categorical_accuracy: 0.9037 - val_loss: 0.1917 - val_categorical_accuracy: 0.9454\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3338 - categorical_accuracy: 0.9045 - val_loss: 0.1886 - val_categorical_accuracy: 0.9465\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3237 - categorical_accuracy: 0.9074 - val_loss: 0.1861 - val_categorical_accuracy: 0.9458\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3218 - categorical_accuracy: 0.9075 - val_loss: 0.1838 - val_categorical_accuracy: 0.9476\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3182 - categorical_accuracy: 0.9081 - val_loss: 0.1821 - val_categorical_accuracy: 0.9483\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3144 - categorical_accuracy: 0.9103 - val_loss: 0.1797 - val_categorical_accuracy: 0.9485\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3061 - categorical_accuracy: 0.9126 - val_loss: 0.1778 - val_categorical_accuracy: 0.9493\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3097 - categorical_accuracy: 0.9109 - val_loss: 0.1746 - val_categorical_accuracy: 0.9508\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3000 - categorical_accuracy: 0.9142 - val_loss: 0.1738 - val_categorical_accuracy: 0.9508\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3000 - categorical_accuracy: 0.9138 - val_loss: 0.1705 - val_categorical_accuracy: 0.9523\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2938 - categorical_accuracy: 0.9167 - val_loss: 0.1696 - val_categorical_accuracy: 0.9524\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2886 - categorical_accuracy: 0.9161 - val_loss: 0.1680 - val_categorical_accuracy: 0.9527\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2878 - categorical_accuracy: 0.9177 - val_loss: 0.1646 - val_categorical_accuracy: 0.9526\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2851 - categorical_accuracy: 0.9181 - val_loss: 0.1642 - val_categorical_accuracy: 0.9538\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2856 - categorical_accuracy: 0.9192 - val_loss: 0.1621 - val_categorical_accuracy: 0.9552\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2828 - categorical_accuracy: 0.9184 - val_loss: 0.1593 - val_categorical_accuracy: 0.9560\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2729 - categorical_accuracy: 0.9207 - val_loss: 0.1582 - val_categorical_accuracy: 0.9556\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2752 - categorical_accuracy: 0.9203 - val_loss: 0.1580 - val_categorical_accuracy: 0.9561\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2672 - categorical_accuracy: 0.9226 - val_loss: 0.1556 - val_categorical_accuracy: 0.9562\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2708 - categorical_accuracy: 0.9214 - val_loss: 0.1543 - val_categorical_accuracy: 0.9575\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2641 - categorical_accuracy: 0.9242 - val_loss: 0.1532 - val_categorical_accuracy: 0.9580\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2606 - categorical_accuracy: 0.9253 - val_loss: 0.1514 - val_categorical_accuracy: 0.9576\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2600 - categorical_accuracy: 0.9251 - val_loss: 0.1498 - val_categorical_accuracy: 0.9583\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2586 - categorical_accuracy: 0.9269 - val_loss: 0.1481 - val_categorical_accuracy: 0.9589\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2552 - categorical_accuracy: 0.9266 - val_loss: 0.1474 - val_categorical_accuracy: 0.9587\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2503 - categorical_accuracy: 0.9287 - val_loss: 0.1462 - val_categorical_accuracy: 0.9583\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2536 - categorical_accuracy: 0.9273 - val_loss: 0.1443 - val_categorical_accuracy: 0.9593\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2505 - categorical_accuracy: 0.9271 - val_loss: 0.1431 - val_categorical_accuracy: 0.9597\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2494 - categorical_accuracy: 0.9287 - val_loss: 0.1421 - val_categorical_accuracy: 0.9603\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2444 - categorical_accuracy: 0.9294 - val_loss: 0.1415 - val_categorical_accuracy: 0.9602\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2447 - categorical_accuracy: 0.9291 - val_loss: 0.1413 - val_categorical_accuracy: 0.9608\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2413 - categorical_accuracy: 0.9310 - val_loss: 0.1395 - val_categorical_accuracy: 0.9611\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2400 - categorical_accuracy: 0.9301 - val_loss: 0.1383 - val_categorical_accuracy: 0.9610\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2401 - categorical_accuracy: 0.9300 - val_loss: 0.1382 - val_categorical_accuracy: 0.9608\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2384 - categorical_accuracy: 0.9315 - val_loss: 0.1367 - val_categorical_accuracy: 0.9618\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2361 - categorical_accuracy: 0.9312 - val_loss: 0.1367 - val_categorical_accuracy: 0.9613\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2336 - categorical_accuracy: 0.9334 - val_loss: 0.1356 - val_categorical_accuracy: 0.9618\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2296 - categorical_accuracy: 0.9339 - val_loss: 0.1345 - val_categorical_accuracy: 0.9618\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2337 - categorical_accuracy: 0.9321 - val_loss: 0.1336 - val_categorical_accuracy: 0.9626\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2307 - categorical_accuracy: 0.9334 - val_loss: 0.1329 - val_categorical_accuracy: 0.9623\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2277 - categorical_accuracy: 0.9337 - val_loss: 0.1324 - val_categorical_accuracy: 0.9632\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2245 - categorical_accuracy: 0.9353 - val_loss: 0.1315 - val_categorical_accuracy: 0.9628\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2236 - categorical_accuracy: 0.9351 - val_loss: 0.1308 - val_categorical_accuracy: 0.9639\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2208 - categorical_accuracy: 0.9373 - val_loss: 0.1300 - val_categorical_accuracy: 0.9629\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2201 - categorical_accuracy: 0.9368 - val_loss: 0.1288 - val_categorical_accuracy: 0.9639\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2165 - categorical_accuracy: 0.9381 - val_loss: 0.1284 - val_categorical_accuracy: 0.9641\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2151 - categorical_accuracy: 0.9381 - val_loss: 0.1276 - val_categorical_accuracy: 0.9648\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2173 - categorical_accuracy: 0.9364 - val_loss: 0.1260 - val_categorical_accuracy: 0.9648\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2142 - categorical_accuracy: 0.9373 - val_loss: 0.1260 - val_categorical_accuracy: 0.9648\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2115 - categorical_accuracy: 0.9386 - val_loss: 0.1262 - val_categorical_accuracy: 0.9648\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2136 - categorical_accuracy: 0.9384 - val_loss: 0.1244 - val_categorical_accuracy: 0.9653\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2116 - categorical_accuracy: 0.9381 - val_loss: 0.1243 - val_categorical_accuracy: 0.9653\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2063 - categorical_accuracy: 0.9403 - val_loss: 0.1239 - val_categorical_accuracy: 0.9656\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2094 - categorical_accuracy: 0.9405 - val_loss: 0.1229 - val_categorical_accuracy: 0.9661\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2080 - categorical_accuracy: 0.9403 - val_loss: 0.1219 - val_categorical_accuracy: 0.9659\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2057 - categorical_accuracy: 0.9392 - val_loss: 0.1221 - val_categorical_accuracy: 0.9660\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2047 - categorical_accuracy: 0.9409 - val_loss: 0.1216 - val_categorical_accuracy: 0.9662\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2035 - categorical_accuracy: 0.9420 - val_loss: 0.1213 - val_categorical_accuracy: 0.9664\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2055 - categorical_accuracy: 0.9416 - val_loss: 0.1204 - val_categorical_accuracy: 0.9664\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2033 - categorical_accuracy: 0.9413 - val_loss: 0.1197 - val_categorical_accuracy: 0.9663\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2023 - categorical_accuracy: 0.9418 - val_loss: 0.1187 - val_categorical_accuracy: 0.9673\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2027 - categorical_accuracy: 0.9414 - val_loss: 0.1185 - val_categorical_accuracy: 0.9675\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2020 - categorical_accuracy: 0.9413 - val_loss: 0.1180 - val_categorical_accuracy: 0.9673\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1996 - categorical_accuracy: 0.9427 - val_loss: 0.1172 - val_categorical_accuracy: 0.9678\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1974 - categorical_accuracy: 0.9421 - val_loss: 0.1172 - val_categorical_accuracy: 0.9677\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1965 - categorical_accuracy: 0.9423 - val_loss: 0.1164 - val_categorical_accuracy: 0.9671\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1946 - categorical_accuracy: 0.9449 - val_loss: 0.1163 - val_categorical_accuracy: 0.9680\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1938 - categorical_accuracy: 0.9441 - val_loss: 0.1156 - val_categorical_accuracy: 0.9682\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1936 - categorical_accuracy: 0.9438 - val_loss: 0.1155 - val_categorical_accuracy: 0.9681\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1907 - categorical_accuracy: 0.9448 - val_loss: 0.1146 - val_categorical_accuracy: 0.9683\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1886 - categorical_accuracy: 0.9440 - val_loss: 0.1150 - val_categorical_accuracy: 0.9673\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1892 - categorical_accuracy: 0.9449 - val_loss: 0.1140 - val_categorical_accuracy: 0.9682\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1899 - categorical_accuracy: 0.9447 - val_loss: 0.1139 - val_categorical_accuracy: 0.9676\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1854 - categorical_accuracy: 0.9453 - val_loss: 0.1130 - val_categorical_accuracy: 0.9680\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1855 - categorical_accuracy: 0.9458 - val_loss: 0.1130 - val_categorical_accuracy: 0.9685\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1847 - categorical_accuracy: 0.9458 - val_loss: 0.1126 - val_categorical_accuracy: 0.9687\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1865 - categorical_accuracy: 0.9460 - val_loss: 0.1129 - val_categorical_accuracy: 0.9684\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1795 - categorical_accuracy: 0.9474 - val_loss: 0.1141 - val_categorical_accuracy: 0.9687\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1868 - categorical_accuracy: 0.9453 - val_loss: 0.1120 - val_categorical_accuracy: 0.9688\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1841 - categorical_accuracy: 0.9456 - val_loss: 0.1121 - val_categorical_accuracy: 0.9689\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1824 - categorical_accuracy: 0.9461 - val_loss: 0.1115 - val_categorical_accuracy: 0.9695\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1799 - categorical_accuracy: 0.9474 - val_loss: 0.1102 - val_categorical_accuracy: 0.9686\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1826 - categorical_accuracy: 0.9468 - val_loss: 0.1108 - val_categorical_accuracy: 0.9696\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1782 - categorical_accuracy: 0.9478 - val_loss: 0.1106 - val_categorical_accuracy: 0.9692\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1805 - categorical_accuracy: 0.9470 - val_loss: 0.1100 - val_categorical_accuracy: 0.9688\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1745 - categorical_accuracy: 0.9488 - val_loss: 0.1095 - val_categorical_accuracy: 0.9693\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1757 - categorical_accuracy: 0.9481 - val_loss: 0.1096 - val_categorical_accuracy: 0.9695\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1774 - categorical_accuracy: 0.9475 - val_loss: 0.1096 - val_categorical_accuracy: 0.9694\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1725 - categorical_accuracy: 0.9495 - val_loss: 0.1090 - val_categorical_accuracy: 0.9691\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1743 - categorical_accuracy: 0.9490 - val_loss: 0.1095 - val_categorical_accuracy: 0.9693\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1720 - categorical_accuracy: 0.9498 - val_loss: 0.1080 - val_categorical_accuracy: 0.9697\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1728 - categorical_accuracy: 0.9497 - val_loss: 0.1077 - val_categorical_accuracy: 0.9699\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1703 - categorical_accuracy: 0.9493 - val_loss: 0.1071 - val_categorical_accuracy: 0.9704\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1746 - categorical_accuracy: 0.9490 - val_loss: 0.1073 - val_categorical_accuracy: 0.9703\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1708 - categorical_accuracy: 0.9507 - val_loss: 0.1075 - val_categorical_accuracy: 0.9701\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1690 - categorical_accuracy: 0.9504 - val_loss: 0.1065 - val_categorical_accuracy: 0.9703\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1673 - categorical_accuracy: 0.9505 - val_loss: 0.1067 - val_categorical_accuracy: 0.9703\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1717 - categorical_accuracy: 0.9486 - val_loss: 0.1058 - val_categorical_accuracy: 0.9704\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1672 - categorical_accuracy: 0.9504 - val_loss: 0.1064 - val_categorical_accuracy: 0.9700\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1703 - categorical_accuracy: 0.9498 - val_loss: 0.1058 - val_categorical_accuracy: 0.9704\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1644 - categorical_accuracy: 0.9504 - val_loss: 0.1061 - val_categorical_accuracy: 0.9703\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1670 - categorical_accuracy: 0.9510 - val_loss: 0.1060 - val_categorical_accuracy: 0.9703\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1693 - categorical_accuracy: 0.9504 - val_loss: 0.1055 - val_categorical_accuracy: 0.9701\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1628 - categorical_accuracy: 0.9522 - val_loss: 0.1052 - val_categorical_accuracy: 0.9701\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1638 - categorical_accuracy: 0.9512 - val_loss: 0.1041 - val_categorical_accuracy: 0.9706\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1650 - categorical_accuracy: 0.9517 - val_loss: 0.1044 - val_categorical_accuracy: 0.9705\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1615 - categorical_accuracy: 0.9526 - val_loss: 0.1042 - val_categorical_accuracy: 0.9712\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1595 - categorical_accuracy: 0.9537 - val_loss: 0.1039 - val_categorical_accuracy: 0.9708\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1594 - categorical_accuracy: 0.9524 - val_loss: 0.1034 - val_categorical_accuracy: 0.9707\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1629 - categorical_accuracy: 0.9527 - val_loss: 0.1034 - val_categorical_accuracy: 0.9707\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1597 - categorical_accuracy: 0.9533 - val_loss: 0.1036 - val_categorical_accuracy: 0.9715\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1597 - categorical_accuracy: 0.9522 - val_loss: 0.1029 - val_categorical_accuracy: 0.9711\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1573 - categorical_accuracy: 0.9534 - val_loss: 0.1025 - val_categorical_accuracy: 0.9712\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1616 - categorical_accuracy: 0.9526 - val_loss: 0.1026 - val_categorical_accuracy: 0.9708\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1556 - categorical_accuracy: 0.9537 - val_loss: 0.1018 - val_categorical_accuracy: 0.9718\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1541 - categorical_accuracy: 0.9548 - val_loss: 0.1026 - val_categorical_accuracy: 0.9710\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1585 - categorical_accuracy: 0.9537 - val_loss: 0.1021 - val_categorical_accuracy: 0.9723\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1561 - categorical_accuracy: 0.9538 - val_loss: 0.1029 - val_categorical_accuracy: 0.9712\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1539 - categorical_accuracy: 0.9553 - val_loss: 0.1016 - val_categorical_accuracy: 0.9722\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1569 - categorical_accuracy: 0.9544 - val_loss: 0.1022 - val_categorical_accuracy: 0.9719\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1522 - categorical_accuracy: 0.9554 - val_loss: 0.1011 - val_categorical_accuracy: 0.9718\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1529 - categorical_accuracy: 0.9557 - val_loss: 0.1006 - val_categorical_accuracy: 0.9723\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1512 - categorical_accuracy: 0.9544 - val_loss: 0.1029 - val_categorical_accuracy: 0.9715\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1514 - categorical_accuracy: 0.9568 - val_loss: 0.1008 - val_categorical_accuracy: 0.9720\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1524 - categorical_accuracy: 0.9541 - val_loss: 0.1010 - val_categorical_accuracy: 0.9713\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1561 - categorical_accuracy: 0.9531 - val_loss: 0.1012 - val_categorical_accuracy: 0.9716\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1524 - categorical_accuracy: 0.9550 - val_loss: 0.1006 - val_categorical_accuracy: 0.9720\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1498 - categorical_accuracy: 0.9564 - val_loss: 0.1008 - val_categorical_accuracy: 0.9721\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1552 - categorical_accuracy: 0.9535 - val_loss: 0.1012 - val_categorical_accuracy: 0.9718\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1537 - categorical_accuracy: 0.9547 - val_loss: 0.0998 - val_categorical_accuracy: 0.9722\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1450 - categorical_accuracy: 0.9569 - val_loss: 0.0991 - val_categorical_accuracy: 0.9721\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1527 - categorical_accuracy: 0.9542 - val_loss: 0.0996 - val_categorical_accuracy: 0.9723\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1509 - categorical_accuracy: 0.9554 - val_loss: 0.0991 - val_categorical_accuracy: 0.9721\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1514 - categorical_accuracy: 0.9558 - val_loss: 0.0996 - val_categorical_accuracy: 0.9723\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1473 - categorical_accuracy: 0.9564 - val_loss: 0.0990 - val_categorical_accuracy: 0.9725\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1487 - categorical_accuracy: 0.9560 - val_loss: 0.0993 - val_categorical_accuracy: 0.9722\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1463 - categorical_accuracy: 0.9566 - val_loss: 0.0997 - val_categorical_accuracy: 0.9721\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1463 - categorical_accuracy: 0.9571 - val_loss: 0.0985 - val_categorical_accuracy: 0.9723\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1471 - categorical_accuracy: 0.9567 - val_loss: 0.0988 - val_categorical_accuracy: 0.9725\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1467 - categorical_accuracy: 0.9567 - val_loss: 0.0992 - val_categorical_accuracy: 0.9728\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1451 - categorical_accuracy: 0.9565 - val_loss: 0.0990 - val_categorical_accuracy: 0.9723\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1457 - categorical_accuracy: 0.9565 - val_loss: 0.0983 - val_categorical_accuracy: 0.9726\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1425 - categorical_accuracy: 0.9575 - val_loss: 0.0976 - val_categorical_accuracy: 0.9732\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1441 - categorical_accuracy: 0.9582 - val_loss: 0.0975 - val_categorical_accuracy: 0.9726\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1483 - categorical_accuracy: 0.9559 - val_loss: 0.0975 - val_categorical_accuracy: 0.9729\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1432 - categorical_accuracy: 0.9573 - val_loss: 0.0984 - val_categorical_accuracy: 0.9730\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1433 - categorical_accuracy: 0.9583 - val_loss: 0.0976 - val_categorical_accuracy: 0.9730\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1449 - categorical_accuracy: 0.9567 - val_loss: 0.0978 - val_categorical_accuracy: 0.9719\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1424 - categorical_accuracy: 0.9581 - val_loss: 0.0978 - val_categorical_accuracy: 0.9723\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1446 - categorical_accuracy: 0.9569 - val_loss: 0.0974 - val_categorical_accuracy: 0.9727\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1403 - categorical_accuracy: 0.9580 - val_loss: 0.0975 - val_categorical_accuracy: 0.9723\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1420 - categorical_accuracy: 0.9582 - val_loss: 0.0965 - val_categorical_accuracy: 0.9723\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1391 - categorical_accuracy: 0.9586 - val_loss: 0.0961 - val_categorical_accuracy: 0.9730\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1413 - categorical_accuracy: 0.9574 - val_loss: 0.0961 - val_categorical_accuracy: 0.9728\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1410 - categorical_accuracy: 0.9591 - val_loss: 0.0965 - val_categorical_accuracy: 0.9733\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1382 - categorical_accuracy: 0.9599 - val_loss: 0.0961 - val_categorical_accuracy: 0.9730\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1379 - categorical_accuracy: 0.9592 - val_loss: 0.0967 - val_categorical_accuracy: 0.9732\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1385 - categorical_accuracy: 0.9592 - val_loss: 0.0964 - val_categorical_accuracy: 0.9732\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1394 - categorical_accuracy: 0.9576 - val_loss: 0.0968 - val_categorical_accuracy: 0.9732\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1352 - categorical_accuracy: 0.9592 - val_loss: 0.0960 - val_categorical_accuracy: 0.9733\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1392 - categorical_accuracy: 0.9582 - val_loss: 0.0958 - val_categorical_accuracy: 0.9735\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1396 - categorical_accuracy: 0.9586 - val_loss: 0.0956 - val_categorical_accuracy: 0.9735\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1359 - categorical_accuracy: 0.9600 - val_loss: 0.0959 - val_categorical_accuracy: 0.9731\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1359 - categorical_accuracy: 0.9597 - val_loss: 0.0962 - val_categorical_accuracy: 0.9732\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1373 - categorical_accuracy: 0.9601 - val_loss: 0.0955 - val_categorical_accuracy: 0.9735\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1340 - categorical_accuracy: 0.9595 - val_loss: 0.0958 - val_categorical_accuracy: 0.9733\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1363 - categorical_accuracy: 0.9589 - val_loss: 0.0955 - val_categorical_accuracy: 0.9733\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1326 - categorical_accuracy: 0.9608 - val_loss: 0.0951 - val_categorical_accuracy: 0.9736\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1342 - categorical_accuracy: 0.9609 - val_loss: 0.0942 - val_categorical_accuracy: 0.9737\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.1334 - categorical_accuracy: 0.9609 - val_loss: 0.0948 - val_categorical_accuracy: 0.9736\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1348 - categorical_accuracy: 0.9613 - val_loss: 0.0952 - val_categorical_accuracy: 0.9734\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.1329 - categorical_accuracy: 0.9603 - val_loss: 0.0947 - val_categorical_accuracy: 0.9735\n",
      "10000/10000 [==============================] - 0s 28us/step\n",
      "Test score: 0.0951715635990724\n",
      "Test accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=128, epochs=200,\n",
    "verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training history is available via Keras' <code>history</code> object. Its a dictionary with the following keys (this depends on how the model was compiled). It is particularly helpful to observe where training and test accuracy start to diverge, which is the point at which further training does not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_categorical_accuracy', 'loss', 'categorical_accuracy'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12857bac8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.legend(['Training','Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving network performance (the old way)\n",
    "\n",
    "We note that all training so far has been done on the raw 28x28 bytes data. Furthermore, the 2D input data has been turned into a column vector leaving it up to the network to cherry-pick the salient information. In classic machine learning this has never been done. Instead, the trick has always been to carefully engineer features that both reduce the amount of data the network has to deal with and integrate human expert knowledge as best as possible.\n",
    "\n",
    "Looking at the MNIST data, there are many ideas that come to mind. For example, we might use basic image statistics to generate a first set of features. These could be the percentage of the matrix actually covered by pixels (with the numbers 8 and 9 using more pixels than the number 1, e.g.), the \"center of mass\" of the images with some numbers like 1 and 8 having a center of mass closer to the center and so on.\n",
    "\n",
    "We can also convolve the image with a Sobel-like filter to detect edges. Finally, we could downsample the image by grouping pixels into groups of 4x4 or even 7x7, and thresholding them. Here, the idea would be that presence of data in the new 16 or 49 \"super-pixels\" would actually suffice to classify the digits. \n",
    "\n",
    "Here, we first convolve the image with an edge detection kernel, then downsample it to a 7x7 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import signal\n",
    "from skimage.transform import downscale_local_mean # scikit-image\n",
    "\n",
    "(X_train_orig, y_train), (X_test_orig, y_test) = mnist.load_data()\n",
    "X_train=np.zeros((60000,7,7))\n",
    "X_test=np.zeros((10000,7,7))\n",
    "\n",
    "for I in range(X_train.shape[0]):\n",
    "    X_train[I]=downscale_local_mean(signal.convolve2d(X_train_orig[I,:,:]/255,[[-1,-1,-1],[-1,-8,-1],[-1,-1,-1]],mode='valid'),(4,4))\n",
    "    X_train[I]/=X_train[I].min() # values are negative after convolution\n",
    "\n",
    "for I in range(X_test.shape[0]):\n",
    "    X_test[I]=downscale_local_mean(signal.convolve2d(X_test_orig[I,:,:]/255,[[-1,-1,-1],[-1,-8,-1],[-1,-1,-1]],mode='valid'),(4,4))\n",
    "    X_test[I]/=X_test[I].min()\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for I in range(4):\n",
    "    for J in range(4):\n",
    "        plt.subplot(4,4,I*4+J+1)\n",
    "        plt.imshow(X_train[I*4+J,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 100)               5000      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 6,010\n",
      "Trainable params: 6,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 2.0864 - categorical_accuracy: 0.4040 - val_loss: 1.8903 - val_categorical_accuracy: 0.6118\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 1.7164 - categorical_accuracy: 0.6621 - val_loss: 1.5237 - val_categorical_accuracy: 0.7340\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 1.3858 - categorical_accuracy: 0.7418 - val_loss: 1.2200 - val_categorical_accuracy: 0.7856\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 1.1316 - categorical_accuracy: 0.7783 - val_loss: 1.0033 - val_categorical_accuracy: 0.8095\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.9562 - categorical_accuracy: 0.7999 - val_loss: 0.8575 - val_categorical_accuracy: 0.8287\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.8376 - categorical_accuracy: 0.8153 - val_loss: 0.7582 - val_categorical_accuracy: 0.8395\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.7555 - categorical_accuracy: 0.8250 - val_loss: 0.6887 - val_categorical_accuracy: 0.8478\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.6964 - categorical_accuracy: 0.8327 - val_loss: 0.6376 - val_categorical_accuracy: 0.8539\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.6521 - categorical_accuracy: 0.8373 - val_loss: 0.5993 - val_categorical_accuracy: 0.8553\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.6178 - categorical_accuracy: 0.8415 - val_loss: 0.5685 - val_categorical_accuracy: 0.8600\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.5904 - categorical_accuracy: 0.8450 - val_loss: 0.5446 - val_categorical_accuracy: 0.8625\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.5679 - categorical_accuracy: 0.8474 - val_loss: 0.5243 - val_categorical_accuracy: 0.8645\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.5491 - categorical_accuracy: 0.8505 - val_loss: 0.5072 - val_categorical_accuracy: 0.8685\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.5331 - categorical_accuracy: 0.8533 - val_loss: 0.4930 - val_categorical_accuracy: 0.8698\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.5194 - categorical_accuracy: 0.8545 - val_loss: 0.4805 - val_categorical_accuracy: 0.8706\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.5072 - categorical_accuracy: 0.8569 - val_loss: 0.4695 - val_categorical_accuracy: 0.8733\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4966 - categorical_accuracy: 0.8594 - val_loss: 0.4600 - val_categorical_accuracy: 0.8748\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4869 - categorical_accuracy: 0.8608 - val_loss: 0.4510 - val_categorical_accuracy: 0.8757\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4783 - categorical_accuracy: 0.8626 - val_loss: 0.4435 - val_categorical_accuracy: 0.8759\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4705 - categorical_accuracy: 0.8640 - val_loss: 0.4362 - val_categorical_accuracy: 0.8775\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4632 - categorical_accuracy: 0.8655 - val_loss: 0.4296 - val_categorical_accuracy: 0.8791\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4565 - categorical_accuracy: 0.8673 - val_loss: 0.4237 - val_categorical_accuracy: 0.8793\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4503 - categorical_accuracy: 0.8691 - val_loss: 0.4181 - val_categorical_accuracy: 0.8807\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4445 - categorical_accuracy: 0.8702 - val_loss: 0.4131 - val_categorical_accuracy: 0.8840\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4390 - categorical_accuracy: 0.8715 - val_loss: 0.4081 - val_categorical_accuracy: 0.8846\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4339 - categorical_accuracy: 0.8725 - val_loss: 0.4034 - val_categorical_accuracy: 0.8855\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.4291 - categorical_accuracy: 0.8735 - val_loss: 0.3992 - val_categorical_accuracy: 0.8871\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.4245 - categorical_accuracy: 0.8748 - val_loss: 0.3949 - val_categorical_accuracy: 0.8877\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.4201 - categorical_accuracy: 0.8757 - val_loss: 0.3911 - val_categorical_accuracy: 0.8885\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4159 - categorical_accuracy: 0.8765 - val_loss: 0.3874 - val_categorical_accuracy: 0.8902\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.4119 - categorical_accuracy: 0.8778 - val_loss: 0.3840 - val_categorical_accuracy: 0.8905\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.4082 - categorical_accuracy: 0.8793 - val_loss: 0.3806 - val_categorical_accuracy: 0.8918\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.4045 - categorical_accuracy: 0.8796 - val_loss: 0.3774 - val_categorical_accuracy: 0.8929\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.4009 - categorical_accuracy: 0.8807 - val_loss: 0.3741 - val_categorical_accuracy: 0.8935\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3976 - categorical_accuracy: 0.8813 - val_loss: 0.3712 - val_categorical_accuracy: 0.8940\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3943 - categorical_accuracy: 0.8827 - val_loss: 0.3680 - val_categorical_accuracy: 0.8940\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3911 - categorical_accuracy: 0.8833 - val_loss: 0.3652 - val_categorical_accuracy: 0.8949\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3881 - categorical_accuracy: 0.8841 - val_loss: 0.3628 - val_categorical_accuracy: 0.8952\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3851 - categorical_accuracy: 0.8845 - val_loss: 0.3597 - val_categorical_accuracy: 0.8965\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3822 - categorical_accuracy: 0.8855 - val_loss: 0.3574 - val_categorical_accuracy: 0.8973\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3794 - categorical_accuracy: 0.8861 - val_loss: 0.3551 - val_categorical_accuracy: 0.8974\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3768 - categorical_accuracy: 0.8869 - val_loss: 0.3523 - val_categorical_accuracy: 0.8990\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3741 - categorical_accuracy: 0.8873 - val_loss: 0.3500 - val_categorical_accuracy: 0.8992\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3715 - categorical_accuracy: 0.8880 - val_loss: 0.3477 - val_categorical_accuracy: 0.9000\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3691 - categorical_accuracy: 0.8889 - val_loss: 0.3455 - val_categorical_accuracy: 0.8996\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3666 - categorical_accuracy: 0.8898 - val_loss: 0.3437 - val_categorical_accuracy: 0.9002\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3643 - categorical_accuracy: 0.8903 - val_loss: 0.3414 - val_categorical_accuracy: 0.9008\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3619 - categorical_accuracy: 0.8911 - val_loss: 0.3393 - val_categorical_accuracy: 0.9017\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3597 - categorical_accuracy: 0.8915 - val_loss: 0.3373 - val_categorical_accuracy: 0.9023\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3575 - categorical_accuracy: 0.8918 - val_loss: 0.3352 - val_categorical_accuracy: 0.9032\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3553 - categorical_accuracy: 0.8930 - val_loss: 0.3333 - val_categorical_accuracy: 0.9033\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3533 - categorical_accuracy: 0.8932 - val_loss: 0.3315 - val_categorical_accuracy: 0.9038\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3513 - categorical_accuracy: 0.8938 - val_loss: 0.3298 - val_categorical_accuracy: 0.9041\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3493 - categorical_accuracy: 0.8943 - val_loss: 0.3279 - val_categorical_accuracy: 0.9046\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3473 - categorical_accuracy: 0.8950 - val_loss: 0.3263 - val_categorical_accuracy: 0.9056\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3455 - categorical_accuracy: 0.8959 - val_loss: 0.3246 - val_categorical_accuracy: 0.9061\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3436 - categorical_accuracy: 0.8962 - val_loss: 0.3232 - val_categorical_accuracy: 0.9057\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3418 - categorical_accuracy: 0.8963 - val_loss: 0.3214 - val_categorical_accuracy: 0.9067\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3401 - categorical_accuracy: 0.8971 - val_loss: 0.3198 - val_categorical_accuracy: 0.9077\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3383 - categorical_accuracy: 0.8979 - val_loss: 0.3184 - val_categorical_accuracy: 0.9086\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3366 - categorical_accuracy: 0.8982 - val_loss: 0.3169 - val_categorical_accuracy: 0.9080\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3349 - categorical_accuracy: 0.8987 - val_loss: 0.3153 - val_categorical_accuracy: 0.9088\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3333 - categorical_accuracy: 0.8997 - val_loss: 0.3139 - val_categorical_accuracy: 0.9097\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3317 - categorical_accuracy: 0.8994 - val_loss: 0.3125 - val_categorical_accuracy: 0.9101\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3301 - categorical_accuracy: 0.9004 - val_loss: 0.3111 - val_categorical_accuracy: 0.9106\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3285 - categorical_accuracy: 0.9010 - val_loss: 0.3097 - val_categorical_accuracy: 0.9103\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3269 - categorical_accuracy: 0.9015 - val_loss: 0.3083 - val_categorical_accuracy: 0.9107\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3255 - categorical_accuracy: 0.9010 - val_loss: 0.3070 - val_categorical_accuracy: 0.9110\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3240 - categorical_accuracy: 0.9022 - val_loss: 0.3057 - val_categorical_accuracy: 0.9121\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3225 - categorical_accuracy: 0.9025 - val_loss: 0.3046 - val_categorical_accuracy: 0.9121\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3210 - categorical_accuracy: 0.9032 - val_loss: 0.3031 - val_categorical_accuracy: 0.9122\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3197 - categorical_accuracy: 0.9035 - val_loss: 0.3019 - val_categorical_accuracy: 0.9123\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3182 - categorical_accuracy: 0.9035 - val_loss: 0.3008 - val_categorical_accuracy: 0.9127\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3170 - categorical_accuracy: 0.9042 - val_loss: 0.2996 - val_categorical_accuracy: 0.9127\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3155 - categorical_accuracy: 0.9044 - val_loss: 0.2983 - val_categorical_accuracy: 0.9135\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3142 - categorical_accuracy: 0.9049 - val_loss: 0.2972 - val_categorical_accuracy: 0.9138\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3129 - categorical_accuracy: 0.9053 - val_loss: 0.2961 - val_categorical_accuracy: 0.9144\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3116 - categorical_accuracy: 0.9056 - val_loss: 0.2951 - val_categorical_accuracy: 0.9140\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3104 - categorical_accuracy: 0.9056 - val_loss: 0.2939 - val_categorical_accuracy: 0.9143\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 0s 9us/step - loss: 0.3092 - categorical_accuracy: 0.9065 - val_loss: 0.2928 - val_categorical_accuracy: 0.9152\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3079 - categorical_accuracy: 0.9066 - val_loss: 0.2918 - val_categorical_accuracy: 0.9148\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3067 - categorical_accuracy: 0.9070 - val_loss: 0.2905 - val_categorical_accuracy: 0.9153\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3055 - categorical_accuracy: 0.9072 - val_loss: 0.2896 - val_categorical_accuracy: 0.9163\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3043 - categorical_accuracy: 0.9077 - val_loss: 0.2885 - val_categorical_accuracy: 0.9161\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3032 - categorical_accuracy: 0.9078 - val_loss: 0.2874 - val_categorical_accuracy: 0.9163\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3020 - categorical_accuracy: 0.9081 - val_loss: 0.2866 - val_categorical_accuracy: 0.9167\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3009 - categorical_accuracy: 0.9085 - val_loss: 0.2853 - val_categorical_accuracy: 0.9168\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2997 - categorical_accuracy: 0.9089 - val_loss: 0.2844 - val_categorical_accuracy: 0.9168\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2986 - categorical_accuracy: 0.9090 - val_loss: 0.2834 - val_categorical_accuracy: 0.9175\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2976 - categorical_accuracy: 0.9093 - val_loss: 0.2824 - val_categorical_accuracy: 0.9174\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2965 - categorical_accuracy: 0.9095 - val_loss: 0.2814 - val_categorical_accuracy: 0.9177\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2954 - categorical_accuracy: 0.9097 - val_loss: 0.2807 - val_categorical_accuracy: 0.9170\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2943 - categorical_accuracy: 0.9097 - val_loss: 0.2795 - val_categorical_accuracy: 0.9177\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2932 - categorical_accuracy: 0.9107 - val_loss: 0.2786 - val_categorical_accuracy: 0.9177\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2922 - categorical_accuracy: 0.9108 - val_loss: 0.2777 - val_categorical_accuracy: 0.9176\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2912 - categorical_accuracy: 0.9111 - val_loss: 0.2768 - val_categorical_accuracy: 0.9188\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2901 - categorical_accuracy: 0.9112 - val_loss: 0.2762 - val_categorical_accuracy: 0.9184\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2891 - categorical_accuracy: 0.9120 - val_loss: 0.2750 - val_categorical_accuracy: 0.9191\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2881 - categorical_accuracy: 0.9118 - val_loss: 0.2742 - val_categorical_accuracy: 0.9182\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2871 - categorical_accuracy: 0.9126 - val_loss: 0.2736 - val_categorical_accuracy: 0.9195\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2861 - categorical_accuracy: 0.9129 - val_loss: 0.2724 - val_categorical_accuracy: 0.9208\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2851 - categorical_accuracy: 0.9130 - val_loss: 0.2716 - val_categorical_accuracy: 0.9200\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2842 - categorical_accuracy: 0.9132 - val_loss: 0.2707 - val_categorical_accuracy: 0.9204\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2832 - categorical_accuracy: 0.9141 - val_loss: 0.2700 - val_categorical_accuracy: 0.9207\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2823 - categorical_accuracy: 0.9140 - val_loss: 0.2689 - val_categorical_accuracy: 0.9206\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2813 - categorical_accuracy: 0.9142 - val_loss: 0.2684 - val_categorical_accuracy: 0.9205\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2804 - categorical_accuracy: 0.9145 - val_loss: 0.2675 - val_categorical_accuracy: 0.9214\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2795 - categorical_accuracy: 0.9148 - val_loss: 0.2666 - val_categorical_accuracy: 0.9206\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2785 - categorical_accuracy: 0.9148 - val_loss: 0.2658 - val_categorical_accuracy: 0.9212\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2777 - categorical_accuracy: 0.9156 - val_loss: 0.2651 - val_categorical_accuracy: 0.9221\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2768 - categorical_accuracy: 0.9156 - val_loss: 0.2642 - val_categorical_accuracy: 0.9217\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2759 - categorical_accuracy: 0.9164 - val_loss: 0.2634 - val_categorical_accuracy: 0.9224\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2749 - categorical_accuracy: 0.9158 - val_loss: 0.2627 - val_categorical_accuracy: 0.9231\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2741 - categorical_accuracy: 0.9167 - val_loss: 0.2620 - val_categorical_accuracy: 0.9239\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2733 - categorical_accuracy: 0.9168 - val_loss: 0.2611 - val_categorical_accuracy: 0.9229\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2724 - categorical_accuracy: 0.9168 - val_loss: 0.2605 - val_categorical_accuracy: 0.9236\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2716 - categorical_accuracy: 0.9173 - val_loss: 0.2597 - val_categorical_accuracy: 0.9242\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2707 - categorical_accuracy: 0.9176 - val_loss: 0.2592 - val_categorical_accuracy: 0.9237\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2699 - categorical_accuracy: 0.9182 - val_loss: 0.2582 - val_categorical_accuracy: 0.9232\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2691 - categorical_accuracy: 0.9183 - val_loss: 0.2575 - val_categorical_accuracy: 0.9244\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2682 - categorical_accuracy: 0.9188 - val_loss: 0.2571 - val_categorical_accuracy: 0.9247\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2675 - categorical_accuracy: 0.9188 - val_loss: 0.2561 - val_categorical_accuracy: 0.9244\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2666 - categorical_accuracy: 0.9192 - val_loss: 0.2554 - val_categorical_accuracy: 0.9250\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2658 - categorical_accuracy: 0.9194 - val_loss: 0.2547 - val_categorical_accuracy: 0.9252\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2650 - categorical_accuracy: 0.9202 - val_loss: 0.2543 - val_categorical_accuracy: 0.9249\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2642 - categorical_accuracy: 0.9199 - val_loss: 0.2534 - val_categorical_accuracy: 0.9254\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2634 - categorical_accuracy: 0.9201 - val_loss: 0.2526 - val_categorical_accuracy: 0.9248\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2626 - categorical_accuracy: 0.9203 - val_loss: 0.2520 - val_categorical_accuracy: 0.9256\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 10us/step - loss: 0.2619 - categorical_accuracy: 0.9203 - val_loss: 0.2516 - val_categorical_accuracy: 0.9253\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2611 - categorical_accuracy: 0.9204 - val_loss: 0.2506 - val_categorical_accuracy: 0.9265\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2603 - categorical_accuracy: 0.9210 - val_loss: 0.2501 - val_categorical_accuracy: 0.9254\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2596 - categorical_accuracy: 0.9212 - val_loss: 0.2495 - val_categorical_accuracy: 0.9264\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2588 - categorical_accuracy: 0.9218 - val_loss: 0.2489 - val_categorical_accuracy: 0.9263\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2581 - categorical_accuracy: 0.9215 - val_loss: 0.2481 - val_categorical_accuracy: 0.9269\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2573 - categorical_accuracy: 0.9221 - val_loss: 0.2475 - val_categorical_accuracy: 0.9272\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2566 - categorical_accuracy: 0.9221 - val_loss: 0.2468 - val_categorical_accuracy: 0.9276\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2559 - categorical_accuracy: 0.9223 - val_loss: 0.2465 - val_categorical_accuracy: 0.9273\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2551 - categorical_accuracy: 0.9224 - val_loss: 0.2457 - val_categorical_accuracy: 0.9280\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2544 - categorical_accuracy: 0.9224 - val_loss: 0.2452 - val_categorical_accuracy: 0.9279\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2537 - categorical_accuracy: 0.9226 - val_loss: 0.2448 - val_categorical_accuracy: 0.9277\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2530 - categorical_accuracy: 0.9227 - val_loss: 0.2441 - val_categorical_accuracy: 0.9277\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2523 - categorical_accuracy: 0.9232 - val_loss: 0.2434 - val_categorical_accuracy: 0.9284\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2516 - categorical_accuracy: 0.9234 - val_loss: 0.2428 - val_categorical_accuracy: 0.9283\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2509 - categorical_accuracy: 0.9234 - val_loss: 0.2419 - val_categorical_accuracy: 0.9297\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2502 - categorical_accuracy: 0.9234 - val_loss: 0.2415 - val_categorical_accuracy: 0.9293\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2496 - categorical_accuracy: 0.9234 - val_loss: 0.2408 - val_categorical_accuracy: 0.9293\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2488 - categorical_accuracy: 0.9242 - val_loss: 0.2404 - val_categorical_accuracy: 0.9298\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2482 - categorical_accuracy: 0.9240 - val_loss: 0.2397 - val_categorical_accuracy: 0.9297\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2475 - categorical_accuracy: 0.9240 - val_loss: 0.2391 - val_categorical_accuracy: 0.9296\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2469 - categorical_accuracy: 0.9248 - val_loss: 0.2388 - val_categorical_accuracy: 0.9300\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2462 - categorical_accuracy: 0.9245 - val_loss: 0.2381 - val_categorical_accuracy: 0.9300\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2456 - categorical_accuracy: 0.9248 - val_loss: 0.2375 - val_categorical_accuracy: 0.9304\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2449 - categorical_accuracy: 0.9248 - val_loss: 0.2370 - val_categorical_accuracy: 0.9304\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2442 - categorical_accuracy: 0.9253 - val_loss: 0.2370 - val_categorical_accuracy: 0.9297\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 10us/step - loss: 0.2436 - categorical_accuracy: 0.9252 - val_loss: 0.2359 - val_categorical_accuracy: 0.9312\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2430 - categorical_accuracy: 0.9256 - val_loss: 0.2355 - val_categorical_accuracy: 0.9310\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2424 - categorical_accuracy: 0.9259 - val_loss: 0.2350 - val_categorical_accuracy: 0.9306\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2418 - categorical_accuracy: 0.9258 - val_loss: 0.2343 - val_categorical_accuracy: 0.9316\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2412 - categorical_accuracy: 0.9263 - val_loss: 0.2338 - val_categorical_accuracy: 0.9314\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2405 - categorical_accuracy: 0.9263 - val_loss: 0.2333 - val_categorical_accuracy: 0.9312\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2400 - categorical_accuracy: 0.9259 - val_loss: 0.2327 - val_categorical_accuracy: 0.9319\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2393 - categorical_accuracy: 0.9269 - val_loss: 0.2323 - val_categorical_accuracy: 0.9320\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2387 - categorical_accuracy: 0.9267 - val_loss: 0.2318 - val_categorical_accuracy: 0.9322\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2381 - categorical_accuracy: 0.9269 - val_loss: 0.2313 - val_categorical_accuracy: 0.9319\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2375 - categorical_accuracy: 0.9273 - val_loss: 0.2307 - val_categorical_accuracy: 0.9322\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2370 - categorical_accuracy: 0.9272 - val_loss: 0.2302 - val_categorical_accuracy: 0.9328\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2363 - categorical_accuracy: 0.9275 - val_loss: 0.2300 - val_categorical_accuracy: 0.9325\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2359 - categorical_accuracy: 0.9274 - val_loss: 0.2292 - val_categorical_accuracy: 0.9328\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2352 - categorical_accuracy: 0.9278 - val_loss: 0.2288 - val_categorical_accuracy: 0.9334\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2346 - categorical_accuracy: 0.9278 - val_loss: 0.2282 - val_categorical_accuracy: 0.9329\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2341 - categorical_accuracy: 0.9283 - val_loss: 0.2278 - val_categorical_accuracy: 0.9332\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2335 - categorical_accuracy: 0.9280 - val_loss: 0.2273 - val_categorical_accuracy: 0.9336\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2329 - categorical_accuracy: 0.9284 - val_loss: 0.2268 - val_categorical_accuracy: 0.9337\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2324 - categorical_accuracy: 0.9289 - val_loss: 0.2264 - val_categorical_accuracy: 0.9339\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2318 - categorical_accuracy: 0.9288 - val_loss: 0.2259 - val_categorical_accuracy: 0.9343\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2314 - categorical_accuracy: 0.9292 - val_loss: 0.2254 - val_categorical_accuracy: 0.9343\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2308 - categorical_accuracy: 0.9289 - val_loss: 0.2250 - val_categorical_accuracy: 0.9343\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2302 - categorical_accuracy: 0.9295 - val_loss: 0.2246 - val_categorical_accuracy: 0.9340\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2297 - categorical_accuracy: 0.9294 - val_loss: 0.2243 - val_categorical_accuracy: 0.9343\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2292 - categorical_accuracy: 0.9296 - val_loss: 0.2237 - val_categorical_accuracy: 0.9344\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2286 - categorical_accuracy: 0.9298 - val_loss: 0.2232 - val_categorical_accuracy: 0.9348\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2281 - categorical_accuracy: 0.9300 - val_loss: 0.2228 - val_categorical_accuracy: 0.9348\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2276 - categorical_accuracy: 0.9304 - val_loss: 0.2222 - val_categorical_accuracy: 0.9352\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2271 - categorical_accuracy: 0.9307 - val_loss: 0.2220 - val_categorical_accuracy: 0.9349\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2266 - categorical_accuracy: 0.9303 - val_loss: 0.2214 - val_categorical_accuracy: 0.9354\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2260 - categorical_accuracy: 0.9305 - val_loss: 0.2211 - val_categorical_accuracy: 0.9358\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2255 - categorical_accuracy: 0.9311 - val_loss: 0.2205 - val_categorical_accuracy: 0.9353\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2250 - categorical_accuracy: 0.9311 - val_loss: 0.2201 - val_categorical_accuracy: 0.9361\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2244 - categorical_accuracy: 0.9311 - val_loss: 0.2201 - val_categorical_accuracy: 0.9362\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2240 - categorical_accuracy: 0.9314 - val_loss: 0.2193 - val_categorical_accuracy: 0.9358\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2235 - categorical_accuracy: 0.9316 - val_loss: 0.2189 - val_categorical_accuracy: 0.9362\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2230 - categorical_accuracy: 0.9318 - val_loss: 0.2186 - val_categorical_accuracy: 0.9359\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2225 - categorical_accuracy: 0.9320 - val_loss: 0.2183 - val_categorical_accuracy: 0.9358\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2221 - categorical_accuracy: 0.9319 - val_loss: 0.2178 - val_categorical_accuracy: 0.9363\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2216 - categorical_accuracy: 0.9320 - val_loss: 0.2173 - val_categorical_accuracy: 0.9363\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 10us/step - loss: 0.2211 - categorical_accuracy: 0.9322 - val_loss: 0.2169 - val_categorical_accuracy: 0.9371\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2206 - categorical_accuracy: 0.9324 - val_loss: 0.2168 - val_categorical_accuracy: 0.9371\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2201 - categorical_accuracy: 0.9325 - val_loss: 0.2161 - val_categorical_accuracy: 0.9367\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2197 - categorical_accuracy: 0.9324 - val_loss: 0.2157 - val_categorical_accuracy: 0.9367\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2192 - categorical_accuracy: 0.9330 - val_loss: 0.2154 - val_categorical_accuracy: 0.9377\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train.reshape(60000,49) # 7x7\n",
    "X_test=X_test.reshape(10000,49)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(49,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,batch_size=128, epochs=200, verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 13us/step\n",
      "Test score: 0.21038885685503483\n",
      "Test accuracy: 0.9357\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is not bad, albeit not as good as our previous attemptes having a deep network find features themselves. To better understand this: when training the network with the full 28x28 pixels, we are letting the network sift through 784 bytes of data to make a decision, which digit an input depicts. In particular, we have no idea what happens inside the network and what it actually ends up looking for. This is different when doing some of the pre-processing ourselves. Specifically, we decided that the majority of the information should be contained in whether a subset of 7x7 \"super-pixels\" are populated or not. We are then training on the reduced feature set that is represented by only 49 bytes - 16 times less.\n",
    "\n",
    "Albeit we might be able to hand-engineer better features than those chosen here, even some that are able to ultimately beat a brute-force neural network approach in terms of accuracy, machine learning has developed tools that have made hand-engineering features obsolete, rather take advantage of neural network architectures that in themselves perform preprocessing that are conducive to image and speech recognition. These are known as <i>convolutional neural networks</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal training parameters\n",
    "\n",
    "Albeit poking around with the different options of the variying machine learning toolkits is a lot easier than hand-coding features, and decisions can partly be informed by a fundamental understanding of the underlying methods (such as provided by this course), what works and what does not still heavily depends on the data actually at hand. For example, the \"batch size\" is a parameter for which only rough guidelines can be given. Often, playing with certain parameters or even systematically sweeping them can provide some intuition. For example, we can easily test the impact of different batch sizes, by training a model with a couple of different ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "batch_sizes=[8,16,32,64,128,256,512]\n",
    "#batch_sizes=[128,256,512]\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES,input_shape=(784,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['categorical_accuracy'])\n",
    "\n",
    "score=np.zeros((len(batch_sizes),2))\n",
    "times=np.zeros((len(batch_sizes),2))\n",
    "\n",
    "for I, batch_size in enumerate(batch_sizes):\n",
    "    t=time.time()\n",
    "    history = model.fit(X_train, Y_train,batch_size=batch_size, verbose=0, epochs=20, validation_split=VALIDATION_SPLIT)\n",
    "    score[I] = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    times[I] = time.time()-t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuYXXV97/H3d9/mfslkJjNjEjIhCSQR5WIaQK1yqxVQwUrFSyvHch56sUqPiorHU2pPRaytVM859RwqWmqtgEAFQWsRg7Q+cplAkJAhMCFAAplkJsncb/vyPX/slTBMdpKdyey99sz+vJ5nP3uvtX97re9vGPKZtX5r/5a5OyIiItNFwi5ARERKkwJCRERyUkCIiEhOCggREclJASEiIjkpIEREJCcFhIiI5KSAEBGRnBQQIiKSUyzsAo5Hc3Ozd3R0hF2GiMicsnHjxj53bzlauzkdEB0dHXR2doZdhojInGJmL+bTTqeYREQkJwWEiIjkpIAQEZGcFBAiIpKTAkJERHIqy4DY/WIXP714PXt2bA27FBGRklWWAfHYDZ9hyfNDPHr9p8MuRUSkZM3p70EcqydPWUMiBSuC5RUbuulavYbJGJy6uSvU2kRESk3BjiDM7NtmtsfMNk9Z12Rm95vZc8HzgmC9mdk3zKzbzH5tZmcUoqa2++5i2/rFTASxOBGDbeuX0P6THxZidyIic1ohTzH9I/DOaes+Bzzg7quAB4JlgAuBVcHjKuCbhSioddkaqKkinobJKMTTQE0li5aeXIjdiYjMaQULCHd/CNg3bfUlwC3B61uAS6es/yfPehhoNLP2QtRl+wfZ8hstVFy4j61nL8b2DxZiNyIic16xxyBa3X1X8LoHaA1eLwZ2TGm3M1i3i1l28a2/4Olf3sfq+z9E6tJPcMpb3zPbuxARmRdCu4rJ3R3wY/2cmV1lZp1m1tnb2zujfVfWLwQgObJ/Rp8XESkHxQ6I3QdOHQXPe4L1LwNLp7RbEqw7hLvf5O7r3H1dS8tRZ6vNqaYhCIhhBYSIyOEUOyDuAa4IXl8B3D1l/UeCq5nOAgamnIqadQcCIjOmgBAROZyCjUGY2feBc4BmM9sJXAfcANxuZlcCLwLvD5r/GLgI6AZGgY8Wqi6A2rpGUh7BxwYKuRsRkTmtYAHh7h88zFvn52jrwMcKVct0FokwZDVEJhQQIiKHU5ZTbQCMWA1RBYSIyGGVbUCMReuIJ/UdCBGRwynbgBiP1lGZGgq7DBGRklW2ATEZr6cqo4AQETmcsg2IVKKemsxw2GWIiJSssg2ITEUjdT6CZzJhlyIiUpLKNiCoaiRhKcbHRsKuRESkJJVtQESqGgAY6u8LuRIRkdJUtgERq1kAwOjg3pArEREpTWUbEInaJgDGFBAiIjmVbUBUBAExMTT9nkYiIgJlHBDVuieEiMgRlW1A1DY2A5AeVUCIiORSvgHRkD3F5GP9IVciIlKayjYgYvEEw16FjSsgRERyKduAABi2WqITmtFVRCSXsg6IkWgtMU35LSKSU1kHxHi0lgoFhIhITmUdEJOxeqrSmtFVRCSXsg6IZKKBat0TQkQkp7IOiExFPXWuIwgRkVzKOiC8spFqmyA5ORF2KSIiJaesAyJS1Qhoym8RkVzKOiCi1dmAGBlQQIiITFfWARGvDe4JMaApv0VEpivrgKiozc7oOjGsCftERKYr64CoCqb8nhzRPSFERKYr64CoCQIirXtCiIgcoqwDonZB9p4QGU35LSJyiLIOiMqqGsY9jikgREQOUdYBATBsNUQmBsIuQ0Sk5JR9QIxE6jTlt4hIDqEEhJn9NzN72sw2m9n3zazSzJab2SNm1m1mt5lZohi1jEXrSCggREQOUfSAMLPFwCeAde5+ChAFPgB8BbjR3VcC+4Eri1HPRKyWypRmdBURmS6sU0wxoMrMYkA1sAs4D7gjeP8W4NJiFJKM11OV0YyuIiLTFT0g3P1l4G+Al8gGwwCwEeh391TQbCewuBj1pBMN1GrKbxGRQ4RximkBcAmwHHgdUAO88xg+f5WZdZpZZ29v73HX45WN1PkomXT6uLclIjKfhHGK6QJgu7v3unsSuAt4C9AYnHICWAK8nOvD7n6Tu69z93UtLS3HX01VAxFzhgb1bWoRkanCCIiXgLPMrNrMDDgf2AJsAC4L2lwB3F2MYiLV2RldRzSjq4jIa4QxBvEI2cHox4GnghpuAj4LfNLMuoGFwM3FqCde0wTAqO4JISLyGrGjN5l97n4dcN201c8D64tdS6ImewQxPqQjCBGRqcr+m9SVwYyuSc3oKiLyGmUfEDUNQUDopkEiIq+hgAgCIjOmgBARmarsA6K2rpGUR/AxzegqIjJV2QeERSIMacpvEZFDlH1AAIxYLVEFhIjIaygggLForab8FhGZRgEBjEfrqNCU3yIir6GAACbj9VRlFBAiIlMpIIBUop4a3RNCROQ1FBBApqKROh/BM5mwSxERKRkKCICqRhKWYnxsJOxKRERKhgICiFQ1ADDUrxldRUQOUEAAsWBG19FBzegqInKAAgJI1GbvCTGmgBAROUgBAVQEATExtC/kSkRESocCAqg+cE+IYQWEiMgBCgigtrEZgPRYf8iViIiUDgUEUNuQPcXkCggRkYMUEEAsnmDYq7BxBYSIyAEKiMCw1RKd0IyuIiIHKCACO5PV9N63mT07toZdiohISVBABF7cmmHZzgyPXv/psEsRESkJsbALCNuTp6whkYK1wfKKDd10rV7DZAxO3dwVam0iImEq+yOItvvuYtv6xUwEUTkRg23rl9D+kx+GW5iISMjKPiBal62BmiriaZiMQjwN1FSyaOnJYZcmIhKqsj/FBGD7B9myroU3tG5ly75TqNivq5lERBQQwMW3/oIXujrpuO18hs+5nHXvuirskkREQlf2p5gOaOtYQ8aN5J7nwi5FRKQkKCAClVU17LZm4v3Ph12KiEhJOOIpJjNrymMbGXefF3NU9FUspX70pbDLEBEpCUcbg3gleNgR2kSBE2atohCN1nWwrO/f8EwGi+jgSkTK29ECosvdTz9SAzN74lh3amaNwLeAUwAH/gDYCtwGdAAvAO939/3Huu3j4U0rqO8bZW/vKyxsXVLMXYuIlJyj/Zl8dh7byKfNdF8H/s3dVwOnAl3A54AH3H0V8ECwXFRV7dnvPvS+sKXYuxYRKTlHDAh3HwcwsxVmVhG8PsfMPhEcBRxsky8zawDeBtwcfH4yGMO4BLglaHYLcOmxbHc2LFy6BoDBV54p9q5FREpOvifa7wTSZrYSuAlYCvzLDPe5HOgFvmNmT5jZt8ysBmh1911Bmx6gNdeHzewqM+s0s87e3t4ZlpBb27KTSHqUdG/3rG5XRGQuyjcgMu6eAt4L/C93vwZon+E+Y8AZwDeD8Y0Rpp1OcncnOzZxCHe/yd3Xufu6lpaWGZZwmMLiCXZF26gY2D6r2xURmYvyDYikmX0QuAK4N1gXn+E+dwI73f2RYPkOsoGx28zaAYLnPTPc/nHZV7GUBWO61FVEJN+A+CjZwegvuft2M1sOfHcmO3T3HmCHmR2YDe98YAtwD9kAIni+eybbP17j9ctpS79CJp0OY/ciIiUjr7mY3H0L8Ikpy9uBrxzHfj8OfM/MEsDzZAMoAtxuZlcCLwLvP47tz5g1r6Rq9yQ9r2ynbenKMEoQESkJRzyCMLObjraBfNpM5+6bgnGEN7r7pe6+3933uvv57r7K3S9w933Hut3ZUNN+EgB9Lzwdxu5FRErG0Y4gLjWzI13GasC5s1hP6Fo6Xg/AyK5nQ65ERCRcRwuIa/LYxn/MRiGloqW9g1GvwPfqUlcRKW9HDAh3v+VI789HkWiUnmg7VYO61FVEyptmpMuhv/oEmsZ3hF2GiEioFBA5TNQvpy2zm+TkRNiliIiE5pgCwsyqC1VIKYm2rCJuaXpe0kC1iJSvvALCzN5sZluAZ4LlU83s7wtaWYjqF68GYN9LXSFXIiISnnyPIG4EfhvYC+DuT5KdkXVeWtSxFoCxnq0hVyIiEp68TzG5+/RR23k7F8WC5nYGqcF0qauIlLG8ptogO3fSmwE3szhwNdmb/MxLFonQE1tM9fCLYZciIhKafI8g/gj4GLAYeBk4LVietwarT6BlYmfYZYiIhCbfyfr6gA8XuJaSkmxcwaKBBxgfHaayujbsckREii6vgAim9/440DH1M+7+nsKUFb546yoiLzm7Xuhi+drfCLscEZGiy3cM4odk7yH9IyBTuHJKR8Pi1fAY9O/YAgoIESlD+QbEuLt/o6CVlJi25dlZXcd360omESlP+QbE183sOuDfgYPzT7j74wWpqgTUNTTRRyPRfQoIESlP+QbEG4DfB87j1VNMHizPW3viS6gb0aWuIlKe8g2I3wVOdPfJQhZTaoZrl3Hi/l+GXYaISCjy/R7EZqCxkIWUovSCFTTTz2D/3rBLEREpunyPIBqBZ8zsMV47BjFvL3MFqGg9CZ6H3dufpv70eTv1lIhITvkGxHUFraJENZ2wBn4FAy8/AwoIESkz+X6T+heFLqQUtXWsIeNGcs9zYZciIlJ0RxyDMLP/DJ6HzGxwymPIzAaLU2J4Kqtq2G3NxPufD7sUEZGiO9oRRA2Au9cVoZaS1FexlPrRl8IuQ0Sk6I52FZMXpYoSNlrXQVtqJ54pixlGREQOOtoRxCIz++Th3nT3r81yPSXHm1ZQ3zfK3t5XWNi6JOxyRESK5mhHEFGgFqg7zGPeq2o/GYDeF7aEXImISHEd7Qhil7v/ZVEqKVELl64BYPCVZ4B3hFuMiEgRHe0IwopSRQlrW3YSSY+S7tWkfSJSXo4WEOcXpYoSFosn6Im0UjGwPexSRESK6ogB4e77ilVIKdtbeQILxnSpq4iUl3wn65t1ZhY1syfM7N5gebmZPWJm3WZ2m5klwqptuvH65bSlXyGTToddiohI0YQWEMDVQNeU5a8AN7r7SmA/cGUoVeVgC1dQZZPseUWnmUSkfIQSEGa2BLgY+FawbGRvPnRH0OQW4NIwasul5nXZS137Xng65EpERIonrCOIvwM+w6t3p1sI9Lt7KljeCSwOo7BcWjqy96ce2fVsyJWIiBRP0QPCzN4F7HH3jTP8/FVm1mlmnb29vbNcXW4t7R2MeQLfq0tdRaR8hHEE8RbgPWb2AnAr2VNLXwcazezAF/eWAC/n+rC73+Tu69x9XUtLSzHqJRKNsiv6OqoGNQYhIuWj6AHh7te6+xJ37wA+APzc3T8MbAAuC5pdAdxd7NqOpL/6BJrGd4RdhohI0YR5FdN0nwU+aWbdZMckbg65nteYqF9OW2Y3ycmJozcWEZkH8r3laEG4+4PAg8Hr54H1YdZzJNGWVcRfSbPjpWdZuvINYZcjIlJwpXQEUdLqF68GYN9LXUdpKSIyPygg8rSoYy0AYz1bQ65ERKQ4FBB5WtDcziA1mC51FZEyoYDIk0Ui9MQWUz38YtiliIgUhQLiGAxWn0DLxM6wyxARKQoFxDFINq5gkfcxPjocdikiIgWngDgG8UUr6U1G2fA7b2fPDg1Wi8j8poA4Bg1L1vDYs02c8OIoj17/6bDLEREpqFC/KDeXPHnKGhIpWBH8yFZs6KZr9RomY3DqZn03QkTmHx1B5KntvrvYtn4xE0GkTsRg2/oltP/kh+EWJiJSIAqIPLUuWwM1VcTTMBmFeBqoqWTR0pPDLk1EpCB0iukY2P5Btp+7iuiCfpLP9GB9+8MuSUSkYBQQx+DiW38BwI7nnqTtn8/l8eazQq5IRKRwdIppBpauOpWNi36HdX33sH3LY2GXIyJSEAqIGVp9+V8xYtUM3XNt2KWIiBSEAmKGGpvb2LLqD3nj+GP8+sE7wy5HRGTWKSCOw+nvu4aXrZW6h75IOpUKuxwRkVmlgDgOFZXV7D7z8yzPvMjGH34j7HJERGaVAuI4nf6Oj9AVfz0nbv46w4O67FVE5g8FxHGySITohdfTTD9P3fbFsMsREZk1CohZcNIZ59BZfwGn7/xnenbojnMiMj8oIGbJ4vddjwE7f/C5sEsREZkVCohZ0r7sZB5f/CHWDd7Pc088FHY5IiLHTQExi065/C/YSwPJn3wez2TCLkdE5LgoIGZRXUMT3Ws/ztrJp3ji/u+FXY6IyHFRQMyyN733al6ILGXRw19icmI87HJERGZMATHLYvEEA795HUt8F4/f+TdhlyMiMmMKiAJ449vfx1MVZ7Dm2b9nYO/usMsREZkRBUQBWCRC7XtuoNZH6br9z8MuR0RkRhQQBbL89Weyselizuj5ATu7N4ddjojIMVNAFNCJl3+ZFDH2/Ku+PCcic48CooCa207gyY6PcsbIf7Dl4X8LuxwRkWNS9IAws6VmtsHMtpjZ02Z2dbC+yczuN7PngucFxa6tEE57/xfYQxPxn32BTDoddjkiInkL4wgiBXzK3dcCZwEfM7O1wOeAB9x9FfBAsDznVdXU8eLp17Aq9RyP3/cPYZcjIpK3ogeEu+9y98eD10NAF7AYuAS4JWh2C3BpsWsrlDe96w95LrqSJY9/lfHR4bDLERHJS6hjEGbWAZwOPAK0uvuu4K0eoDWksmZdJBpl8vz/SRt9bLr9+rDLERHJS2gBYWa1wJ3An7n74NT33N0BP8znrjKzTjPr7O3tLUKls+P1b76IJ6rfwhu230xfz46wyxEROapQAsLM4mTD4XvuflewereZtQfvtwN7cn3W3W9y93Xuvq6lpaU4Bc+S5vd+mQRJtt3++bBLERE5qjCuYjLgZqDL3b825a17gCuC11cAdxe7tkJbuupUNra+j3V7f8T2LY+FXY6IyBGFcQTxFuD3gfPMbFPwuAi4AfgtM3sOuCBYnnfWXP5XjFg1Q/dcG3YpIiJHFCv2Dt39PwE7zNvnF7OWMDQsbOXhVX/EWc/9Lb9+8E7eeM77wi5JRCQnfZM6BGdc9hl2Whup+6/jpxetZ8+OrWGXJCJyCAVECBIVlew581pe2TrBku1DPHr9p8MuSUTkEEU/xSTw5ClrqErBiuDHv2JDN12r1zAZg1M3d4VcnYhIlo4gQtB2311sW7+YiSCeJ2Kw7aQUre/u5enr38ajd3xNNxoSkdApIELQumwN1FQRT8NkDOJpSLYsZ/uqj1KX6mP95i9S9Y01bPrrd9J5702MDg+EXbKIlCEFREhs/yDbz11Fxc03sv3cVSRGJjj7D77K0i9spvu99/F4+wd43ehW1nVeA19dxca/fS+bfvZ9JifGwy5dRMqEZWe1mJvWrVvnnZ2dYZdRMJl0mmce/XeGOm/lpL0PsIAhBqhha9N5VL/pctaceSHRmIaRROTYmNlGd1931HYKiLkhOTnBlv+8m+Sm21k78BDVNkEvC9jW+ts0nfkhVp32m1hEB4QicnQKiHlsbGSILb+4ncjmO3n9yCMkLMVOa2fn4otof+vvsWz1GWGXKCIlTAFRJgb29/Hshu9RufVfWTu+iag526LL6V32bjrO+QhtJ6wKu0QRKTEKiDLU1/MS3Ru+y4Jtd3NyKvvt7K74WgZXXsqqc3+PpkWLQ65QREqBAqLMvfx8Fzse+i5tL/2IjsxLpDzClqozmFj9O6w+94PUNTSFXaKIhEQBIQdtf/oRen75z3S88hPa6WXc42ypPRt742WsedtlVFbVhF2iiBSRAkIO4ZkMWzf+nIFH/oWVfT9jIQMMeRXPNL6dyjMuZ82b30UsngBg94tdbPqTKzj9/36XRUtPDrlyEZlNCgg5olRykq5f3cf447exev+D1NkYe2mgu/l8GtZ/iO7vfIPlD3az/ZyVvOubPwq7XBGZRQoIydv42AhdD92BP3UH0e90k0gf2mYyCskvf4JE/SKqF7RS29RGY3O7Tk+JzEEKCJmR57c8wqY//zgnPjNERSo7keDOE9OsP2UvixKpQ9qPegX9kQaGo42MxRuZTDSRrloI1QuJ1LWQqF9EVcMiapvaqV/YSm1do77QJxKyfANC8zTIa5y49ky2tLQSf3ro4ESCvvhkKj79bXbsfYWRfbsZG9hDcnAPmeE+GO0jNr6XxOR+aib30jr2PI39A1RaMuf2JzzOgNUxFG1kNNbIREUTqcomvHohkZoW4vWLqGpcRM2CVuqb2qhf0EIkGi3yT0FEQAEhORyYSHDlFX9C9y1/T2TfAA1NLTQ0teT1ec9kGBkZZKCvh+H9PYz19zAx0EtmuBdG+oiO7yM+sZ+q5H6ahl6mYWCQWhvLua2UR9hvdQxFGhiJNTKRWECyciGZqoVEapuJ1bVQ0bCImgVt1DW10riw7eBAu4gcH51ikpIwPjbCwN4ehvb2MNa/m4mBPaSGe/GRvUTG9pKY2Efl5H5q0v3UZwZoZPiw2xqghkFrYDjWyHh8AcnK7Gkvq2kmVpsNlKrGRbM+jqIrv2Su0CkmmVMqq2qoXLKC1iUr8mqfSk7Sv3c3Q/t6GNnfw8RAH6mhPWRG+oiM9hEbzwZK4/hO6kafptEHiVkm57ZGvJKBSAPD0QbG4guYrGgiXdkENc1EaltI1LccHEdpaG6jprYh5zjKYzd8huXPZ28hqyu/ZD7QEYSUhUw6zVB/H4P7eo44jlKd7Kcu3U+jD1JxhHGUfqvPjqPEG/Hv7jrslV/pv/0skVgFkViCSDxBNJYgGq8gGksQi1cQjVcQS2Qf8eB1PFFJPJ7QYL4UjI4gRKaIRKM0LGylYWFrXu3zGUdJTOyjMtnPxCVpujcbS5+PTLnyK8X6U/ax6OGrZ1zzpEdJESNpMZLESREjZTHSFidlMTLB67TFSEfiZCxOJhLHI68+ezQRPMchmjj4sGgciyWwaCL7HEsQicZfG2ZBgCnMypcCQiQHi0SoqWukpq4Rlq8+avuX/vjdxJ/rnnIL2ROZ+OgdbE9Okk5OkEpOkElOkk5OkklNkEllnz01iacmyaQm8fQkpCchlTz42jKp4DmJpSeJZJJYJkkkkyTi2eeop6hIjRD17OuYJ4mSIuapbKx4Nl4SpIhYYc4YTA+zNFFSFiNlcdJkA+y4wixYZ7FENsBiibIOs2KNdykgRGbB9Cu/EvsGWLrq1LDLOkQ6lSKZnCA5OUFqMhtcyckJ0slxUmUSZkmPkgyOxpLEDh6ZzTjMIgcC7dWjspmGWSyWCMIsG2yHu8S7WONdGoMQkZJUCmEWzSRLKsx2/qAm93hXDE7d3JX3djUGISJzWjQWIxqLzYnpXIoVZkMf7mfw0e2c2J18dbzrjCWs/9L/Lki/FBAiIsepmGF27x+/m/jWV8e7qKks2DjE3B+tEREpIwfGuypuvpHt567C9g8WbF86ghARmUMuvvUXB1+vPvOdBd2XjiBERCQnBYSIiORUUgFhZu80s61m1m1mnwu7HhGRclYyAWFmUeD/ABcCa4EPmtnacKsSESlfJRMQwHqg292fd/dJ4FbgkpBrEhEpW6UUEIuBHVOWdwbrXsPMrjKzTjPr7O3tLVpxIiLlZs5d5uruNwE3AZhZr5m9mMfHmoG+ghZWesqtz+XWXyi/Pqu/s2dZPo1KKSBeBpZOWV4SrDssd8/rHphm1pnPvCPzSbn1udz6C+XXZ/W3+ErpFNNjwCozW25mCeADwD0h1yQiUrZK5gjC3VNm9qfAT4Eo8G13fzrkskREylbJBASAu/8Y+HEBNn1TAbZZ6sqtz+XWXyi/Pqu/RTan7wchIiKFU0pjECIiUkLmfUDM1+k7zOzbZrbHzDZPWddkZveb2XPB84JgvZnZN4Kfwa/N7IzwKp8ZM1tqZhvMbIuZPW1mVwfr52WfzazSzB41syeD/n4xWL/czB4J+nVbcEEHZlYRLHcH73eEWf9MmVnUzJ4ws3uD5fne3xfM7Ckz22RmncG6kvmdntcBMc+n7/hHYPpcv58DHnD3VcADwTJk+78qeFwFfLNINc6mFPApd18LnAV8LPhvOV/7PAGc5+6nAqcB7zSzs4CvADe6+0pgP3Bl0P5KYH+w/sag3Vx0NTD13pnzvb8A57r7aVMuaS2d32l3n7cP4Gzgp1OWrwWuDbuuWexfB7B5yvJWoD143Q5sDV7/P+CDudrN1QdwN/Bb5dBnoBp4HDiT7BenYsH6g7/fZK/+Ozt4HQvaWdi1H2M/l5D9B/E84F7A5nN/g9pfAJqnrSuZ3+l5fQRBntN3zCOt7r4reN0DtAav59XPITidcDrwCPO4z8Hplk3AHuB+YBvQ7+6poMnUPh3sb/D+ALCwuBUft78DPgNkguWFzO/+Ajjw72a20cyuCtaVzO90SV3mKrPH3d3M5t0lamZWC9wJ/Jm7D5rZwffmW5/dPQ2cZmaNwL8Cq0MuqWDM7F3AHnffaGbnhF1PEb3V3V82s0XA/Wb2zNQ3w/6dnu9HEMc8fccct9vM2gGC5z3B+nnxczCzONlw+J673xWsntd9BnD3fmAD2VMsjWZ24A+7qX062N/g/QZgb5FLPR5vAd5jZi+Qncn5PODrzN/+AuDuLwfPe8j+EbCeEvqdnu8BUW7Td9wDXBG8voLsefoD6z8SXAVxFjAw5RB2TrDsocLNQJe7f23KW/Oyz2bWEhw5YGZVZMdbusgGxWVBs+n9PfBzuAz4uQcnqucCd7/W3Ze4ewfZ/09/7u4fZp72F8DMasys7sBr4B3AZkrpdzrsQZoiDAJdBDxL9vztfw+7nlns1/eBXUCS7LnIK8meg30AeA74GdAUtDWyV3NtA54C1oVd/wz6+1ay52t/DWwKHhfN1z4DbwSeCPq7GfjzYP2JwKNAN/ADoCJYXxksdwfvnxh2H46j7+cA9873/gZ9ezJ4PH3g36dS+p3WN6lFRCSn+X6KSUREZkgBISIiOSkgREQkJwWEiIjkpIAQEZGcFBBSlswsHcyg+aSZPW5mbz5K+0Yz+5M8tvugmc3oPsJm9uMD330QKQUKCClXY56dQfNUspM4fvko7RuBowbE8XD3izz7rWmRkqCAEIF6slNJY2a1ZvZAcFTxlJldErS5AVgRHHV8NWj72aDNk2Z2w5Tt/W5wL4dnzew3p+/MzNrN7KFgW5sPtAnuDdBsZn8UvLfJzLab2Ybg/XeY2a+C2n4QzEslUjD6opxdYJzGAAAB/0lEQVSUJTNLk/02aiXZKZXP8+xEcTGg2rMTATYDD5Odf38Z2W/3nhJ8/kLgfwAXuPuomTW5+z4zexDY6O6fMrOLgE+6+wXT9v0poNLdvxTcs6Ta3YeCeYjWuXtf0C4O/Bz4a+BXwF3Ahe4+YmafJfut4r8s5M9Jyptmc5VyNebupwGY2dnAP5nZKWSnM7jezN5Gdtrpxbw63fJUFwDfcfdRAHffN+W9AxMJbiR7z47pHgO+HQTAD91902Fq/DrZOYZ+FMx2uhb4ZTCDbYJsaIgUjAJCyp67/yo4WmghO79TC/Amd08Gf9VXHuMmJ4LnNDn+H3P3h4IAuhj4RzP7mrv/09Q2ZvZfyB61/OmBVcD97v7BY6xFZMY0BiFlz8xWA1Gy00U3kL0vQdLMziX7jzTAEFA35WP3Ax81s+pgG03HsL9lwG53/wfgW8AZ095/E/Bp4Pfc/cDNcx4G3mJmK4M2NWZ20rH1VOTY6AhCylVVcLc2yP51foW7p83se8CPzOwpoBN4BsDd95rZL81sM/ATd7/GzE4DOs1sEvgx8Pk8930OcI2ZJYFh4CPT3v9ToAnYEJxO6nT3/xocVXzfzCqCdl8gO1OxSEFokFpERHLSKSYREclJASEiIjkpIEREJCcFhIiI5KSAEBGRnBQQIiKSkwJCRERyUkCIiEhO/x9uNggSRawITwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batch_sizes,score[:,1])\n",
    "plt.plot(batch_sizes,score[:,1],'*')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.xlabel('Batch size')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(batch_sizes,times)\n",
    "plt.plot(batch_sizes,times,'*')\n",
    "plt.ylabel('Time [s]')\n",
    "plt.xlabel('Batch size')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above experiment, that the batch size actually has an optimum. If too low, the gradient jumps back and forth for every new training instance, if too high, the gradient combines too many experiences into one. We also observe, however, that training becomes much, much slower with smaller batch sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "- Coding features by hand as much as possible has been the standard approach to design a machine learning algorithm for many decades. The idea has been that hand-coding leverages as much human expert knowledge as possible, while reducing the data sufficiently to make training a neural network actually tractable. \n",
    "- The ability to train also larger networks efficiently, has allowed to create \"deep\" neural networks that can get very good performance, sometimes even exceeding those of solutions relying on hand-coded features, without much tuning whatsoever.\n",
    "- We have introduced a new feature without much ado, using a part of the test set for validation. This can help the training algorithm in many ways, for example detecting overfitting, stopping once a certain performance on valiation is met, and others.\n",
    "- Albeit the validation set can help to determine the optimal number of epochs, that is the number of times a dataset is presented to the network, the \"batch size\" remains a free parameter. If too low, weights are updated for every training instance, which is very susceptive to noise. If too high, the gradient is averaged over too many instances at once, ignoring possibly important information such as particularly salient examples.  \n",
    "- Overfitting a neural network during training shows that the network is actually capable of modeling the entire dataset. If this is not possible, the network architecture might be too simple to capture the data's non-linearities, requiring additional nodes or layers. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
