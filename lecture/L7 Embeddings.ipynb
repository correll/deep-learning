{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Images: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have almost exclusively used images. We have classified them and we have generated them. How about text? At the end, a text is a time-series of ASCII characters, and we could surely train pattern detectors and generators on them. Grammar is a lot less forgiving then graphics, however, and a slightly different shade of gray that might go unobserved in an image, might make a text unreafable (unreadable). In some applications such as translation, text understanding or generation it might therefore make sense to rely on words, not characters, as the smallest digestible unit. A naive approach would be to create a dictionary in which words are represented by integer values. These in turn can be represented by one-hot encoding:\n",
    "\n",
    "- the   1 00000001\n",
    "- quick 2 00000010\n",
    "- brown 3 00000100\n",
    "- fox   4 00001000\n",
    "- jumps 5 00010000\n",
    "- over  6 00100000\n",
    "- lazy  7 01000000\n",
    "- dog   8 10000000\n",
    "\n",
    "This is costly, as vectors get very long. (Remember, softmax activation requires to sum over all outputs, e.g.) In particular, $N$-dimensional integers are turned into $N$ dimensional vector space.  \n",
    "\n",
    "One way to reduce the dimensionality of one-hot encoding would be to project data into a lower dimensional vector space. This is known as <i>embedding</i> and is a standard layer in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "In its most simplest form, embedding $N$ values into $M$-dimensional vectors could be done by generating $N$ random $M$-dimensional vectors. The embedding layer would simply serve as a look-up table. This can be seen in the keras code below. In our example, $N=8$ and $M=3$ would generate a 3-dimensional embedding. Here <code>input_length</code> refers to the number of integers that will be provided to the embedding layer at a time, here one integer results in three values, two integers into six and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=8, output_dim=3, input_length=1)\n",
    "model.add(embedding_layer)\n",
    "model.compile('adam', 'mse') # will need to provide an optimizer and loss, even though not used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.03080323 -0.00436597 -0.03693813]]]\n",
      "[[[ 0.03868173 -0.04781219 -0.034648  ]]]\n",
      "[[[ 0.04200144 -0.04283803  0.00865404]]]\n",
      "[[[ 0.02593631 -0.02306879  0.02608968]]]\n",
      "[[[ 0.00375088 -0.01421449 -0.01163588]]]\n",
      "[[[-0.02737376 -0.00066825  0.02329269]]]\n",
      "[[[ 0.03849734  0.01492298 -0.01914817]]]\n",
      "[[[ 0.02770283 -0.03706833  0.02405379]]]\n"
     ]
    }
   ],
   "source": [
    "def print_embedding_weights():\n",
    "    for I in range(8):\n",
    "        input_data = np.array(I).reshape(1,1) # turn into one sample with one data point\n",
    "        #input_data = np.array([1,2]).reshape(1,2) # example for input length two\n",
    "\n",
    "        pred = model.predict(input_data)\n",
    "        print(pred)\n",
    "\n",
    "print_embedding_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"one-hot\" encoding can also be an embedding, allowing us to integrate the preprocessing step into the network itself. This can be seen when manually defining the weights of the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "[[[0. 1. 0. 0. 0. 0. 0. 0.]]]\n",
      "[[[0. 0. 1. 0. 0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0. 1. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0. 0. 1. 0. 0.]]]\n",
      "[[[0. 0. 0. 0. 0. 0. 1. 0.]]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "embedding_lookup = np.array([\n",
    "    [1,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,1],\n",
    "])\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=8, output_dim=8, input_length=1)\n",
    "model.add(embedding_layer)\n",
    "model.compile('adam', 'mse')\n",
    "\n",
    "embedding_layer.set_weights([embedding_lookup])\n",
    "\n",
    "print_embedding_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear now how an array of words can be mapped to an integer number (the index into the array). We can also see how to find the index of any given vector. When using one-hot encoding, we use a softmax activation layer and then find the index that yields the highest entry in the softmax vector (arg max). When searching over vectors consisting of random numbers, this step is a little more involved, requiring us to find the vector with the least distance, for example using the cosine distance (dot product), given by\n",
    "\n",
    "$$  \\cos(\\theta)=\\frac{AB}{\\|A\\|\\|B\\|} $$\n",
    "\n",
    "for the vectors $A$ and $B$. When vectors are the same, the angle inbetween them is zero. If they are orthogonal, the angle is 90 degrees and they are very dissimilar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "\n",
    "Assuming we have a neural network that is able to generate words, the cosine distance will allow us finding the closest word in the dictionary. It would therefore be advantagous if the entire embedding would be organized so that closely related words would have similar distances. Having such a corpus would not allow representing texts in a lower dimensional space that can be processed by a standard neural network architecture, but also provide a natural way to capture different expressions that mean the same thing. Here, we can take advantage of the fact that embeddings can also be trained to minimize a loss function that is added to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique that does this, is known as <i>word2vec</i>. Instead of manually annotating similarities between words and thinking up a coding scheme, we smartly design a neural network that generates the desired result - a $M$-dimensional vector that represents each one of $N$ words - as a side-effect of training an appropriate problem. In other words, we take advantage of the fact that dense neural networks automatically create internal representations that we can literally scoop off.\n",
    "\n",
    "One such a network, known as the <i>skip-gram</i> word2vec model, is a model that predicts words that could, or often are, surrounding another word. For example, the sentence\n",
    "\n",
    "<i>The quick brown fox jumps over the lazy dog</i>\n",
    "\n",
    "creates the following associations\n",
    "\n",
    "- the <-> quick\n",
    "- quick <-> brown\n",
    "- brown <-> fox\n",
    "- fox <-> jumps\n",
    "\n",
    "and so on. We can also train a classifier, that would tell us, whether one word is likely to be next to another by creating the following training set\n",
    "\n",
    "- ((The, quick),1)\n",
    "- ((quick, brown),1)\n",
    "- ((brown, fox),1)\n",
    "\n",
    "We would also need to create negative examples, for example by adding random words from this or another corpus:\n",
    "\n",
    "- ((quick, dog),0)\n",
    "- ((brown, dog), 0)\n",
    "- ((quick, zebra), 0)\n",
    "\n",
    "The goal is now to train a network so that words that are likely to follow each other get a high score (1), and words that are never in the same context get a very low score (0). One way to accomplish tis is to design a network that uses the <i>same</i> embedding to encode both word and context word. Words (word and context) are embedded into a 300-dimensional row vector and reshaped into a 300-dimensional column vector in parallel. A so-called <a href=\"https://keras.io/layers/merge/\">merge layer</a>, here the dot product, then normalizes the two data streams and combines them into a single scalar. The result is that the dot-product layer effectively compares input and context word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 3)         30          input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 3, 1)         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_24 (Reshape)            (None, 3, 1)         0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_9 (Dot)                     (None, 1, 1)         0           reshape_23[0][0]                 \n",
      "                                                                 reshape_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 1)            0           dot_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            2           reshape_25[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 32\n",
      "Trainable params: 32\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"611pt\" viewBox=\"0.00 0.00 433.51 458.00\" width=\"578pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 454)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-454 429.5107,-454 429.5107,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 5544422648 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>5544422648</title>\n",
       "<polygon fill=\"none\" points=\"0,-405.5 0,-449.5 203.5107,-449.5 203.5107,-405.5 0,-405.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"39.0967\" y=\"-423.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"78.1934,-405.5 78.1934,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.0278\" y=\"-434.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"78.1934,-427.5 133.8623,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.0278\" y=\"-412.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"133.8623,-405.5 133.8623,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.6865\" y=\"-434.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"133.8623,-427.5 203.5107,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.6865\" y=\"-412.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5544422704 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>5544422704</title>\n",
       "<polygon fill=\"none\" points=\"102.8242,-324.5 102.8242,-368.5 322.6865,-368.5 322.6865,-324.5 102.8242,-324.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.0967\" y=\"-342.3\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"183.3691,-324.5 183.3691,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211.2036\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"183.3691,-346.5 239.0381,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211.2036\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"239.0381,-324.5 239.0381,-368.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.8623\" y=\"-353.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"239.0381,-346.5 322.6865,-346.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.8623\" y=\"-331.3\">(None, 1, 3)</text>\n",
       "</g>\n",
       "<!-- 5544422648&#45;&gt;5544422704 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>5544422648-&gt;5544422704</title>\n",
       "<path d=\"M132.0866,-405.3664C145.0569,-395.9016 160.3539,-384.7389 174.121,-374.6926\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"176.446,-377.3288 182.4608,-368.6068 172.3197,-371.6743 176.446,-377.3288\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5544422592 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>5544422592</title>\n",
       "<polygon fill=\"none\" points=\"222,-405.5 222,-449.5 425.5107,-449.5 425.5107,-405.5 222,-405.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261.0967\" y=\"-423.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"300.1934,-405.5 300.1934,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"328.0278\" y=\"-434.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"300.1934,-427.5 355.8623,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"328.0278\" y=\"-412.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"355.8623,-405.5 355.8623,-449.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390.6865\" y=\"-434.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"355.8623,-427.5 425.5107,-427.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390.6865\" y=\"-412.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5544422592&#45;&gt;5544422704 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>5544422592-&gt;5544422704</title>\n",
       "<path d=\"M293.4241,-405.3664C280.4538,-395.9016 265.1568,-384.7389 251.3897,-374.6926\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"253.191,-371.6743 243.0499,-368.6068 249.0647,-377.3288 253.191,-371.6743\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5544423152 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>5544423152</title>\n",
       "<polygon fill=\"none\" points=\"1.3828,-243.5 1.3828,-287.5 204.1279,-287.5 204.1279,-243.5 1.3828,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"33.0967\" y=\"-261.3\">Reshape</text>\n",
       "<polyline fill=\"none\" points=\"64.8105,-243.5 64.8105,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.645\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64.8105,-265.5 120.4795,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.645\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120.4795,-243.5 120.4795,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.3037\" y=\"-272.3\">(None, 1, 3)</text>\n",
       "<polyline fill=\"none\" points=\"120.4795,-265.5 204.1279,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.3037\" y=\"-250.3\">(None, 3, 1)</text>\n",
       "</g>\n",
       "<!-- 5544422704&#45;&gt;5544423152 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>5544422704-&gt;5544423152</title>\n",
       "<path d=\"M182.6974,-324.3664C169.8439,-314.9016 154.6847,-303.7389 141.0417,-293.6926\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"142.9048,-290.718 132.777,-287.6068 138.7541,-296.3547 142.9048,-290.718\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5544423264 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>5544423264</title>\n",
       "<polygon fill=\"none\" points=\"222.3828,-243.5 222.3828,-287.5 425.1279,-287.5 425.1279,-243.5 222.3828,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.0967\" y=\"-261.3\">Reshape</text>\n",
       "<polyline fill=\"none\" points=\"285.8105,-243.5 285.8105,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.645\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"285.8105,-265.5 341.4795,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.645\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"341.4795,-243.5 341.4795,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383.3037\" y=\"-272.3\">(None, 1, 3)</text>\n",
       "<polyline fill=\"none\" points=\"341.4795,-265.5 425.1279,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383.3037\" y=\"-250.3\">(None, 3, 1)</text>\n",
       "</g>\n",
       "<!-- 5544422704&#45;&gt;5544423264 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>5544422704-&gt;5544423264</title>\n",
       "<path d=\"M243.0866,-324.3664C256.0569,-314.9016 271.3539,-303.7389 285.121,-293.6926\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"287.446,-296.3288 293.4608,-287.6068 283.3197,-290.6743 287.446,-296.3288\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5544422872 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5544422872</title>\n",
       "<polygon fill=\"none\" points=\"82.6104,-162.5 82.6104,-206.5 342.9004,-206.5 342.9004,-162.5 82.6104,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.1104\" y=\"-180.3\">Dot</text>\n",
       "<polyline fill=\"none\" points=\"119.6104,-162.5 119.6104,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147.4448\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"119.6104,-184.5 175.2793,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147.4448\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"175.2793,-162.5 175.2793,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259.0898\" y=\"-191.3\">[(None, 3, 1), (None, 3, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"175.2793,-184.5 342.9004,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.6035\" y=\"-169.3\">(None, 1, 1)</text>\n",
       "</g>\n",
       "<!-- 5544423152&#45;&gt;5544422872 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>5544423152-&gt;5544422872</title>\n",
       "<path d=\"M132.8134,-243.3664C145.6668,-233.9016 160.826,-222.7389 174.4691,-212.6926\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"176.7567,-215.3547 182.7337,-206.6068 172.606,-209.718 176.7567,-215.3547\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5544423264&#45;&gt;5544422872 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>5544423264-&gt;5544422872</title>\n",
       "<path d=\"M293.4241,-243.3664C280.4538,-233.9016 265.1568,-222.7389 251.3897,-212.6926\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"253.191,-209.6743 243.0499,-206.6068 249.0647,-215.3288 253.191,-209.6743\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5544423040 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>5544423040</title>\n",
       "<polygon fill=\"none\" points=\"111.3828,-81.5 111.3828,-125.5 314.1279,-125.5 314.1279,-81.5 111.3828,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.0967\" y=\"-99.3\">Reshape</text>\n",
       "<polyline fill=\"none\" points=\"174.8105,-81.5 174.8105,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.645\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"174.8105,-103.5 230.4795,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.645\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"230.4795,-81.5 230.4795,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272.3037\" y=\"-110.3\">(None, 1, 1)</text>\n",
       "<polyline fill=\"none\" points=\"230.4795,-103.5 314.1279,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272.3037\" y=\"-88.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5544422872&#45;&gt;5544423040 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>5544422872-&gt;5544423040</title>\n",
       "<path d=\"M212.7554,-162.3664C212.7554,-154.1516 212.7554,-144.6579 212.7554,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.2555,-135.6068 212.7554,-125.6068 209.2555,-135.6069 216.2555,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5544423320 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>5544423320</title>\n",
       "<polygon fill=\"none\" points=\"124.6035,-.5 124.6035,-44.5 300.9072,-44.5 300.9072,-.5 124.6035,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"150.0967\" y=\"-18.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"175.5898,-.5 175.5898,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.4243\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"175.5898,-22.5 231.2588,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.4243\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"231.2588,-.5 231.2588,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.083\" y=\"-29.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"231.2588,-22.5 300.9072,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.083\" y=\"-7.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5544423040&#45;&gt;5544423320 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>5544423040-&gt;5544423320</title>\n",
       "<path d=\"M212.7554,-81.3664C212.7554,-73.1516 212.7554,-63.6579 212.7554,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.2555,-54.6068 212.7554,-44.6068 209.2555,-54.6069 216.2555,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "embed_size = 3\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers import dot\n",
    "\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, embed_size, input_length=1, name='embedding') # Define Embedding() only once so that both word and context are using the same embedding. \n",
    "\n",
    "word_embedding = embedding(input_target)\n",
    "word_embedding = Reshape((embed_size, 1))(word_embedding)\n",
    "context_embedding = embedding(input_context)\n",
    "context_embedding = Reshape((embed_size, 1))(context_embedding)\n",
    "\n",
    "# now perform the dot product operation  \n",
    "dot_product = dot([word_embedding, context_embedding], axes=1, normalize=True)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())\n",
    "\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above will find an embedding in which words that are appearing next to another in a text are represented by vectors that have a smaller distance than vectors of words that never appear next to each other. In order to train such a model, we have to generate appropriate skipgrams for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs:  32\n",
      "(over (6), jumps (5)) -> 1\n",
      "(dog (8), lazy (7)) -> 1\n",
      "(brown (3), quick (2)) -> 1\n",
      "(lazy (7), dog (8)) -> 1\n",
      "(the (1), over (6)) -> 1\n",
      "(the (1), quick (2)) -> 0\n",
      "(over (6), brown (3)) -> 0\n",
      "(quick (2), brown (3)) -> 1\n",
      "(fox (4), jumps (5)) -> 1\n",
      "(the (1), lazy (7)) -> 1\n",
      "(jumps (5), over (6)) -> 1\n",
      "(the (1), lazy (7)) -> 0\n",
      "(jumps (5), over (6)) -> 0\n",
      "(the (1), lazy (7)) -> 0\n",
      "(quick (2), quick (2)) -> 0\n",
      "(lazy (7), the (1)) -> 1\n",
      "(brown (3), fox (4)) -> 1\n",
      "(fox (4), brown (3)) -> 0\n",
      "(quick (2), over (6)) -> 0\n",
      "(brown (3), over (6)) -> 0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
    "pairs, labels = skipgrams(wids, len(word2id), window_size=1)\n",
    "print(\"Number of pairs: \",len(pairs))\n",
    "for i in range(20):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11477307975292206"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "history=model.fit([asarray(pairs)[:,0], asarray(pairs)[:,1]],labels,epochs=200,verbose=0)\n",
    "model.evaluate([asarray(pairs)[:,0], asarray(pairs)[:,1]],labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.03853421, -0.01923045, -0.03822639],\n",
       "        [-0.00731206, -0.13755003,  0.12676875],\n",
       "        [-0.01641215,  0.12862098,  0.06812601],\n",
       "        [-0.09955261,  0.0113467 , -0.2285318 ],\n",
       "        [-0.00181953,  0.14252363,  0.06642576],\n",
       "        [-0.08305285, -0.09567424, -0.07200117],\n",
       "        [ 0.03106353,  0.1278866 , -0.09797202],\n",
       "        [-0.17730547,  0.06396743, -0.0027189 ],\n",
       "        [ 0.1971729 ,  0.01422863, -0.05654199],\n",
       "        [-0.02735932, -0.00215567, -0.01546366]], dtype=float32)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store this embedding and later use it to encode words from a text. This is yet another application of transfer learning and good embeddings are costly to produce and tremendously useful. For example, google provides a word2vec embedding training on a large amount of news articles. A iPython notebook demonstrating this can be found here\n",
    "\n",
    "https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_02_word2vec.ipynb\n",
    "\n",
    "Why using a pre-trained embedding makes sense becomes more obvious when considering a task of classifying online ratings. While the meaning of individual words can surely be learned from a large enough corpus using a word2vec embedding will also allow us to deal with ratings that use novel words with similar meaning, e.g. For example, the most similar words to \"tasty\" in the google news dataset are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 0.8730389475822449),\n",
       " ('scrumptious', 0.8007042407989502),\n",
       " ('yummy', 0.7856924533843994),\n",
       " ('flavorful', 0.7420164346694946),\n",
       " ('delectable', 0.7385421991348267),\n",
       " ('juicy_flavorful', 0.7114803791046143),\n",
       " ('appetizing', 0.7017217874526978),\n",
       " ('crunchy_salty', 0.7012300491333008),\n",
       " ('flavourful', 0.6912213563919067),\n",
       " ('flavoursome', 0.6857702732086182)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[('delicious', 0.8730389475822449),\n",
    " ('scrumptious', 0.8007042407989502),\n",
    " ('yummy', 0.7856924533843994),\n",
    " ('flavorful', 0.7420164346694946),\n",
    " ('delectable', 0.7385421991348267),\n",
    " ('juicy_flavorful', 0.7114803791046143),\n",
    " ('appetizing', 0.7017217874526978),\n",
    " ('crunchy_salty', 0.7012300491333008),\n",
    " ('flavourful', 0.6912213563919067),\n",
    " ('flavoursome', 0.6857702732086182)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it is likely that similar sentences with such a word instead of \"tasty\" w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "- We have shown \"embeddings\" in the context of words as lanugage usually has a lot of them and a more direct encoding is infeasible. Embeddings do work in any of these situations in which a dimensionality reduction of otherwise discrete classes is desired.\n",
    "\n",
    "- We have also seen that embeddings can be learned to encode desired properties, such as similarity between individual elements. This can be learned by using Keras' \"merge\" layers, and in particular the dot product. Merge layers are a powerful method to create interesting new systems. For example, a \"concatenate\" layer can be used to merge networks with different kinds of input. For example, a house's price might be predicted from statistical data as well as images, see for example https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
