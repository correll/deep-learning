{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects\n",
    "\n",
    "Your final project should show innovation in at least data preprocessing or network architecture. Straighforward applications of standard processing pipelines on standard network architectures are not acceptable. Possible projects include:\n",
    "\n",
    "- Train a convolutional neural network on depth data, e.g. from an Intel RealSense, to augment object recognition\n",
    "- Combine the output of a YOLO or MASK R-CNN classifier with a word embedding to understand scene context\n",
    "- Classify force/torque data from assembly to estimate whether an assembly is successful or failing\n",
    "- Build a pipeline for gaze detection that can fit on an embedded computer\n",
    "- Participate in a competition on Kaggle or the OpenAI gym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks: LSTMs\n",
    "\n",
    "Recurrent networks are adding the time dimension to a neural network by feeding their output back to the input. This is done in a relatively straightforward fashion. First, the output of a cell is directly fed back to a special recurrent cell. Second, in order to capture N time steps, the recurrent cell is simply replicated N times, connecting the output of the first to the input of the second and so on. This is illustrated below. Dotted lines show the flow of information during backpropagation.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/rnn_backprop.svg\" width=\"50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although RNNs can theoretically make relationships between very new and very old information, for example two related words in a sentence that are a few words appart, but this relationship is difficult to learn using backpropagation, however. The reason can be understood, when considering how the gradient is calculated. The gradient is calculated for the \"loss\" with respect to the weights $W$. The loss $L$ is calculated via some suitable metric that compares the output $y(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vanishing and exploding gradient problem\n",
    "Note that when implementing, running, and training an RNN, all timesteps are presented at the same time. Also note that the parameters $u$, $v$, and $w$ are constant. Computing the gradient for the loss function $L$ with respect to the weights w looks as follows:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w}=\\sum_{\\forall t}\\frac{\\partial L_t}{\\partial w}$$\n",
    "\n",
    "Here, t=4. Similar equations can be written for the parameters $u$ and $v$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider the gradient at timestep $t=4$ using the chain rule:\n",
    "    \n",
    "$$ \\frac{\\partial L_4}{\\partial w}=\\frac{\\partial L_4}{\\partial y_4}\\frac{\\partial y_4}{\\partial h_3}\\frac{\\partial h_3}{\\partial w} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further following the computation graph reveals the following four branches\n",
    "$$ \\frac{\\partial L_4}{\\partial w} = \\frac{\\partial L_4}{\\partial y_4}\\frac{\\partial y_4}{\\partial h_3}\\frac{\\partial h_3}{\\partial h_3}\\frac{\\partial h_3}{\\partial w}\n",
    "+ \\frac{\\partial L_4}{\\partial y_4}\\frac{\\partial y_4}{\\partial h_3}\\frac{\\partial h_3}{\\partial h_2}\\frac{\\partial h_2}{\\partial w}\n",
    "+ \\frac{\\partial L_4}{\\partial y_4}\\frac{\\partial y_4}{\\partial h_3}\\frac{\\partial h_3}{\\partial h_1}\\frac{\\partial h_1}{\\partial w}\n",
    "+ \\frac{\\partial L_4}{\\partial y_4}\\frac{\\partial y_4}{\\partial h_3}\\frac{\\partial h_3}{\\partial h_0}\\frac{\\partial h_0}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n",
    "\n",
    "$$ \\frac{\\partial L_4}{\\partial w} = \\sum_{t=0}^{t=3}\\frac{\\partial L_4}{\\partial y_4}\\frac{\\partial y_4}{\\partial h_3}\\frac{\\partial h_3}{\\partial h_t}\\frac{\\partial h_3}{\\partial w} $$\n",
    "\n",
    "which is further expanded to \n",
    "\n",
    "$$ \\frac{\\partial L_4}{\\partial w} = \\sum_{t=0}^{t=3}\\frac{\\partial L_4}{\\partial y_4}\\frac{\\partial y_4}{\\partial h_3}\\prod_{j=t+1}^{3}\\frac{\\partial h_j}{\\partial h_{j-1}}\\frac{\\partial h_3}{\\partial w} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With gradients smaller than one, the product in the gradient calculation will quickly diminish the contribution of this gradient, making it difficult to capture long-term relationships. Alternatively, with gradients larger than one, the product will quickly become very large. This is known as <i>vanishing</i> or <i>exploding</i> gradient problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "\n",
    "There exist other cell models, that do not suffer from the vanishing/exploding gradient problem. One of which is the so-called <i>long short term memory</i> (LSTM) cell. Instead of just using one network layer using a <i>tanh</i>, but four non-linear activations that interact with each other. A good resource that graphically explains this is the blog post <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTMs</a>. A typical LSTM cell is shown below. \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/LSTM.svg\" width=\"50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs ($x$), outputs ($y$), and hidden states $h$ and $c$ are vectors. Thick black lines indicate the flow of data. Two lines merging, such as $x_t$ and $h_{t-1}$ indicate concatenation. Black lines diverging indicate data being copied, maintaining their dimension. The circled \"x\", \"+\" and \"tanh\" symbols indicate element-wise multiplication and addition, respectively. The rectangles are neural network layers with sigmoid and tanh activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM seems complicated at first. The critical element of the LSTM cell is the <i>cell state</i> $c_t$. The four neural networks below can affect how the cell state changes from $c_{t-1}$ to $c_t$. We note that if $f$ consists of all ones, the cell state remains unchanged. If $f$ consists of all zeros, the previous cell state will be forgotten. The sigmoid layer leading to $f$ is therefore also called the <i>forget gate layer</i>. The cell state can be further changed by adding the dot product of $i$ and $g$.\n",
    "\n",
    "There exist a large variety of LSTM models, a notable one being the <i>Gated Recurrent Unit</i> (GRU), which shows minor changes in performance for some types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "LSTMs are a drop-in replacement for simple RNN units, albeit requiring four times the amount of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "time_steps=5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=1, input_shape=(time_steps,1), activation=\"tanh\")) # build RNN that takes up to time_step values\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
