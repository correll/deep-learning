{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghjxqFli-dSK"
   },
   "source": [
    "# Building a ChatBot with a Seq2Seq LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVRnJnma_m2q"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/2142/0*LL_VfEsZQ1wW1391.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPTQzX4R_-1f"
   },
   "source": [
    "We'll build a Seq2Seq model just like this one, but with LSTM cells instead of GRU cells. We'll use GloVe embeddings to build a chatbot using an open-source Kaggle dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0dERmwjAWcs"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-kT6WWVAyFb"
   },
   "source": [
    "We'll use the dataset from the source below.  It contains pairs of questions and answers based on a number of subjects like food, history, AI etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiEhtosxA4gb"
   },
   "source": [
    "[ChatterBot Language Training Corpus](https://github.com/gunthercox/chatterbot-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76_ms8sc955i"
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "r = requests.get( 'https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true' ) \n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZqqRlc8Dx-l"
   },
   "source": [
    "Reading the data from the files:\n",
    "\n",
    "We parse each of the `.yaml` files.\n",
    "\n",
    "*   Concatenate two or more sentences if the answer has two or more of them\n",
    "*   Remove unwanted data types which are produced while parsing the data\n",
    "*   Append `<START>` and `<END>` to all the `answers`\n",
    "*   Create a `Tokenizer` and load the whole vocabulary ( `questions` + `answers` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "r3hd5Q-eDqMi",
    "outputId": "15d789ac-6c4e-4bcc-a8ad-97bc8c756ec5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "\n",
    "print( tf.VERSION )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eAeT8FquECoc",
    "outputId": "824a5da0-5268-40c7-fe27-d27097363bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 1894\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import preprocessing , utils\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "dir_path = 'chatbot_nlp/data'\n",
    "files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for filepath in files_list:\n",
    "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len( con ) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[ 1 : ]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append( ans )\n",
    "        elif len( con )> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])\n",
    "\n",
    "answers_with_tags = list()\n",
    "for i in range( len( answers ) ):\n",
    "    if type( answers[i] ) == str:\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop( i )\n",
    "\n",
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4sI741xER29"
   },
   "source": [
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gltCS5oXEcAp"
   },
   "source": [
    "Our model requires three arrays namely \n",
    "\n",
    "1) `encoder_input_data`: Tokenize the questions and pad them to their max length\n",
    "\n",
    "2) `decoder_input_data` : Tokenize the answers and pad them to their max length\n",
    "\n",
    "3) `decoder_output_data` : Tokenize the answers and remove the first element from all the tokenized_answers = This is the `<START>` element which we added earlier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "rSJRjPRzEJ85",
    "outputId": "efe84e6b-ffe3-4d3c-c555-079d16d70676"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564, 22) 22\n",
      "(564, 74) 74\n",
      "(564, 74, 1894)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "    vocab.append(word)\n",
    "\n",
    "def tokenize(sentences):\n",
    "    tokens_list = []\n",
    "    vocabulary = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n",
    "        tokens = sentence.split()\n",
    "        vocabulary += tokens\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list , vocabulary\n",
    "\n",
    "p = tokenize(questions + answers)\n",
    "model = Word2Vec(p[ 0 ]) \n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE , 100))\n",
    "for i in range(len(tokenizer.word_index)):\n",
    "  try:\n",
    "    embedding_matrix[i] = model[vocab[i]]\n",
    "  except KeyError:\n",
    "    continue \n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions , maxlen=maxlen_questions , padding='post')\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "onehot_answers = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
    "decoder_output_data = np.array(onehot_answers)\n",
    "print( decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-JEf7XqKf9t"
   },
   "source": [
    "### Model Architecture A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "vNdcMNhKGpjm",
    "outputId": "90d6f1a1-f0cf-41a6-feaa-fb67415d3e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    378800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    378800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1894)   380694      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,779,894\n",
      "Trainable params: 1,779,894\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3K8pAjYMbt2B"
   },
   "source": [
    "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
    "\n",
    "\n",
    "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
    "*   Embedding layer : For converting token vectors to fix sized dense vectors.\n",
    "*   LSTM layer : Provide access to Long-Short Term cells.\n",
    "\n",
    "Working : \n",
    "\n",
    "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
    "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
    "3.   These states are set in the LSTM cell of the decoder.\n",
    "4.   The decoder_input_data comes in through the Embedding layer.\n",
    "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yR1UNpleKjQS",
    "outputId": "fe67f1a7-25f2-4fa9-8dad-49ea752e8d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 564 samples\n",
      "Epoch 1/200\n",
      "564/564 [==============================] - 12s 21ms/sample - loss: 1.3390\n",
      "Epoch 2/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.1268\n",
      "Epoch 3/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.1022\n",
      "Epoch 4/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.0883\n",
      "Epoch 5/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.0735\n",
      "Epoch 6/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.0598\n",
      "Epoch 7/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.0466\n",
      "Epoch 8/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.0340\n",
      "Epoch 9/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.0222\n",
      "Epoch 10/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 1.0109\n",
      "Epoch 11/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9986\n",
      "Epoch 12/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9857\n",
      "Epoch 13/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9737\n",
      "Epoch 14/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9605\n",
      "Epoch 15/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9493\n",
      "Epoch 16/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9365\n",
      "Epoch 17/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9251\n",
      "Epoch 18/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9128\n",
      "Epoch 19/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.9009\n",
      "Epoch 20/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.8883\n",
      "Epoch 21/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.8772\n",
      "Epoch 22/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.8653\n",
      "Epoch 23/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.8539\n",
      "Epoch 24/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.8418\n",
      "Epoch 25/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.8307\n",
      "Epoch 26/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.8205\n",
      "Epoch 27/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.8079\n",
      "Epoch 28/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7971\n",
      "Epoch 29/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7870\n",
      "Epoch 30/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7780\n",
      "Epoch 31/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7658\n",
      "Epoch 32/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7577\n",
      "Epoch 33/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7462\n",
      "Epoch 34/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7374\n",
      "Epoch 35/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7262\n",
      "Epoch 36/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7193\n",
      "Epoch 37/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.7077\n",
      "Epoch 38/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6981\n",
      "Epoch 39/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6899\n",
      "Epoch 40/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6797\n",
      "Epoch 41/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6700\n",
      "Epoch 42/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6606\n",
      "Epoch 43/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6521\n",
      "Epoch 44/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6425\n",
      "Epoch 45/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6343\n",
      "Epoch 46/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6235\n",
      "Epoch 47/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6163\n",
      "Epoch 48/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.6050\n",
      "Epoch 49/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5980\n",
      "Epoch 50/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5876\n",
      "Epoch 51/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5800\n",
      "Epoch 52/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5707\n",
      "Epoch 53/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5622\n",
      "Epoch 54/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5530\n",
      "Epoch 55/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5449\n",
      "Epoch 56/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5369\n",
      "Epoch 57/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5267\n",
      "Epoch 58/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5197\n",
      "Epoch 59/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5109\n",
      "Epoch 60/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.5022\n",
      "Epoch 61/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4936\n",
      "Epoch 62/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4857\n",
      "Epoch 63/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4758\n",
      "Epoch 64/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4710\n",
      "Epoch 65/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4612\n",
      "Epoch 66/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4540\n",
      "Epoch 67/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4479\n",
      "Epoch 68/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4371\n",
      "Epoch 69/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4318\n",
      "Epoch 70/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4236\n",
      "Epoch 71/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4154\n",
      "Epoch 72/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4083\n",
      "Epoch 73/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.4002\n",
      "Epoch 74/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3933\n",
      "Epoch 75/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3903\n",
      "Epoch 76/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3794\n",
      "Epoch 77/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3720\n",
      "Epoch 78/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3651\n",
      "Epoch 79/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3600\n",
      "Epoch 80/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3534\n",
      "Epoch 81/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3451\n",
      "Epoch 82/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3383\n",
      "Epoch 83/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3342\n",
      "Epoch 84/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3249\n",
      "Epoch 85/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3210\n",
      "Epoch 86/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3136\n",
      "Epoch 87/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3092\n",
      "Epoch 88/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.3008\n",
      "Epoch 89/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2947\n",
      "Epoch 90/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2916\n",
      "Epoch 91/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2837\n",
      "Epoch 92/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2788\n",
      "Epoch 93/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2720\n",
      "Epoch 94/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2672\n",
      "Epoch 95/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2619\n",
      "Epoch 96/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2573\n",
      "Epoch 97/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2502\n",
      "Epoch 98/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2471\n",
      "Epoch 99/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2388\n",
      "Epoch 100/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2359\n",
      "Epoch 101/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2301\n",
      "Epoch 102/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2251\n",
      "Epoch 103/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2231\n",
      "Epoch 104/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2152\n",
      "Epoch 105/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2108\n",
      "Epoch 106/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2058\n",
      "Epoch 107/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.2033\n",
      "Epoch 108/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1990\n",
      "Epoch 109/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1933\n",
      "Epoch 110/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1899\n",
      "Epoch 111/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1834\n",
      "Epoch 112/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1798\n",
      "Epoch 113/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1780\n",
      "Epoch 114/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1728\n",
      "Epoch 115/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1682\n",
      "Epoch 116/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1668\n",
      "Epoch 117/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1602\n",
      "Epoch 118/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1576\n",
      "Epoch 119/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1539\n",
      "Epoch 120/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1496\n",
      "Epoch 121/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1466\n",
      "Epoch 122/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1427\n",
      "Epoch 123/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1421\n",
      "Epoch 124/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1353\n",
      "Epoch 125/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1337\n",
      "Epoch 126/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1308\n",
      "Epoch 127/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1268\n",
      "Epoch 128/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1234\n",
      "Epoch 129/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1205\n",
      "Epoch 130/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1180\n",
      "Epoch 131/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1147\n",
      "Epoch 132/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1116\n",
      "Epoch 133/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1099\n",
      "Epoch 134/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1051\n",
      "Epoch 135/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1040\n",
      "Epoch 136/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.1018\n",
      "Epoch 137/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0985\n",
      "Epoch 138/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0969\n",
      "Epoch 139/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0935\n",
      "Epoch 140/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0913\n",
      "Epoch 141/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0888\n",
      "Epoch 142/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0876\n",
      "Epoch 143/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0832\n",
      "Epoch 144/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0833\n",
      "Epoch 145/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0798\n",
      "Epoch 146/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0786\n",
      "Epoch 147/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0767\n",
      "Epoch 148/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0742\n",
      "Epoch 149/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0725\n",
      "Epoch 150/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0704\n",
      "Epoch 151/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0673\n",
      "Epoch 152/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0692\n",
      "Epoch 153/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0646\n",
      "Epoch 154/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0636\n",
      "Epoch 155/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0611\n",
      "Epoch 156/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0596\n",
      "Epoch 157/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0594\n",
      "Epoch 158/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0579\n",
      "Epoch 159/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0541\n",
      "Epoch 160/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0541\n",
      "Epoch 161/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0544\n",
      "Epoch 162/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0506\n",
      "Epoch 163/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0501\n",
      "Epoch 164/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0490\n",
      "Epoch 165/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0482\n",
      "Epoch 166/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0467\n",
      "Epoch 167/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0445\n",
      "Epoch 168/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0440\n",
      "Epoch 169/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0430\n",
      "Epoch 170/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0413\n",
      "Epoch 171/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0412\n",
      "Epoch 172/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0400\n",
      "Epoch 173/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0390\n",
      "Epoch 174/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0370\n",
      "Epoch 175/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0369\n",
      "Epoch 176/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0357\n",
      "Epoch 177/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0361\n",
      "Epoch 178/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0340\n",
      "Epoch 179/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0352\n",
      "Epoch 180/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0324\n",
      "Epoch 181/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0313\n",
      "Epoch 182/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0311\n",
      "Epoch 183/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0307\n",
      "Epoch 184/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0298\n",
      "Epoch 185/200\n",
      "564/564 [==============================] - 1s 3ms/sample - loss: 0.0293\n",
      "Epoch 186/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0285\n",
      "Epoch 187/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0288\n",
      "Epoch 188/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0270\n",
      "Epoch 189/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0271\n",
      "Epoch 190/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0267\n",
      "Epoch 191/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0253\n",
      "Epoch 192/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0252\n",
      "Epoch 193/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0244\n",
      "Epoch 194/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0246\n",
      "Epoch 195/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0240\n",
      "Epoch 196/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0233\n",
      "Epoch 197/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0226\n",
      "Epoch 198/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0226\n",
      "Epoch 199/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0219\n",
      "Epoch 200/200\n",
      "564/564 [==============================] - 2s 3ms/sample - loss: 0.0216\n"
     ]
    }
   ],
   "source": [
    "# Start Training!\n",
    "history = model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=64, epochs=200) \n",
    "model.save( 'model.h5' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8RXMgsuPKvh8",
    "outputId": "24704ad1-0893-46a6-f099-68756b947b9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2874118f28>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Dz_l5ETMDu7"
   },
   "source": [
    "## Defining inference models\n",
    "We create inference models which help in predicting answers.\n",
    "\n",
    "**Encoder inference model** : Takes the question as input and outputs LSTM states ( `h` and `c` ).\n",
    "\n",
    "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the `<start>` tag ). It will output the answers for the question which we fed to the encoder model and its state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmRP07m1LoUJ"
   },
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRgz0MbqMU6k"
   },
   "source": [
    "## Converse with ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfLTNDMtMhVd"
   },
   "source": [
    "1.   First, we take a question as input and predict the state values using `enc_model`.\n",
    "2.   We set the state values in the decoder's LSTM.\n",
    "3.   Then, we generate a sequence which contains the `<start>` element.\n",
    "4.   We input this sequence in the `dec_model`.\n",
    "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
    "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum answer length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqsJ05MOMToC"
   },
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "colab_type": "code",
    "id": "GU6YdoJsMbiV",
    "outputId": "87505bbc-16c5-49bc-8091-16785fd2a72f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : who invented computers\n",
      " it's a bit ambigous but british scientist charles babbage is regarded as the father of computers one might argue that john von neumann invented computers as we know them because he invented the princeton architecture in which instructions and data share the same memory field but are differentiated by context end\n",
      "Enter question : what is operating system\n",
      " any software that supports python end\n",
      "Enter question : which is better windows or macos\n",
      " it depends on which machine you're using to talk to me i'd prefer to not hurt your feelings linux always linux what are you trying to accomplish the os should support your goals end\n",
      "Enter question : you are cruel\n",
      " i couldn't have said it better myself end\n",
      "Enter question : ?\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a3576c66748e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter question : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1825298c5d8b>\u001b[0m in \u001b[0;36mstr_to_tokens\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokens_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtokens_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen_questions\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '?'"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uECymvOZdO5E"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CIUd3VWdSjd"
   },
   "source": [
    "## From the conversation:\n",
    "\n",
    "\n",
    "Enter question : who invented computers <br>\n",
    " it's a bit ambigous but british scientist charles babbage is regarded as the father of computers one might argue that john von neumann invented computers as we know them because he invented the princeton architecture in which instructions and data share the same memory field but are differentiated by context end <br>\n",
    " <br>\n",
    "Enter question : what is operating system <br>\n",
    " any software that supports python end <br>\n",
    " <br>\n",
    "Enter question : which is better windows or macos <br>\n",
    " it depends on which machine you're using to talk to me i'd prefer to not hurt your feelings linux always linux what are you trying to accomplish the os should support your goals end <br>\n",
    " <br>\n",
    "Enter question : you are cruel <br>\n",
    " i couldn't have said it better myself end\n",
    " <br>\n",
    " \n",
    "Enter question : ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CGYNjWoMqs_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ChatBot Seq2Seq model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
