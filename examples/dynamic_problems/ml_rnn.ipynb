{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ML Recurrent Neural Network Example</h1>\n",
    "This example demonstrates classification of time-series data from the force-torque sensor to detect grasping failure. The data is taken from the <a href=\"https://archive.ics.uci.edu/ml/datasets/Robot+Execution+Failures\">Robot execution failures</a> dataset. It consists of 87 labeled instances of sequences of 15 F/T measurements and class labels.\n",
    "\n",
    "The data is parsed into a pandas dataframe, broken into training and test set, and run through a recurrent neural network using keras. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import data processing tools\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib ipympl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import ML tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Import tools needed for visualization\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of time series of 6-DoF force/torque measurements that are labeled with success ('normal') or failure ('obstruction', 'collision', 'fr_collision'). The raw data can be inspected <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/robotfailure-mld/lp1.data\">here</a> and needs to be converted in table format. <br><br>\n",
    "The code for parsing the data has been adopted from a <a href=\"https://tsfresh.readthedocs.io/en/latest/_modules/tsfresh/examples/robot_execution_failures.html\">tsfresh tutorial</a>. It proceeds by reading the data file line by line and storing an entire time series in a single row. Sucess and the various failure modes are coded as a boolean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    id_to_target = {}\n",
    "    df_rows = []\n",
    "    df_block = []\n",
    "\n",
    "    with open('lp1.data') as f:\n",
    "        cur_id = 0\n",
    "        time = 0\n",
    "\n",
    "        for line in f.readlines():\n",
    "            # New sample --> increase id, reset time and determine target\n",
    "            if line[0] not in ['\\t', '\\n']:               \n",
    "                time = 0\n",
    "                #id_to_target[cur_id] = line.strip()\n",
    "                id_to_target[cur_id] = (line.strip()=='normal')\n",
    "                if(df_block): \n",
    "                    df_rows.append(np.array(df_block).reshape(1,90)[0].tolist())\n",
    "                df_block = []\n",
    "                cur_id += 1\n",
    "            # Data row --> split and convert values, create complete df row\n",
    "            elif line[0] == '\\t':\n",
    "                values = list(map(int, line.split('\\t')[1:]))\n",
    "                #df_rows.append([cur_id, time] + values)\n",
    "                df_block.append(values)\n",
    "                time += 1\n",
    "        df_rows.append(np.array(df_block).reshape(1,90)[0].tolist())\n",
    "\n",
    "    columns=[]\n",
    "    for i in range(0,15):\n",
    "        columns=columns+['F_x'+str(i), 'F_y'+str(i), 'F_z'+str(i), 'T_x'+str(i), 'T_y'+str(i), 'T_z'+str(i)]\n",
    "    \n",
    "    df = pd.DataFrame(df_rows, columns=columns) # Store all data in a Pandas dataframe\n",
    "    y = pd.Series(id_to_target) # Store all class labels in a Pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_x0</th>\n",
       "      <th>F_y0</th>\n",
       "      <th>F_z0</th>\n",
       "      <th>T_x0</th>\n",
       "      <th>T_y0</th>\n",
       "      <th>T_z0</th>\n",
       "      <th>F_x1</th>\n",
       "      <th>F_y1</th>\n",
       "      <th>F_z1</th>\n",
       "      <th>T_x1</th>\n",
       "      <th>...</th>\n",
       "      <th>F_z13</th>\n",
       "      <th>T_x13</th>\n",
       "      <th>T_y13</th>\n",
       "      <th>T_z13</th>\n",
       "      <th>F_x14</th>\n",
       "      <th>F_y14</th>\n",
       "      <th>F_z14</th>\n",
       "      <th>T_x14</th>\n",
       "      <th>T_y14</th>\n",
       "      <th>T_z14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-29</td>\n",
       "      <td>-10</td>\n",
       "      <td>-208</td>\n",
       "      <td>180</td>\n",
       "      <td>12</td>\n",
       "      <td>-11</td>\n",
       "      <td>-29</td>\n",
       "      <td>-4</td>\n",
       "      <td>-246</td>\n",
       "      <td>192</td>\n",
       "      <td>...</td>\n",
       "      <td>-918</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>-23</td>\n",
       "      <td>-105</td>\n",
       "      <td>63</td>\n",
       "      <td>-912</td>\n",
       "      <td>394</td>\n",
       "      <td>100</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>-25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>-26</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>-46</td>\n",
       "      <td>14</td>\n",
       "      <td>-7</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>-42</td>\n",
       "      <td>10</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>-14</td>\n",
       "      <td>-46</td>\n",
       "      <td>16</td>\n",
       "      <td>-7</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>-27</td>\n",
       "      <td>-49</td>\n",
       "      <td>...</td>\n",
       "      <td>-346</td>\n",
       "      <td>-88</td>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>148</td>\n",
       "      <td>69</td>\n",
       "      <td>-411</td>\n",
       "      <td>-95</td>\n",
       "      <td>191</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>171</td>\n",
       "      <td>86</td>\n",
       "      <td>-486</td>\n",
       "      <td>-113</td>\n",
       "      <td>222</td>\n",
       "      <td>13</td>\n",
       "      <td>198</td>\n",
       "      <td>96</td>\n",
       "      <td>-566</td>\n",
       "      <td>-125</td>\n",
       "      <td>...</td>\n",
       "      <td>-1036</td>\n",
       "      <td>-118</td>\n",
       "      <td>466</td>\n",
       "      <td>44</td>\n",
       "      <td>342</td>\n",
       "      <td>154</td>\n",
       "      <td>-1036</td>\n",
       "      <td>-118</td>\n",
       "      <td>466</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-6</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>-15</td>\n",
       "      <td>-17</td>\n",
       "      <td>3</td>\n",
       "      <td>-7</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>-17</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>-29</td>\n",
       "      <td>-27</td>\n",
       "      <td>5</td>\n",
       "      <td>-13</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    F_x0  F_y0  F_z0  T_x0  T_y0  T_z0  F_x1  F_y1  F_z1  T_x1  ...    F_z13  \\\n",
       "83   -29   -10  -208   180    12   -11   -29    -4  -246   192  ...     -918   \n",
       "84     4     6    50   -25     1     0     4     8    44   -26  ...        4   \n",
       "85    21    15   -14   -46    16    -7    23    18   -27   -49  ...     -346   \n",
       "86   171    86  -486  -113   222    13   198    96  -566  -125  ...    -1036   \n",
       "87    -6     3    52   -15   -17     3    -7     2    52   -17  ...       26   \n",
       "\n",
       "    T_x13  T_y13  T_z13  F_x14  F_y14  F_z14  T_x14  T_y14  T_z14  \n",
       "83    396    101    -23   -105     63   -912    394    100    -25  \n",
       "84    -46     14     -7     19     14      2    -42     10     -7  \n",
       "85    -88    154      4    148     69   -411    -95    191      8  \n",
       "86   -118    466     44    342    154  -1036   -118    466     44  \n",
       "87    -29    -27      5    -13      2     15    -25    -25      6  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    True\n",
       "1    True\n",
       "2    True\n",
       "3    True\n",
       "4    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=y\n",
    "features=np.array(df).astype(int)\n",
    "features.reshape(88,6,15)\n",
    "#feature_list=list(df.columns)\n",
    "\n",
    "labels=labels.astype(int)\n",
    "categorical_labels = np_utils.to_categorical(labels, num_classes=2).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features, categorical_labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(X_train.shape[0],15,6)\n",
    "X_test=X_test.reshape(X_test.shape[0],15,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 16)                240       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 394\n",
      "Trainable params: 394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "HIDDEN_SIZE = 8\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "#NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(HIDDEN_SIZE, return_sequences=False,unroll=True),input_shape=(15, 6)))\n",
    "model.add(Dense(HIDDEN_SIZE))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"rmsprop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56 samples, validate on 14 samples\n",
      "Epoch 1/200\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 1.0476 - val_loss: 1.0608\n",
      "Epoch 2/200\n",
      "56/56 [==============================] - 0s 400us/step - loss: 0.9552 - val_loss: 0.9866\n",
      "Epoch 3/200\n",
      "56/56 [==============================] - 0s 404us/step - loss: 0.8943 - val_loss: 0.9298\n",
      "Epoch 4/200\n",
      "56/56 [==============================] - 0s 370us/step - loss: 0.8419 - val_loss: 0.8846\n",
      "Epoch 5/200\n",
      "56/56 [==============================] - 0s 366us/step - loss: 0.7919 - val_loss: 0.8366\n",
      "Epoch 6/200\n",
      "56/56 [==============================] - 0s 364us/step - loss: 0.7511 - val_loss: 0.8002\n",
      "Epoch 7/200\n",
      "56/56 [==============================] - 0s 377us/step - loss: 0.7137 - val_loss: 0.7630\n",
      "Epoch 8/200\n",
      "56/56 [==============================] - 0s 375us/step - loss: 0.6785 - val_loss: 0.7277\n",
      "Epoch 9/200\n",
      "56/56 [==============================] - 0s 373us/step - loss: 0.6444 - val_loss: 0.6945\n",
      "Epoch 10/200\n",
      "56/56 [==============================] - 0s 367us/step - loss: 0.6136 - val_loss: 0.6665\n",
      "Epoch 11/200\n",
      "56/56 [==============================] - 0s 388us/step - loss: 0.5844 - val_loss: 0.6443\n",
      "Epoch 12/200\n",
      "56/56 [==============================] - 0s 367us/step - loss: 0.5559 - val_loss: 0.6258\n",
      "Epoch 13/200\n",
      "56/56 [==============================] - 0s 368us/step - loss: 0.5277 - val_loss: 0.6048\n",
      "Epoch 14/200\n",
      "56/56 [==============================] - 0s 370us/step - loss: 0.5056 - val_loss: 0.5856\n",
      "Epoch 15/200\n",
      "56/56 [==============================] - 0s 356us/step - loss: 0.4865 - val_loss: 0.5718\n",
      "Epoch 16/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.4706 - val_loss: 0.5589\n",
      "Epoch 17/200\n",
      "56/56 [==============================] - 0s 336us/step - loss: 0.4559 - val_loss: 0.5462\n",
      "Epoch 18/200\n",
      "56/56 [==============================] - 0s 329us/step - loss: 0.4407 - val_loss: 0.5323\n",
      "Epoch 19/200\n",
      "56/56 [==============================] - 0s 334us/step - loss: 0.4263 - val_loss: 0.5199\n",
      "Epoch 20/200\n",
      "56/56 [==============================] - 0s 331us/step - loss: 0.4134 - val_loss: 0.5095\n",
      "Epoch 21/200\n",
      "56/56 [==============================] - 0s 330us/step - loss: 0.4010 - val_loss: 0.4976\n",
      "Epoch 22/200\n",
      "56/56 [==============================] - 0s 324us/step - loss: 0.3882 - val_loss: 0.4871\n",
      "Epoch 23/200\n",
      "56/56 [==============================] - 0s 332us/step - loss: 0.3753 - val_loss: 0.4771\n",
      "Epoch 24/200\n",
      "56/56 [==============================] - 0s 333us/step - loss: 0.3643 - val_loss: 0.4687\n",
      "Epoch 25/200\n",
      "56/56 [==============================] - 0s 330us/step - loss: 0.3534 - val_loss: 0.4604\n",
      "Epoch 26/200\n",
      "56/56 [==============================] - 0s 326us/step - loss: 0.3438 - val_loss: 0.4523\n",
      "Epoch 27/200\n",
      "56/56 [==============================] - 0s 326us/step - loss: 0.3349 - val_loss: 0.4446\n",
      "Epoch 28/200\n",
      "56/56 [==============================] - 0s 328us/step - loss: 0.3258 - val_loss: 0.4389\n",
      "Epoch 29/200\n",
      "56/56 [==============================] - 0s 333us/step - loss: 0.3171 - val_loss: 0.4321\n",
      "Epoch 30/200\n",
      "56/56 [==============================] - 0s 325us/step - loss: 0.3102 - val_loss: 0.4269\n",
      "Epoch 31/200\n",
      "56/56 [==============================] - 0s 426us/step - loss: 0.3015 - val_loss: 0.4222\n",
      "Epoch 32/200\n",
      "56/56 [==============================] - 0s 389us/step - loss: 0.2943 - val_loss: 0.4169\n",
      "Epoch 33/200\n",
      "56/56 [==============================] - 0s 335us/step - loss: 0.2869 - val_loss: 0.4129\n",
      "Epoch 34/200\n",
      "56/56 [==============================] - 0s 320us/step - loss: 0.2795 - val_loss: 0.4086\n",
      "Epoch 35/200\n",
      "56/56 [==============================] - 0s 333us/step - loss: 0.2728 - val_loss: 0.4047\n",
      "Epoch 36/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.2663 - val_loss: 0.4016\n",
      "Epoch 37/200\n",
      "56/56 [==============================] - 0s 334us/step - loss: 0.2595 - val_loss: 0.3982\n",
      "Epoch 38/200\n",
      "56/56 [==============================] - 0s 326us/step - loss: 0.2541 - val_loss: 0.3956\n",
      "Epoch 39/200\n",
      "56/56 [==============================] - 0s 347us/step - loss: 0.2467 - val_loss: 0.3929\n",
      "Epoch 40/200\n",
      "56/56 [==============================] - 0s 362us/step - loss: 0.2402 - val_loss: 0.3907\n",
      "Epoch 41/200\n",
      "56/56 [==============================] - 0s 323us/step - loss: 0.2341 - val_loss: 0.3877\n",
      "Epoch 42/200\n",
      "56/56 [==============================] - 0s 321us/step - loss: 0.2280 - val_loss: 0.3841\n",
      "Epoch 43/200\n",
      "56/56 [==============================] - 0s 378us/step - loss: 0.2238 - val_loss: 0.3810\n",
      "Epoch 44/200\n",
      "56/56 [==============================] - 0s 333us/step - loss: 0.2177 - val_loss: 0.3779\n",
      "Epoch 45/200\n",
      "56/56 [==============================] - 0s 342us/step - loss: 0.2134 - val_loss: 0.3754\n",
      "Epoch 46/200\n",
      "56/56 [==============================] - 0s 336us/step - loss: 0.2093 - val_loss: 0.3731\n",
      "Epoch 47/200\n",
      "56/56 [==============================] - 0s 342us/step - loss: 0.2040 - val_loss: 0.3700\n",
      "Epoch 48/200\n",
      "56/56 [==============================] - 0s 353us/step - loss: 0.2014 - val_loss: 0.3679\n",
      "Epoch 49/200\n",
      "56/56 [==============================] - 0s 346us/step - loss: 0.1977 - val_loss: 0.3658\n",
      "Epoch 50/200\n",
      "56/56 [==============================] - 0s 355us/step - loss: 0.1934 - val_loss: 0.3643\n",
      "Epoch 51/200\n",
      "56/56 [==============================] - 0s 370us/step - loss: 0.1913 - val_loss: 0.3631\n",
      "Epoch 52/200\n",
      "56/56 [==============================] - 0s 337us/step - loss: 0.1872 - val_loss: 0.3614\n",
      "Epoch 53/200\n",
      "56/56 [==============================] - 0s 363us/step - loss: 0.1844 - val_loss: 0.3602\n",
      "Epoch 54/200\n",
      "56/56 [==============================] - 0s 337us/step - loss: 0.1801 - val_loss: 0.3594\n",
      "Epoch 55/200\n",
      "56/56 [==============================] - 0s 348us/step - loss: 0.1783 - val_loss: 0.3583\n",
      "Epoch 56/200\n",
      "56/56 [==============================] - 0s 338us/step - loss: 0.1743 - val_loss: 0.3576\n",
      "Epoch 57/200\n",
      "56/56 [==============================] - 0s 346us/step - loss: 0.1735 - val_loss: 0.3569\n",
      "Epoch 58/200\n",
      "56/56 [==============================] - 0s 332us/step - loss: 0.1695 - val_loss: 0.3561\n",
      "Epoch 59/200\n",
      "56/56 [==============================] - 0s 337us/step - loss: 0.1668 - val_loss: 0.3553\n",
      "Epoch 60/200\n",
      "56/56 [==============================] - 0s 329us/step - loss: 0.1626 - val_loss: 0.3554\n",
      "Epoch 61/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.1588 - val_loss: 0.3552\n",
      "Epoch 62/200\n",
      "56/56 [==============================] - 0s 348us/step - loss: 0.1552 - val_loss: 0.3544\n",
      "Epoch 63/200\n",
      "56/56 [==============================] - 0s 337us/step - loss: 0.1542 - val_loss: 0.3536\n",
      "Epoch 64/200\n",
      "56/56 [==============================] - 0s 335us/step - loss: 0.1510 - val_loss: 0.3539\n",
      "Epoch 65/200\n",
      "56/56 [==============================] - 0s 330us/step - loss: 0.1489 - val_loss: 0.3539\n",
      "Epoch 66/200\n",
      "56/56 [==============================] - 0s 344us/step - loss: 0.1477 - val_loss: 0.3539\n",
      "Epoch 67/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.1443 - val_loss: 0.3536\n",
      "Epoch 68/200\n",
      "56/56 [==============================] - 0s 342us/step - loss: 0.1448 - val_loss: 0.3515\n",
      "Epoch 69/200\n",
      "56/56 [==============================] - 0s 323us/step - loss: 0.1421 - val_loss: 0.3511\n",
      "Epoch 70/200\n",
      "56/56 [==============================] - 0s 334us/step - loss: 0.1391 - val_loss: 0.3508\n",
      "Epoch 71/200\n",
      "56/56 [==============================] - 0s 314us/step - loss: 0.1379 - val_loss: 0.3510\n",
      "Epoch 72/200\n",
      "56/56 [==============================] - 0s 331us/step - loss: 0.1374 - val_loss: 0.3499\n",
      "Epoch 73/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.1340 - val_loss: 0.3491\n",
      "Epoch 74/200\n",
      "56/56 [==============================] - 0s 344us/step - loss: 0.1323 - val_loss: 0.3486\n",
      "Epoch 75/200\n",
      "56/56 [==============================] - 0s 339us/step - loss: 0.1321 - val_loss: 0.3479\n",
      "Epoch 76/200\n",
      "56/56 [==============================] - 0s 325us/step - loss: 0.1309 - val_loss: 0.3466\n",
      "Epoch 77/200\n",
      "56/56 [==============================] - 0s 334us/step - loss: 0.1265 - val_loss: 0.3462\n",
      "Epoch 78/200\n",
      "56/56 [==============================] - 0s 336us/step - loss: 0.1238 - val_loss: 0.3450\n",
      "Epoch 79/200\n",
      "56/56 [==============================] - 0s 362us/step - loss: 0.1230 - val_loss: 0.3451\n",
      "Epoch 80/200\n",
      "56/56 [==============================] - 0s 326us/step - loss: 0.1222 - val_loss: 0.3435\n",
      "Epoch 81/200\n",
      "56/56 [==============================] - 0s 326us/step - loss: 0.1208 - val_loss: 0.3424\n",
      "Epoch 82/200\n",
      "56/56 [==============================] - 0s 325us/step - loss: 0.1186 - val_loss: 0.3422\n",
      "Epoch 83/200\n",
      "56/56 [==============================] - 0s 324us/step - loss: 0.1190 - val_loss: 0.3414\n",
      "Epoch 84/200\n",
      "56/56 [==============================] - 0s 344us/step - loss: 0.1168 - val_loss: 0.3402\n",
      "Epoch 85/200\n",
      "56/56 [==============================] - 0s 348us/step - loss: 0.1187 - val_loss: 0.3408\n",
      "Epoch 86/200\n",
      "56/56 [==============================] - 0s 341us/step - loss: 0.1148 - val_loss: 0.3404\n",
      "Epoch 87/200\n",
      "56/56 [==============================] - 0s 321us/step - loss: 0.1139 - val_loss: 0.3397\n",
      "Epoch 88/200\n",
      "56/56 [==============================] - 0s 327us/step - loss: 0.1138 - val_loss: 0.3390\n",
      "Epoch 89/200\n",
      "56/56 [==============================] - 0s 356us/step - loss: 0.1124 - val_loss: 0.3394\n",
      "Epoch 90/200\n",
      "56/56 [==============================] - 0s 330us/step - loss: 0.1130 - val_loss: 0.3378\n",
      "Epoch 91/200\n",
      "56/56 [==============================] - 0s 346us/step - loss: 0.1128 - val_loss: 0.3383\n",
      "Epoch 92/200\n",
      "56/56 [==============================] - 0s 343us/step - loss: 0.1107 - val_loss: 0.3384\n",
      "Epoch 93/200\n",
      "56/56 [==============================] - 0s 338us/step - loss: 0.1103 - val_loss: 0.3367\n",
      "Epoch 94/200\n",
      "56/56 [==============================] - 0s 358us/step - loss: 0.1103 - val_loss: 0.3374\n",
      "Epoch 95/200\n",
      "56/56 [==============================] - 0s 379us/step - loss: 0.1080 - val_loss: 0.3363\n",
      "Epoch 96/200\n",
      "56/56 [==============================] - 0s 391us/step - loss: 0.1090 - val_loss: 0.3363\n",
      "Epoch 97/200\n",
      "56/56 [==============================] - 0s 394us/step - loss: 0.1075 - val_loss: 0.3367\n",
      "Epoch 98/200\n",
      "56/56 [==============================] - 0s 316us/step - loss: 0.1064 - val_loss: 0.3356\n",
      "Epoch 99/200\n",
      "56/56 [==============================] - 0s 359us/step - loss: 0.1062 - val_loss: 0.3350\n",
      "Epoch 100/200\n",
      "56/56 [==============================] - 0s 320us/step - loss: 0.1047 - val_loss: 0.3352\n",
      "Epoch 101/200\n",
      "56/56 [==============================] - 0s 336us/step - loss: 0.1058 - val_loss: 0.3360\n",
      "Epoch 102/200\n",
      "56/56 [==============================] - 0s 332us/step - loss: 0.1038 - val_loss: 0.3345\n",
      "Epoch 103/200\n",
      "56/56 [==============================] - 0s 331us/step - loss: 0.1033 - val_loss: 0.3351\n",
      "Epoch 104/200\n",
      "56/56 [==============================] - 0s 346us/step - loss: 0.1030 - val_loss: 0.3348\n",
      "Epoch 105/200\n",
      "56/56 [==============================] - 0s 331us/step - loss: 0.1032 - val_loss: 0.3342\n",
      "Epoch 106/200\n",
      "56/56 [==============================] - 0s 344us/step - loss: 0.1033 - val_loss: 0.3349\n",
      "Epoch 107/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.1012 - val_loss: 0.3344\n",
      "Epoch 108/200\n",
      "56/56 [==============================] - 0s 349us/step - loss: 0.1007 - val_loss: 0.3337\n",
      "Epoch 109/200\n",
      "56/56 [==============================] - 0s 353us/step - loss: 0.1010 - val_loss: 0.3332\n",
      "Epoch 110/200\n",
      "56/56 [==============================] - 0s 367us/step - loss: 0.1003 - val_loss: 0.3339\n",
      "Epoch 111/200\n",
      "56/56 [==============================] - 0s 332us/step - loss: 0.1012 - val_loss: 0.3347\n",
      "Epoch 112/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.0989 - val_loss: 0.3334\n",
      "Epoch 113/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.0987 - val_loss: 0.3329\n",
      "Epoch 114/200\n",
      "56/56 [==============================] - 0s 332us/step - loss: 0.0985 - val_loss: 0.3329\n",
      "Epoch 115/200\n",
      "56/56 [==============================] - 0s 328us/step - loss: 0.0983 - val_loss: 0.3329\n",
      "Epoch 116/200\n",
      "56/56 [==============================] - 0s 337us/step - loss: 0.0976 - val_loss: 0.3331\n",
      "Epoch 117/200\n",
      "56/56 [==============================] - 0s 334us/step - loss: 0.0975 - val_loss: 0.3319\n",
      "Epoch 118/200\n",
      "56/56 [==============================] - 0s 328us/step - loss: 0.0965 - val_loss: 0.3318\n",
      "Epoch 119/200\n",
      "56/56 [==============================] - 0s 345us/step - loss: 0.0960 - val_loss: 0.3323\n",
      "Epoch 120/200\n",
      "56/56 [==============================] - 0s 342us/step - loss: 0.0963 - val_loss: 0.3308\n",
      "Epoch 121/200\n",
      "56/56 [==============================] - 0s 346us/step - loss: 0.0967 - val_loss: 0.3316\n",
      "Epoch 122/200\n",
      "56/56 [==============================] - 0s 331us/step - loss: 0.0956 - val_loss: 0.3307\n",
      "Epoch 123/200\n",
      "56/56 [==============================] - 0s 329us/step - loss: 0.0942 - val_loss: 0.3309\n",
      "Epoch 124/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.0947 - val_loss: 0.3310\n",
      "Epoch 125/200\n",
      "56/56 [==============================] - 0s 349us/step - loss: 0.0950 - val_loss: 0.3299\n",
      "Epoch 126/200\n",
      "56/56 [==============================] - 0s 338us/step - loss: 0.0940 - val_loss: 0.3299\n",
      "Epoch 127/200\n",
      "56/56 [==============================] - 0s 362us/step - loss: 0.0939 - val_loss: 0.3292\n",
      "Epoch 128/200\n",
      "56/56 [==============================] - 0s 346us/step - loss: 0.0947 - val_loss: 0.3291\n",
      "Epoch 129/200\n",
      "56/56 [==============================] - 0s 385us/step - loss: 0.0931 - val_loss: 0.3284\n",
      "Epoch 130/200\n",
      "56/56 [==============================] - 0s 352us/step - loss: 0.0932 - val_loss: 0.3286\n",
      "Epoch 131/200\n",
      "56/56 [==============================] - 0s 355us/step - loss: 0.0931 - val_loss: 0.3269\n",
      "Epoch 132/200\n",
      "56/56 [==============================] - 0s 338us/step - loss: 0.0921 - val_loss: 0.3274\n",
      "Epoch 133/200\n",
      "56/56 [==============================] - 0s 344us/step - loss: 0.0923 - val_loss: 0.3263\n",
      "Epoch 134/200\n",
      "56/56 [==============================] - 0s 341us/step - loss: 0.0921 - val_loss: 0.3256\n",
      "Epoch 135/200\n",
      "56/56 [==============================] - 0s 337us/step - loss: 0.0921 - val_loss: 0.3238\n",
      "Epoch 136/200\n",
      "56/56 [==============================] - 0s 345us/step - loss: 0.0927 - val_loss: 0.3200\n",
      "Epoch 137/200\n",
      "56/56 [==============================] - 0s 341us/step - loss: 0.0913 - val_loss: 0.3211\n",
      "Epoch 138/200\n",
      "56/56 [==============================] - 0s 333us/step - loss: 0.0916 - val_loss: 0.3166\n",
      "Epoch 139/200\n",
      "56/56 [==============================] - 0s 343us/step - loss: 0.0911 - val_loss: 0.3124\n",
      "Epoch 140/200\n",
      "56/56 [==============================] - 0s 343us/step - loss: 0.0901 - val_loss: 0.3117\n",
      "Epoch 141/200\n",
      "56/56 [==============================] - 0s 341us/step - loss: 0.0904 - val_loss: 0.3062\n",
      "Epoch 142/200\n",
      "56/56 [==============================] - 0s 425us/step - loss: 0.0897 - val_loss: 0.3040\n",
      "Epoch 143/200\n",
      "56/56 [==============================] - 0s 338us/step - loss: 0.0913 - val_loss: 0.3009\n",
      "Epoch 144/200\n",
      "56/56 [==============================] - 0s 345us/step - loss: 0.0898 - val_loss: 0.2987\n",
      "Epoch 145/200\n",
      "56/56 [==============================] - 0s 336us/step - loss: 0.0902 - val_loss: 0.2953\n",
      "Epoch 146/200\n",
      "56/56 [==============================] - 0s 349us/step - loss: 0.0895 - val_loss: 0.2963\n",
      "Epoch 147/200\n",
      "56/56 [==============================] - 0s 331us/step - loss: 0.0920 - val_loss: 0.2942\n",
      "Epoch 148/200\n",
      "56/56 [==============================] - 0s 335us/step - loss: 0.0890 - val_loss: 0.2941\n",
      "Epoch 149/200\n",
      "56/56 [==============================] - 0s 357us/step - loss: 0.0887 - val_loss: 0.2911\n",
      "Epoch 150/200\n",
      "56/56 [==============================] - 0s 328us/step - loss: 0.0895 - val_loss: 0.2912\n",
      "Epoch 151/200\n",
      "56/56 [==============================] - 0s 331us/step - loss: 0.0898 - val_loss: 0.2890\n",
      "Epoch 152/200\n",
      "56/56 [==============================] - 0s 375us/step - loss: 0.0885 - val_loss: 0.2867\n",
      "Epoch 153/200\n",
      "56/56 [==============================] - 0s 336us/step - loss: 0.0885 - val_loss: 0.2862\n",
      "Epoch 154/200\n",
      "56/56 [==============================] - 0s 360us/step - loss: 0.0880 - val_loss: 0.2878\n",
      "Epoch 155/200\n",
      "56/56 [==============================] - 0s 340us/step - loss: 0.0886 - val_loss: 0.2864\n",
      "Epoch 156/200\n",
      "56/56 [==============================] - 0s 330us/step - loss: 0.0884 - val_loss: 0.2886\n",
      "Epoch 157/200\n",
      "56/56 [==============================] - 0s 358us/step - loss: 0.0892 - val_loss: 0.2896\n",
      "Epoch 158/200\n",
      "56/56 [==============================] - 0s 323us/step - loss: 0.0880 - val_loss: 0.2888\n",
      "Epoch 159/200\n",
      "56/56 [==============================] - 0s 351us/step - loss: 0.0872 - val_loss: 0.2882\n",
      "Epoch 160/200\n",
      "56/56 [==============================] - 0s 356us/step - loss: 0.0880 - val_loss: 0.2855\n",
      "Epoch 161/200\n",
      "56/56 [==============================] - 0s 333us/step - loss: 0.0883 - val_loss: 0.2880\n",
      "Epoch 162/200\n",
      "56/56 [==============================] - 0s 346us/step - loss: 0.0874 - val_loss: 0.2863\n",
      "Epoch 163/200\n",
      "56/56 [==============================] - 0s 336us/step - loss: 0.0868 - val_loss: 0.2832\n",
      "Epoch 164/200\n",
      "56/56 [==============================] - 0s 366us/step - loss: 0.0899 - val_loss: 0.2885\n",
      "Epoch 165/200\n",
      "56/56 [==============================] - 0s 373us/step - loss: 0.0872 - val_loss: 0.2857\n",
      "Epoch 166/200\n",
      "56/56 [==============================] - 0s 339us/step - loss: 0.0867 - val_loss: 0.2826\n",
      "Epoch 167/200\n",
      "56/56 [==============================] - 0s 354us/step - loss: 0.0866 - val_loss: 0.2858\n",
      "Epoch 168/200\n",
      "56/56 [==============================] - 0s 388us/step - loss: 0.0856 - val_loss: 0.2838\n",
      "Epoch 169/200\n",
      "56/56 [==============================] - 0s 349us/step - loss: 0.0866 - val_loss: 0.2814\n",
      "Epoch 170/200\n",
      "56/56 [==============================] - 0s 344us/step - loss: 0.0867 - val_loss: 0.2791\n",
      "Epoch 171/200\n",
      "56/56 [==============================] - 0s 346us/step - loss: 0.0862 - val_loss: 0.2784\n",
      "Epoch 172/200\n",
      "56/56 [==============================] - 0s 349us/step - loss: 0.0884 - val_loss: 0.2828\n",
      "Epoch 173/200\n",
      "56/56 [==============================] - 0s 349us/step - loss: 0.0866 - val_loss: 0.2808\n",
      "Epoch 174/200\n",
      "56/56 [==============================] - 0s 361us/step - loss: 0.0852 - val_loss: 0.2802\n",
      "Epoch 175/200\n",
      "56/56 [==============================] - 0s 345us/step - loss: 0.0871 - val_loss: 0.2791\n",
      "Epoch 176/200\n",
      "56/56 [==============================] - 0s 339us/step - loss: 0.0860 - val_loss: 0.2770\n",
      "Epoch 177/200\n",
      "56/56 [==============================] - 0s 337us/step - loss: 0.0854 - val_loss: 0.2806\n",
      "Epoch 178/200\n",
      "56/56 [==============================] - 0s 337us/step - loss: 0.0854 - val_loss: 0.2792\n",
      "Epoch 179/200\n",
      "56/56 [==============================] - 0s 355us/step - loss: 0.0858 - val_loss: 0.2769\n",
      "Epoch 180/200\n",
      "56/56 [==============================] - 0s 322us/step - loss: 0.0861 - val_loss: 0.2768\n",
      "Epoch 181/200\n",
      "56/56 [==============================] - 0s 344us/step - loss: 0.0852 - val_loss: 0.2736\n",
      "Epoch 182/200\n",
      "56/56 [==============================] - 0s 351us/step - loss: 0.0848 - val_loss: 0.2750\n",
      "Epoch 183/200\n",
      "56/56 [==============================] - 0s 336us/step - loss: 0.0857 - val_loss: 0.2730\n",
      "Epoch 184/200\n",
      "56/56 [==============================] - 0s 348us/step - loss: 0.0853 - val_loss: 0.2704\n",
      "Epoch 185/200\n",
      "56/56 [==============================] - 0s 332us/step - loss: 0.0847 - val_loss: 0.2682\n",
      "Epoch 186/200\n",
      "56/56 [==============================] - 0s 334us/step - loss: 0.0845 - val_loss: 0.2620\n",
      "Epoch 187/200\n",
      "56/56 [==============================] - 0s 321us/step - loss: 0.0848 - val_loss: 0.2561\n",
      "Epoch 188/200\n",
      "56/56 [==============================] - 0s 348us/step - loss: 0.0850 - val_loss: 0.2567\n",
      "Epoch 189/200\n",
      "56/56 [==============================] - 0s 351us/step - loss: 0.0833 - val_loss: 0.2580\n",
      "Epoch 190/200\n",
      "56/56 [==============================] - 0s 343us/step - loss: 0.0844 - val_loss: 0.2562\n",
      "Epoch 191/200\n",
      "56/56 [==============================] - 0s 338us/step - loss: 0.0842 - val_loss: 0.2578\n",
      "Epoch 192/200\n",
      "56/56 [==============================] - 0s 348us/step - loss: 0.0836 - val_loss: 0.2564\n",
      "Epoch 193/200\n",
      "56/56 [==============================] - 0s 347us/step - loss: 0.0847 - val_loss: 0.2604\n",
      "Epoch 194/200\n",
      "56/56 [==============================] - 0s 342us/step - loss: 0.0834 - val_loss: 0.2578\n",
      "Epoch 195/200\n",
      "56/56 [==============================] - 0s 378us/step - loss: 0.0835 - val_loss: 0.2566\n",
      "Epoch 196/200\n",
      "56/56 [==============================] - 0s 322us/step - loss: 0.0829 - val_loss: 0.2549\n",
      "Epoch 197/200\n",
      "56/56 [==============================] - 0s 333us/step - loss: 0.0829 - val_loss: 0.2567\n",
      "Epoch 198/200\n",
      "56/56 [==============================] - 0s 338us/step - loss: 0.0845 - val_loss: 0.2555\n",
      "Epoch 199/200\n",
      "56/56 [==============================] - 0s 355us/step - loss: 0.0831 - val_loss: 0.2586\n",
      "Epoch 200/200\n",
      "56/56 [==============================] - 0s 348us/step - loss: 0.0831 - val_loss: 0.2580\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH=200\n",
    "BATCH_SIZE = 8\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.round(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14b59fef0>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Results</h2>\n",
    "We can now compare the predictions made by the random forest with the known labels from the test set and calculate the percentage of errorneous predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(predictions-Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  [11.11111111 11.11111111] %\n"
     ]
    }
   ],
   "source": [
    "print('Error: ',sum(errors)/len(Y_test)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
