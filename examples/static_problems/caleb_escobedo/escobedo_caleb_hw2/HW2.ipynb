{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning HW2: Time-Series Data\n",
    "1) Load data in \n",
    "\n",
    "2) Visualize data\n",
    "\n",
    "3) Define neural network\n",
    "\n",
    "4) optimize hyperparameters\n",
    "\n",
    "5) Show results\n",
    "\n",
    "This code was adapted from \"Recurrent Neural Networks by Example in Python\" by Will Koehrsen: https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11695 unique words.\n",
      "There are 293001 sequences.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "def format_sequence(s):\n",
    "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
    "    \n",
    "    # Add spaces around punctuation\n",
    "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
    "    \n",
    "    # Remove references to figures\n",
    "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
    "    \n",
    "    # Remove double spaces\n",
    "    s = re.sub(r'\\s\\s', ' ', s)\n",
    "    return s\n",
    "\n",
    "def make_sequences(texts, training_length = 50,\n",
    "                   lower = True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "    \n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    print(f'There are {num_words} unique words.')\n",
    "    \n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n",
    "    \n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "    \n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "        \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "        \n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length: i + 1]\n",
    "            \n",
    "            # Set the features and label\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    print(f'There are {len(features)} sequences.')\n",
    "    \n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels\n",
    "\n",
    "def create_train_valid(features,\n",
    "                       labels,\n",
    "                       num_words,\n",
    "                       train_fraction=0.7):\n",
    "    \"\"\"Create training and validation features and labels.\"\"\"\n",
    "    \n",
    "    # Randomly shuffle features and labels\n",
    "    features, labels = shuffle(features, labels, random_state=50)\n",
    "\n",
    "    # Decide on number of samples for training\n",
    "    train_end = int(train_fraction * len(labels))\n",
    "\n",
    "    train_features = np.array(features[:train_end])\n",
    "    valid_features = np.array(features[train_end:])\n",
    "\n",
    "    train_labels = labels[:train_end]\n",
    "    valid_labels = labels[train_end:]\n",
    "\n",
    "    # Convert to arrays\n",
    "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "    # Using int8 for memory savings\n",
    "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "    # One hot encoding of labels\n",
    "    for example_index, word_index in enumerate(train_labels):\n",
    "        y_train[example_index, word_index] = 1\n",
    "\n",
    "    for example_index, word_index in enumerate(valid_labels):\n",
    "        y_valid[example_index, word_index] = 1\n",
    "\n",
    "    # Memory management\n",
    "    import gc\n",
    "    gc.enable()\n",
    "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
    "    gc.collect()\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "# First we load in the data using pandas\n",
    "import pandas as pd \n",
    "data = pd.read_csv(\"patent_data.csv\", parse_dates=['patent_date']).dropna(subset = ['patent_abstract'])\n",
    "abstracts = [format_sequence(a) for a in list(data['patent_abstract'])]\n",
    "word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(abstracts, 50)\n",
    "X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n",
    "training_dict = {'X_train': X_train, 'X_valid': X_valid, 'y_train': y_train, 'y_valid': y_valid}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we load in the data we need to prune the data of all weird characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: a value of a website visitor includes initializing a calculation model for calculating the value of the website visitor the calculation model being a neural network model with visitor information as an input and the visitor's value as an output training the calculation model by using a data sample and\n",
      "\n",
      "Label: determining\n",
      "\n",
      "Features: does not involve reading the main text of the mail and thus the sensitivity of the mail information is protected these attributes are chosen using filters built into the detection hardware neural networks and support vector machine built into the detection hardware are then used on these attributes to detect\n",
      "\n",
      "Label: pattern\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(training_dict['X_train'][:2]):\n",
    "    text = []\n",
    "    for idx in sequence:\n",
    "        text.append(idx_word[idx])\n",
    "        \n",
    "    print('Features: ' + ' '.join(text) + '\\n')\n",
    "    print('Label: ' + idx_word[np.argmax(training_dict['y_train'][i])] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make RNN model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1169500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11695)             760175    \n",
      "=================================================================\n",
      "Total params: 1,976,075\n",
      "Trainable params: 1,976,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(word_idx) + 1,\n",
    "        output_dim=100,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64, return_sequences=False, dropout=0.1,\n",
    "        recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(len(word_idx) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
