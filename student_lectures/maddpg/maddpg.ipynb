{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADDPG: Robots that work together\n",
    "by Caleb Escobedo\n",
    "\n",
    "1. What are Multi-agent games good for?\n",
    "    * Cooperative\n",
    "    * Competitive\n",
    "1. What makes Multi-agent learning so hard?\n",
    "    * Non-stationary environments\n",
    "    * Experience replay does not work the same here as in DDPG\n",
    "1. How do we solve the multi-agent problem? \n",
    "    * Centralized training with decentralized execution\n",
    "    * Augment Critic\n",
    "    * Augment replay buffer\n",
    "1. Current Progress\n",
    "    * Environments\n",
    "    * Codeing\n",
    "    * Future work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are multi-agent games good for?\n",
    "Images from [Lowe et. al](https://arxiv.org/pdf/1706.02275.pdf)\n",
    "\n",
    "### Cooperative - \"All agents must maximize a shared return”\n",
    "Agents must coordinate their actions in order to get the highest shared return that they can. In the following example scenario, the agents must work together to capture as many X’s as they can in a timely manner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/cooperative.png\", width=400, height=400>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/cooperative.png\", width=400, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competitive - \"Agents have conflicting goals\"\n",
    "\n",
    "Here the predators goal is to capture the prey, but the prey’s objective is opposing the predators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/both.png\", width=400, height=400>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/both.png\", width=400, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooperative communication\n",
    "Here there are two agent’s cooperatively completing a task, but one agent has information that the other must listen to in order to complete it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/info_share.png\", width=400, height=400>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/info_share.png\", width=400, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What makes Multi-agent learning so hard?\n",
    "\n",
    "### Non-stationary environments\n",
    "\n",
    "Example Start: Both agents are going towards the first x. This is not the optimal reward possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/start.png\", width=500, height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/start.png\", width=500, height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal end state: Only one agent switches their action to maximize reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/goal.png\", width=500, height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/goal.png\", width=500, height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrect update: We assume that the enviornment is static and both agents update their policy to ge the other objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/update_bad.png\", width=500, height=500>\n",
       "<img src=\"img/sidewalk.gif\", width=500, height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/update_bad.png\", width=500, height=500>\n",
    "<img src=\"img/sidewalk.gif\", width=500, height=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay does not work the same here as in DDPG\n",
    "\n",
    "As seen below, the original DDPG algorithm held values from past trajectories in a replay buffer. These values were randomly sampled to learn on, this is acceptable because it is a static environment (other agents actions don’t change over time). In a multi-agent setting, all agents' actions are changing over time and must be accounted for in order to achieve the maximum possible reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/old_rb.png\", width=700, height=700>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/old_rb.png\", width=700, height=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How do we solve the multi-agent problem? \n",
    "\n",
    "### Centralized training with decentralized execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/new_training.png\", width=700, height=700>\n",
       "<img src=\"img/spurs.gif\", width=700, height=700>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/new_training.png\", width=700, height=700>\n",
    "<img src=\"img/spurs.gif\", width=700, height=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment Critic\n",
    "Each agent's critic now takes in all other agent actions and observations to best update it’s own policy for the value of a certain state. In addition, each agent approximates all other policies throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/q_function.png\", width=500, height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/q_function.png\", width=500, height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment replay buffer\n",
    "\n",
    "\"if we know the actions taken by all agents, the environment is stationary even as the policies change\" - Lowe et. al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/rb.png\", width=500, height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/rb.png\", width=500, height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Current Progress\n",
    "###  Environments\n",
    "\n",
    "OpenAI provies all sample environments seen in section 1 to use for multi-agent reinforcement learning setting here: https://github.com/openai/multiagent-particle-envs . In addition to their environments, they offer open source code for their MADDPG algorithm that can easily be set up and used in a couple of minutes https://github.com/openai/maddpg .\n",
    "\n",
    "### Coding\n",
    "\n",
    "Implementation from Scratch - At the start of this project, my goal was to implement MADDPG completely from scratch and then extend it to work on the multi-agent setting that I wanted to expand towards. **This was too bold for a semester-long project.** I spent a large amount of time debugging the keras syntax as opposed to implementing the specific algorithmic steps. All of my current code for my single agent DDPG algorithm can be seen on my github https://github.com/calebescobedo/reinforcement_learning . \n",
    "\n",
    "### Future work\n",
    "\n",
    "Extend the completed first implementation of [MADDPG](https://github.com/openai/maddpg) to do what my final group project evaluation is going to be. From the past work with my two collaborators Joewie Koh and Guohui Ding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/cars.png\", width=700, height=700>\n",
       "<img src=\"img/arms.png\", width=700, height=700>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/cars.png\", width=700, height=700>\n",
    "<img src=\"img/arms.png\", width=700, height=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
