{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL involves learning what to do in order to maximize a numerical reward. Essentially, how to map situations to actions while maximizing a signal. \n",
    "The learner is not told in any way which actions to take, instead it must discover which actions yield the most reward by trying them out. This is one of the main challenges in RL; the trade-off between *exploration* and *explotation*. The agent has to exploit what it already knows in order to obtain a reward, but it also has to explore to make better action selections. Note that actions may affect not only immediate rewards, but also future rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Games (e.g., [Atari](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), [Go](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf), [Chess](https://arxiv.org/abs/1712.01815))\n",
    "2. [Robotic Control](https://arxiv.org/pdf/1504.00702.pdf)\n",
    "3. [Traffic Light Control](http://web.eecs.utk.edu/~ielhanan/Papers/IET_ITS_2010.pdf)\n",
    "\n",
    "For more applications please refer to Dr. Yuxi Li's RL Applications [paper](https://arxiv.org/pdf/1908.06973.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> <b> Policy </b>: defines the learning agent's way of behaving at a given time. In some cases the policy can be as simple as a lookup table, whereas in others it may involve complex search processes. In general, policies are stochastic.</li>\n",
    "    \n",
    "<li> <b> State </b>: concrete and immediate situation in which an agent finds itself.  </li>\n",
    "    \n",
    "<li> <b> Action </b>: set of all possible moves the agent can make.  </li>\n",
    "\n",
    "<li> <b> Reward Signal </b>: defines the goal in a RL problem. The agent's sole objective is to maximize the total reward  it receives over the long run. The reward signal defines what are the good and bad events for the agent -- analogous to pleasure and pain.</li>\n",
    "\n",
    "<li> <b> Value Function </b>: roughly speaking, a value function is the total amount of reward an agent can expect to accumulate. Whereas rewards are immediate, values indicate long-term desirability of states, which takes into account the states that are likely to follow and the rewards available in those states. Value functions will be closer to how pleased or displeased we are that our enviroment is in a particular state.</li>\n",
    "\n",
    "<li> <b> Model </b>: is what mimics the behavior of the enviorment. Models are used for planning.</li>\n",
    "    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a recycle bot, where the agent can:  <br>\n",
    "(1) actively search for a can, <br>\n",
    "(2) remain stationary waiting for a can, or <br>\n",
    "(3) recharge its battery. \n",
    "<br>\n",
    "The agent makes its decisions solely as a function of the energy level of the battery. In this case the **states** are *high* or *low*. **Actions** are *wait*, *search*, and *recharge*. <br>\n",
    "\n",
    "\n",
    "![MDP](MDP.jpg \"MDP\")\n",
    "![Agent-environment Interaction](RL_Model.jpg \"Agent-environment Interaction\")\n",
    "\n",
    "<br>\n",
    "\n",
    "This system is a finite MDP, where the value function ($v_{\\pi}(s)$) can be defined as the expected value of a random variable given that the agent follows a policy ($\\pi$), $v_{\\pi}(s)$ has recurssive properties and it can be showed that the value function can be written as a Bellman equation with relates the value of a state and the values of its successor states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{\\pi}(s) = \\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|r,a)[r+\\gamma v_{\\pi}(s')]$ <br>\n",
    "<br>\n",
    "$a$ = actions\n",
    "$s'$ = future states\n",
    "$r$ = the rewards\n",
    "$\\gamma$ = discounted factor (to dampen future rewards)\n",
    "\n",
    "The *Bellman equation* averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning (DL) in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the state space or action space are too large to be completely known a neural network can be used to approximate a value function or policy function. Meaning that neural networks can learn to map states to values or state/action pairs to Q values (meausre of overall expected reward). These networks are trained on samples from the state or action to learn to predict how valuable those are relative to the target. Normally, the state will be something visual like a game screen, so Convolutional Neural Networks (CNNs) are often used. However, insted of distinguishing between a dog and a cat like in supervised learning in RL CNNs are used to determine possible actions based on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN_SL](CNN_cat.jpg \"CNN\")\n",
    "![CNN_RL](CNN_game.jpg \"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially CNNs are initialized stochastically and using feedback from the enviroment the CNN can use the difference between its expected reward and the actual reward to adjust its weights. The main difference with supervised learning's backpropagation is that in supervise learning the CNN starts with the true labels wheras in RL the CNN relies on the enviroment to send a scalar number in response to each new action (rewards). The complication here is that enviorments are often not differentiable (or the dynamics of the enviroment are unknown) which is a requirement for backpropagation; the loss is differentible with respect to the weights (to compute the gradients). Therefore, RL uses two main algorithms:\n",
    "\n",
    "<ol>\n",
    "    <li> <b> Policy Gradients </b>: gradient estimation of the reward with respect to the weights. </li>\n",
    "    <li> <b> Q-learning </b>: learn the expected reward of each state/action pair using standard backpropagation then taking the highest expected reward. Given an action compute the state and learn the expected reward, repeat this for different actions and maximize the reward.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q-learning](Qlearning.jpg \"Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to this [Nature paper](https://www.nature.com/articles/nature14236) for a full explanation on Q-Learning in Deep RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\artur\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n",
      "    15/100000: episode: 1, duration: 0.025s, episode steps:  15, steps per second: 611, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "    38/100000: episode: 2, duration: 0.007s, episode steps:  23, steps per second: 3307, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.696 [0.000, 1.000],  mean_best_reward: --\n",
      "    55/100000: episode: 3, duration: 0.005s, episode steps:  17, steps per second: 3372, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      "    66/100000: episode: 4, duration: 0.004s, episode steps:  11, steps per second: 3102, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  mean_best_reward: --\n",
      "    88/100000: episode: 5, duration: 0.006s, episode steps:  22, steps per second: 3390, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      "   117/100000: episode: 6, duration: 0.010s, episode steps:  29, steps per second: 3026, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      "   131/100000: episode: 7, duration: 0.005s, episode steps:  14, steps per second: 3096, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "   147/100000: episode: 8, duration: 0.005s, episode steps:  16, steps per second: 3341, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "   172/100000: episode: 9, duration: 0.007s, episode steps:  25, steps per second: 3528, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.320 [0.000, 1.000],  mean_best_reward: --\n",
      "   231/100000: episode: 10, duration: 0.016s, episode steps:  59, steps per second: 3725, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      "   242/100000: episode: 11, duration: 0.003s, episode steps:  11, steps per second: 3195, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      "   264/100000: episode: 12, duration: 0.006s, episode steps:  22, steps per second: 3582, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      "   282/100000: episode: 13, duration: 0.005s, episode steps:  18, steps per second: 3369, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  mean_best_reward: --\n",
      "   299/100000: episode: 14, duration: 0.005s, episode steps:  17, steps per second: 3373, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      "   337/100000: episode: 15, duration: 0.011s, episode steps:  38, steps per second: 3398, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.605 [0.000, 1.000],  mean_best_reward: --\n",
      "   350/100000: episode: 16, duration: 0.004s, episode steps:  13, steps per second: 3216, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  mean_best_reward: --\n",
      "   369/100000: episode: 17, duration: 0.005s, episode steps:  19, steps per second: 3498, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  mean_best_reward: --\n",
      "   383/100000: episode: 18, duration: 0.005s, episode steps:  14, steps per second: 2835, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "   396/100000: episode: 19, duration: 0.005s, episode steps:  13, steps per second: 2537, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      "   440/100000: episode: 20, duration: 0.014s, episode steps:  44, steps per second: 3132, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "   474/100000: episode: 21, duration: 0.013s, episode steps:  34, steps per second: 2576, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      "   493/100000: episode: 22, duration: 0.008s, episode steps:  19, steps per second: 2260, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "   504/100000: episode: 23, duration: 0.006s, episode steps:  11, steps per second: 1966, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "   517/100000: episode: 24, duration: 0.006s, episode steps:  13, steps per second: 2165, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  mean_best_reward: --\n",
      "   529/100000: episode: 25, duration: 0.004s, episode steps:  12, steps per second: 2764, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  mean_best_reward: --\n",
      "   541/100000: episode: 26, duration: 0.005s, episode steps:  12, steps per second: 2570, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "   575/100000: episode: 27, duration: 0.012s, episode steps:  34, steps per second: 2865, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "   592/100000: episode: 28, duration: 0.006s, episode steps:  17, steps per second: 3039, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "   602/100000: episode: 29, duration: 0.004s, episode steps:  10, steps per second: 2830, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  mean_best_reward: --\n",
      "   614/100000: episode: 30, duration: 0.004s, episode steps:  12, steps per second: 2675, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  mean_best_reward: --\n",
      "   627/100000: episode: 31, duration: 0.005s, episode steps:  13, steps per second: 2878, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  mean_best_reward: --\n",
      "   646/100000: episode: 32, duration: 0.006s, episode steps:  19, steps per second: 3304, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "   656/100000: episode: 33, duration: 0.003s, episode steps:  10, steps per second: 3158, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  mean_best_reward: --\n",
      "   671/100000: episode: 34, duration: 0.004s, episode steps:  15, steps per second: 3366, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  mean_best_reward: --\n",
      "   693/100000: episode: 35, duration: 0.007s, episode steps:  22, steps per second: 3261, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  mean_best_reward: --\n",
      "   705/100000: episode: 36, duration: 0.004s, episode steps:  12, steps per second: 3232, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "   722/100000: episode: 37, duration: 0.005s, episode steps:  17, steps per second: 3434, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "   736/100000: episode: 38, duration: 0.005s, episode steps:  14, steps per second: 3034, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "   761/100000: episode: 39, duration: 0.008s, episode steps:  25, steps per second: 3293, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "   773/100000: episode: 40, duration: 0.004s, episode steps:  12, steps per second: 2991, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  mean_best_reward: --\n",
      "   793/100000: episode: 41, duration: 0.006s, episode steps:  20, steps per second: 3283, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  mean_best_reward: --\n",
      "   815/100000: episode: 42, duration: 0.006s, episode steps:  22, steps per second: 3551, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "   835/100000: episode: 43, duration: 0.006s, episode steps:  20, steps per second: 3446, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      "   849/100000: episode: 44, duration: 0.004s, episode steps:  14, steps per second: 3212, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "   880/100000: episode: 45, duration: 0.009s, episode steps:  31, steps per second: 3589, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      "   899/100000: episode: 46, duration: 0.006s, episode steps:  19, steps per second: 3423, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  mean_best_reward: --\n",
      "   914/100000: episode: 47, duration: 0.005s, episode steps:  15, steps per second: 3075, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "   942/100000: episode: 48, duration: 0.009s, episode steps:  28, steps per second: 3287, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.679 [0.000, 1.000],  mean_best_reward: --\n",
      "   971/100000: episode: 49, duration: 0.008s, episode steps:  29, steps per second: 3557, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      "   999/100000: episode: 50, duration: 0.009s, episode steps:  28, steps per second: 3152, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  1021/100000: episode: 51, duration: 0.007s, episode steps:  22, steps per second: 3328, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      "  1035/100000: episode: 52, duration: 0.004s, episode steps:  14, steps per second: 3201, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  1072/100000: episode: 53, duration: 0.011s, episode steps:  37, steps per second: 3413, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      "  1091/100000: episode: 54, duration: 0.007s, episode steps:  19, steps per second: 2884, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "  1107/100000: episode: 55, duration: 0.005s, episode steps:  16, steps per second: 3091, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      "  1121/100000: episode: 56, duration: 0.005s, episode steps:  14, steps per second: 2680, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "  1136/100000: episode: 57, duration: 0.005s, episode steps:  15, steps per second: 3273, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  1146/100000: episode: 58, duration: 0.003s, episode steps:  10, steps per second: 2859, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  mean_best_reward: --\n",
      "  1165/100000: episode: 59, duration: 0.007s, episode steps:  19, steps per second: 2627, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      "  1181/100000: episode: 60, duration: 0.005s, episode steps:  16, steps per second: 3268, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      "  1202/100000: episode: 61, duration: 0.006s, episode steps:  21, steps per second: 3304, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  1216/100000: episode: 62, duration: 0.004s, episode steps:  14, steps per second: 3287, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n",
      "  1235/100000: episode: 63, duration: 0.005s, episode steps:  19, steps per second: 3483, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      "  1264/100000: episode: 64, duration: 0.009s, episode steps:  29, steps per second: 3162, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      "  1277/100000: episode: 65, duration: 0.004s, episode steps:  13, steps per second: 3254, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      "  1286/100000: episode: 66, duration: 0.003s, episode steps:   9, steps per second: 3030, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  mean_best_reward: --\n",
      "  1297/100000: episode: 67, duration: 0.004s, episode steps:  11, steps per second: 2478, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  1311/100000: episode: 68, duration: 0.005s, episode steps:  14, steps per second: 2865, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n",
      "  1333/100000: episode: 69, duration: 0.006s, episode steps:  22, steps per second: 3498, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      "  1369/100000: episode: 70, duration: 0.012s, episode steps:  36, steps per second: 3053, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      "  1386/100000: episode: 71, duration: 0.005s, episode steps:  17, steps per second: 3121, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  1397/100000: episode: 72, duration: 0.005s, episode steps:  11, steps per second: 2107, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      "  1411/100000: episode: 73, duration: 0.004s, episode steps:  14, steps per second: 3171, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "  1424/100000: episode: 74, duration: 0.004s, episode steps:  13, steps per second: 3208, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  1438/100000: episode: 75, duration: 0.006s, episode steps:  14, steps per second: 2533, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "  1457/100000: episode: 76, duration: 0.006s, episode steps:  19, steps per second: 3008, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      "  1500/100000: episode: 77, duration: 0.013s, episode steps:  43, steps per second: 3275, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      "  1509/100000: episode: 78, duration: 0.003s, episode steps:   9, steps per second: 2889, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  mean_best_reward: --\n",
      "  1520/100000: episode: 79, duration: 0.005s, episode steps:  11, steps per second: 2402, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  1535/100000: episode: 80, duration: 0.006s, episode steps:  15, steps per second: 2688, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  1544/100000: episode: 81, duration: 0.003s, episode steps:   9, steps per second: 2954, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  mean_best_reward: --\n",
      "  1566/100000: episode: 82, duration: 0.007s, episode steps:  22, steps per second: 3311, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      "  1579/100000: episode: 83, duration: 0.005s, episode steps:  13, steps per second: 2764, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  1590/100000: episode: 84, duration: 0.004s, episode steps:  11, steps per second: 3142, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  mean_best_reward: --\n",
      "  1607/100000: episode: 85, duration: 0.006s, episode steps:  17, steps per second: 3087, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  mean_best_reward: --\n",
      "  1617/100000: episode: 86, duration: 0.004s, episode steps:  10, steps per second: 2850, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  mean_best_reward: --\n",
      "  1634/100000: episode: 87, duration: 0.005s, episode steps:  17, steps per second: 3180, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  mean_best_reward: --\n",
      "  1673/100000: episode: 88, duration: 0.011s, episode steps:  39, steps per second: 3609, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  mean_best_reward: --\n",
      "  1685/100000: episode: 89, duration: 0.004s, episode steps:  12, steps per second: 3235, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  1711/100000: episode: 90, duration: 0.007s, episode steps:  26, steps per second: 3546, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      "  1723/100000: episode: 91, duration: 0.004s, episode steps:  12, steps per second: 2861, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  1739/100000: episode: 92, duration: 0.005s, episode steps:  16, steps per second: 3307, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      "  1751/100000: episode: 93, duration: 0.004s, episode steps:  12, steps per second: 3003, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      "  1774/100000: episode: 94, duration: 0.007s, episode steps:  23, steps per second: 3517, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      "  1785/100000: episode: 95, duration: 0.004s, episode steps:  11, steps per second: 3115, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      "  1797/100000: episode: 96, duration: 0.004s, episode steps:  12, steps per second: 3304, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      "  1845/100000: episode: 97, duration: 0.013s, episode steps:  48, steps per second: 3600, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      "  1863/100000: episode: 98, duration: 0.005s, episode steps:  18, steps per second: 3314, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  1879/100000: episode: 99, duration: 0.005s, episode steps:  16, steps per second: 3300, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  mean_best_reward: --\n",
      "  1894/100000: episode: 100, duration: 0.004s, episode steps:  15, steps per second: 3409, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.133 [0.000, 1.000],  mean_best_reward: --\n",
      "  1907/100000: episode: 101, duration: 0.004s, episode steps:  13, steps per second: 3229, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  1950/100000: episode: 102, duration: 0.012s, episode steps:  43, steps per second: 3448, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.628 [0.000, 1.000],  mean_best_reward: --\n",
      "  1962/100000: episode: 103, duration: 0.004s, episode steps:  12, steps per second: 3116, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  1974/100000: episode: 104, duration: 0.004s, episode steps:  12, steps per second: 3146, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      "  1987/100000: episode: 105, duration: 0.004s, episode steps:  13, steps per second: 3323, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  mean_best_reward: --\n",
      "  2001/100000: episode: 106, duration: 0.004s, episode steps:  14, steps per second: 3334, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  2011/100000: episode: 107, duration: 0.003s, episode steps:  10, steps per second: 3123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  mean_best_reward: --\n",
      "  2063/100000: episode: 108, duration: 0.016s, episode steps:  52, steps per second: 3224, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      "  2071/100000: episode: 109, duration: 0.003s, episode steps:   8, steps per second: 2811, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  2082/100000: episode: 110, duration: 0.004s, episode steps:  11, steps per second: 2908, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      "  2096/100000: episode: 111, duration: 0.004s, episode steps:  14, steps per second: 3132, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  2117/100000: episode: 112, duration: 0.006s, episode steps:  21, steps per second: 3302, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  2140/100000: episode: 113, duration: 0.007s, episode steps:  23, steps per second: 3140, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      "  2180/100000: episode: 114, duration: 0.011s, episode steps:  40, steps per second: 3514, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2201/100000: episode: 115, duration: 0.007s, episode steps:  21, steps per second: 2901, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  2226/100000: episode: 116, duration: 0.007s, episode steps:  25, steps per second: 3375, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      "  2252/100000: episode: 117, duration: 0.009s, episode steps:  26, steps per second: 2947, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  mean_best_reward: --\n",
      "  2271/100000: episode: 118, duration: 0.006s, episode steps:  19, steps per second: 3419, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  2294/100000: episode: 119, duration: 0.007s, episode steps:  23, steps per second: 3508, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  mean_best_reward: --\n",
      "  2312/100000: episode: 120, duration: 0.005s, episode steps:  18, steps per second: 3315, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  mean_best_reward: --\n",
      "  2322/100000: episode: 121, duration: 0.003s, episode steps:  10, steps per second: 3090, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      "  2340/100000: episode: 122, duration: 0.005s, episode steps:  18, steps per second: 3274, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  2354/100000: episode: 123, duration: 0.004s, episode steps:  14, steps per second: 3276, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  2365/100000: episode: 124, duration: 0.003s, episode steps:  11, steps per second: 3167, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  2377/100000: episode: 125, duration: 0.004s, episode steps:  12, steps per second: 2900, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      "  2413/100000: episode: 126, duration: 0.011s, episode steps:  36, steps per second: 3368, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      "  2435/100000: episode: 127, duration: 0.007s, episode steps:  22, steps per second: 3373, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2454/100000: episode: 128, duration: 0.006s, episode steps:  19, steps per second: 3374, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  2478/100000: episode: 129, duration: 0.007s, episode steps:  24, steps per second: 3390, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2489/100000: episode: 130, duration: 0.003s, episode steps:  11, steps per second: 3157, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      "  2504/100000: episode: 131, duration: 0.004s, episode steps:  15, steps per second: 3387, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      "  2532/100000: episode: 132, duration: 0.008s, episode steps:  28, steps per second: 3351, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2550/100000: episode: 133, duration: 0.006s, episode steps:  18, steps per second: 3159, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  2562/100000: episode: 134, duration: 0.004s, episode steps:  12, steps per second: 3179, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      "  2591/100000: episode: 135, duration: 0.008s, episode steps:  29, steps per second: 3517, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      "  2613/100000: episode: 136, duration: 0.007s, episode steps:  22, steps per second: 3329, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      "  2629/100000: episode: 137, duration: 0.006s, episode steps:  16, steps per second: 2867, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2640/100000: episode: 138, duration: 0.005s, episode steps:  11, steps per second: 2342, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  2652/100000: episode: 139, duration: 0.004s, episode steps:  12, steps per second: 2962, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  2676/100000: episode: 140, duration: 0.008s, episode steps:  24, steps per second: 3144, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      "  2699/100000: episode: 141, duration: 0.007s, episode steps:  23, steps per second: 3412, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  mean_best_reward: --\n",
      "  2708/100000: episode: 142, duration: 0.003s, episode steps:   9, steps per second: 3007, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  mean_best_reward: --\n",
      "  2743/100000: episode: 143, duration: 0.010s, episode steps:  35, steps per second: 3393, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.686 [0.000, 1.000],  mean_best_reward: --\n",
      "  2756/100000: episode: 144, duration: 0.004s, episode steps:  13, steps per second: 3250, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  2769/100000: episode: 145, duration: 0.004s, episode steps:  13, steps per second: 3174, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  mean_best_reward: --\n",
      "  2871/100000: episode: 146, duration: 0.030s, episode steps: 102, steps per second: 3381, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  2909/100000: episode: 147, duration: 0.011s, episode steps:  38, steps per second: 3511, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      "  2922/100000: episode: 148, duration: 0.004s, episode steps:  13, steps per second: 3065, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      "  2997/100000: episode: 149, duration: 0.022s, episode steps:  75, steps per second: 3345, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  3034/100000: episode: 150, duration: 0.011s, episode steps:  37, steps per second: 3507, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.351 [0.000, 1.000],  mean_best_reward: --\n",
      "  3051/100000: episode: 151, duration: 0.006s, episode steps:  17, steps per second: 2792, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  mean_best_reward: 50.000000\n",
      "  3115/100000: episode: 152, duration: 0.019s, episode steps:  64, steps per second: 3420, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      "  3133/100000: episode: 153, duration: 0.005s, episode steps:  18, steps per second: 3331, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  3161/100000: episode: 154, duration: 0.010s, episode steps:  28, steps per second: 2920, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  mean_best_reward: --\n",
      "  3221/100000: episode: 155, duration: 0.018s, episode steps:  60, steps per second: 3413, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      "  3242/100000: episode: 156, duration: 0.007s, episode steps:  21, steps per second: 3125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  3281/100000: episode: 157, duration: 0.012s, episode steps:  39, steps per second: 3230, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      "  3382/100000: episode: 158, duration: 0.031s, episode steps: 101, steps per second: 3215, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      "  3413/100000: episode: 159, duration: 0.009s, episode steps:  31, steps per second: 3448, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      "  3448/100000: episode: 160, duration: 0.011s, episode steps:  35, steps per second: 3126, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      "  3465/100000: episode: 161, duration: 0.006s, episode steps:  17, steps per second: 3087, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n",
      "  3474/100000: episode: 162, duration: 0.004s, episode steps:   9, steps per second: 2306, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  3552/100000: episode: 163, duration: 0.023s, episode steps:  78, steps per second: 3420, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      "  3567/100000: episode: 164, duration: 0.006s, episode steps:  15, steps per second: 2511, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  mean_best_reward: --\n",
      "  3613/100000: episode: 165, duration: 0.017s, episode steps:  46, steps per second: 2668, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  3632/100000: episode: 166, duration: 0.007s, episode steps:  19, steps per second: 2870, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  3650/100000: episode: 167, duration: 0.007s, episode steps:  18, steps per second: 2417, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      "  3672/100000: episode: 168, duration: 0.008s, episode steps:  22, steps per second: 2836, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      "  3697/100000: episode: 169, duration: 0.010s, episode steps:  25, steps per second: 2550, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      "  3709/100000: episode: 170, duration: 0.005s, episode steps:  12, steps per second: 2208, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  3725/100000: episode: 171, duration: 0.007s, episode steps:  16, steps per second: 2284, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      "  3757/100000: episode: 172, duration: 0.012s, episode steps:  32, steps per second: 2639, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      "  3772/100000: episode: 173, duration: 0.008s, episode steps:  15, steps per second: 1955, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  3801/100000: episode: 174, duration: 0.011s, episode steps:  29, steps per second: 2735, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      "  3857/100000: episode: 175, duration: 0.021s, episode steps:  56, steps per second: 2649, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      "  3885/100000: episode: 176, duration: 0.009s, episode steps:  28, steps per second: 3254, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  mean_best_reward: --\n",
      "  3897/100000: episode: 177, duration: 0.005s, episode steps:  12, steps per second: 2637, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  3912/100000: episode: 178, duration: 0.005s, episode steps:  15, steps per second: 3119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  3950/100000: episode: 179, duration: 0.011s, episode steps:  38, steps per second: 3444, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      "  3973/100000: episode: 180, duration: 0.006s, episode steps:  23, steps per second: 3546, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      "  3992/100000: episode: 181, duration: 0.006s, episode steps:  19, steps per second: 3138, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      "  4004/100000: episode: 182, duration: 0.004s, episode steps:  12, steps per second: 2720, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  4025/100000: episode: 183, duration: 0.006s, episode steps:  21, steps per second: 3460, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  4042/100000: episode: 184, duration: 0.005s, episode steps:  17, steps per second: 3305, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      "  4058/100000: episode: 185, duration: 0.005s, episode steps:  16, steps per second: 3198, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      "  4117/100000: episode: 186, duration: 0.016s, episode steps:  59, steps per second: 3769, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      "  4165/100000: episode: 187, duration: 0.014s, episode steps:  48, steps per second: 3454, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      "  4213/100000: episode: 188, duration: 0.013s, episode steps:  48, steps per second: 3680, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      "  4229/100000: episode: 189, duration: 0.005s, episode steps:  16, steps per second: 3421, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      "  4243/100000: episode: 190, duration: 0.004s, episode steps:  14, steps per second: 3244, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  4260/100000: episode: 191, duration: 0.005s, episode steps:  17, steps per second: 3188, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      "  4273/100000: episode: 192, duration: 0.005s, episode steps:  13, steps per second: 2640, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      "  4283/100000: episode: 193, duration: 0.003s, episode steps:  10, steps per second: 3003, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      "  4308/100000: episode: 194, duration: 0.007s, episode steps:  25, steps per second: 3518, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      "  4321/100000: episode: 195, duration: 0.004s, episode steps:  13, steps per second: 2928, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  4360/100000: episode: 196, duration: 0.011s, episode steps:  39, steps per second: 3571, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      "  4413/100000: episode: 197, duration: 0.015s, episode steps:  53, steps per second: 3567, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      "  4439/100000: episode: 198, duration: 0.008s, episode steps:  26, steps per second: 3067, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      "  4451/100000: episode: 199, duration: 0.004s, episode steps:  12, steps per second: 2689, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  4472/100000: episode: 200, duration: 0.007s, episode steps:  21, steps per second: 3137, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      "  4483/100000: episode: 201, duration: 0.004s, episode steps:  11, steps per second: 3028, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: 90.000000\n",
      "  4520/100000: episode: 202, duration: 0.011s, episode steps:  37, steps per second: 3334, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      "  4533/100000: episode: 203, duration: 0.004s, episode steps:  13, steps per second: 3166, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  4570/100000: episode: 204, duration: 0.011s, episode steps:  37, steps per second: 3447, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      "  4580/100000: episode: 205, duration: 0.004s, episode steps:  10, steps per second: 2411, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      "  4594/100000: episode: 206, duration: 0.004s, episode steps:  14, steps per second: 3175, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n",
      "  4619/100000: episode: 207, duration: 0.007s, episode steps:  25, steps per second: 3361, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      "  4634/100000: episode: 208, duration: 0.006s, episode steps:  15, steps per second: 2365, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  4652/100000: episode: 209, duration: 0.006s, episode steps:  18, steps per second: 3004, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  4669/100000: episode: 210, duration: 0.007s, episode steps:  17, steps per second: 2291, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  4683/100000: episode: 211, duration: 0.004s, episode steps:  14, steps per second: 3343, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "  4709/100000: episode: 212, duration: 0.009s, episode steps:  26, steps per second: 3034, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  4733/100000: episode: 213, duration: 0.007s, episode steps:  24, steps per second: 3462, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      "  4757/100000: episode: 214, duration: 0.008s, episode steps:  24, steps per second: 3059, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      "  4790/100000: episode: 215, duration: 0.009s, episode steps:  33, steps per second: 3558, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      "  4812/100000: episode: 216, duration: 0.007s, episode steps:  22, steps per second: 3128, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      "  4837/100000: episode: 217, duration: 0.009s, episode steps:  25, steps per second: 2668, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      "  4874/100000: episode: 218, duration: 0.011s, episode steps:  37, steps per second: 3311, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  mean_best_reward: --\n",
      "  4907/100000: episode: 219, duration: 0.010s, episode steps:  33, steps per second: 3279, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "  4937/100000: episode: 220, duration: 0.008s, episode steps:  30, steps per second: 3637, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  mean_best_reward: --\n",
      "  4974/100000: episode: 221, duration: 0.012s, episode steps:  37, steps per second: 3139, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      "  5002/100000: episode: 222, duration: 0.009s, episode steps:  28, steps per second: 3173, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      "  5023/100000: episode: 223, duration: 0.007s, episode steps:  21, steps per second: 3180, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  5069/100000: episode: 224, duration: 0.013s, episode steps:  46, steps per second: 3502, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  5111/100000: episode: 225, duration: 0.012s, episode steps:  42, steps per second: 3410, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      "  5130/100000: episode: 226, duration: 0.006s, episode steps:  19, steps per second: 3122, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      "  5144/100000: episode: 227, duration: 0.004s, episode steps:  14, steps per second: 3296, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "  5156/100000: episode: 228, duration: 0.004s, episode steps:  12, steps per second: 3284, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      "  5168/100000: episode: 229, duration: 0.004s, episode steps:  12, steps per second: 3288, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  5226/100000: episode: 230, duration: 0.016s, episode steps:  58, steps per second: 3698, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  5242/100000: episode: 231, duration: 0.005s, episode steps:  16, steps per second: 3367, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  5267/100000: episode: 232, duration: 0.007s, episode steps:  25, steps per second: 3408, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      "  5286/100000: episode: 233, duration: 0.006s, episode steps:  19, steps per second: 2942, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  5299/100000: episode: 234, duration: 0.004s, episode steps:  13, steps per second: 3332, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      "  5328/100000: episode: 235, duration: 0.008s, episode steps:  29, steps per second: 3599, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      "  5357/100000: episode: 236, duration: 0.008s, episode steps:  29, steps per second: 3548, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      "  5368/100000: episode: 237, duration: 0.004s, episode steps:  11, steps per second: 3010, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  mean_best_reward: --\n",
      "  5431/100000: episode: 238, duration: 0.018s, episode steps:  63, steps per second: 3541, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      "  5474/100000: episode: 239, duration: 0.012s, episode steps:  43, steps per second: 3679, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      "  5532/100000: episode: 240, duration: 0.016s, episode steps:  58, steps per second: 3536, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  mean_best_reward: --\n",
      "  5551/100000: episode: 241, duration: 0.008s, episode steps:  19, steps per second: 2430, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "  5610/100000: episode: 242, duration: 0.017s, episode steps:  59, steps per second: 3459, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      "  5622/100000: episode: 243, duration: 0.004s, episode steps:  12, steps per second: 3057, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  5652/100000: episode: 244, duration: 0.008s, episode steps:  30, steps per second: 3588, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  5685/100000: episode: 245, duration: 0.010s, episode steps:  33, steps per second: 3414, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      "  5714/100000: episode: 246, duration: 0.010s, episode steps:  29, steps per second: 3034, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      "  5745/100000: episode: 247, duration: 0.010s, episode steps:  31, steps per second: 2962, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      "  5772/100000: episode: 248, duration: 0.008s, episode steps:  27, steps per second: 3372, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      "  5787/100000: episode: 249, duration: 0.005s, episode steps:  15, steps per second: 2974, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  5808/100000: episode: 250, duration: 0.007s, episode steps:  21, steps per second: 3185, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  5839/100000: episode: 251, duration: 0.010s, episode steps:  31, steps per second: 3248, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  mean_best_reward: 59.500000\n",
      "  5860/100000: episode: 252, duration: 0.006s, episode steps:  21, steps per second: 3425, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      "  5874/100000: episode: 253, duration: 0.004s, episode steps:  14, steps per second: 3298, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  5896/100000: episode: 254, duration: 0.007s, episode steps:  22, steps per second: 3266, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  5911/100000: episode: 255, duration: 0.006s, episode steps:  15, steps per second: 2717, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      "  5935/100000: episode: 256, duration: 0.009s, episode steps:  24, steps per second: 2819, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  5958/100000: episode: 257, duration: 0.008s, episode steps:  23, steps per second: 3018, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      "  6006/100000: episode: 258, duration: 0.015s, episode steps:  48, steps per second: 3217, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      "  6019/100000: episode: 259, duration: 0.004s, episode steps:  13, steps per second: 3343, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  6032/100000: episode: 260, duration: 0.005s, episode steps:  13, steps per second: 2428, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  6062/100000: episode: 261, duration: 0.010s, episode steps:  30, steps per second: 3101, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  6091/100000: episode: 262, duration: 0.009s, episode steps:  29, steps per second: 3409, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      "  6135/100000: episode: 263, duration: 0.015s, episode steps:  44, steps per second: 2887, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      "  6170/100000: episode: 264, duration: 0.011s, episode steps:  35, steps per second: 3272, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      "  6199/100000: episode: 265, duration: 0.008s, episode steps:  29, steps per second: 3442, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      "  6272/100000: episode: 266, duration: 0.019s, episode steps:  73, steps per second: 3785, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      "  6315/100000: episode: 267, duration: 0.013s, episode steps:  43, steps per second: 3395, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      "  6324/100000: episode: 268, duration: 0.003s, episode steps:   9, steps per second: 2775, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  mean_best_reward: --\n",
      "  6347/100000: episode: 269, duration: 0.007s, episode steps:  23, steps per second: 3498, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      "  6362/100000: episode: 270, duration: 0.004s, episode steps:  15, steps per second: 3337, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  6373/100000: episode: 271, duration: 0.003s, episode steps:  11, steps per second: 3201, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  mean_best_reward: --\n",
      "  6403/100000: episode: 272, duration: 0.008s, episode steps:  30, steps per second: 3573, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      "  6419/100000: episode: 273, duration: 0.005s, episode steps:  16, steps per second: 3315, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      "  6444/100000: episode: 274, duration: 0.007s, episode steps:  25, steps per second: 3548, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      "  6455/100000: episode: 275, duration: 0.004s, episode steps:  11, steps per second: 2985, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      "  6521/100000: episode: 276, duration: 0.018s, episode steps:  66, steps per second: 3660, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  mean_best_reward: --\n",
      "  6544/100000: episode: 277, duration: 0.006s, episode steps:  23, steps per second: 3560, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      "  6561/100000: episode: 278, duration: 0.005s, episode steps:  17, steps per second: 3357, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      "  6579/100000: episode: 279, duration: 0.005s, episode steps:  18, steps per second: 3412, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  6624/100000: episode: 280, duration: 0.012s, episode steps:  45, steps per second: 3707, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      "  6650/100000: episode: 281, duration: 0.008s, episode steps:  26, steps per second: 3345, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      "  6684/100000: episode: 282, duration: 0.010s, episode steps:  34, steps per second: 3284, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      "  6717/100000: episode: 283, duration: 0.010s, episode steps:  33, steps per second: 3362, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      "  6742/100000: episode: 284, duration: 0.008s, episode steps:  25, steps per second: 3186, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      "  6755/100000: episode: 285, duration: 0.004s, episode steps:  13, steps per second: 3323, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      "  6801/100000: episode: 286, duration: 0.013s, episode steps:  46, steps per second: 3452, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      "  6840/100000: episode: 287, duration: 0.012s, episode steps:  39, steps per second: 3374, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      "  6898/100000: episode: 288, duration: 0.016s, episode steps:  58, steps per second: 3556, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      "  6917/100000: episode: 289, duration: 0.006s, episode steps:  19, steps per second: 3357, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "  6929/100000: episode: 290, duration: 0.004s, episode steps:  12, steps per second: 3096, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  7033/100000: episode: 291, duration: 0.036s, episode steps: 104, steps per second: 2899, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      "  7053/100000: episode: 292, duration: 0.010s, episode steps:  20, steps per second: 2022, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  7075/100000: episode: 293, duration: 0.008s, episode steps:  22, steps per second: 2621, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "  7089/100000: episode: 294, duration: 0.007s, episode steps:  14, steps per second: 2042, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  7104/100000: episode: 295, duration: 0.006s, episode steps:  15, steps per second: 2640, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  7120/100000: episode: 296, duration: 0.009s, episode steps:  16, steps per second: 1876, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      "  7139/100000: episode: 297, duration: 0.007s, episode steps:  19, steps per second: 2842, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "  7160/100000: episode: 298, duration: 0.011s, episode steps:  21, steps per second: 1924, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      "  7176/100000: episode: 299, duration: 0.008s, episode steps:  16, steps per second: 1918, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      "  7193/100000: episode: 300, duration: 0.006s, episode steps:  17, steps per second: 3083, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  7216/100000: episode: 301, duration: 0.008s, episode steps:  23, steps per second: 2905, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: 82.500000\n",
      "  7248/100000: episode: 302, duration: 0.009s, episode steps:  32, steps per second: 3571, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      "  7361/100000: episode: 303, duration: 0.033s, episode steps: 113, steps per second: 3423, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      "  7389/100000: episode: 304, duration: 0.010s, episode steps:  28, steps per second: 2847, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      "  7407/100000: episode: 305, duration: 0.005s, episode steps:  18, steps per second: 3365, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  7420/100000: episode: 306, duration: 0.005s, episode steps:  13, steps per second: 2807, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  7452/100000: episode: 307, duration: 0.009s, episode steps:  32, steps per second: 3476, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  7476/100000: episode: 308, duration: 0.007s, episode steps:  24, steps per second: 3263, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      "  7491/100000: episode: 309, duration: 0.005s, episode steps:  15, steps per second: 2985, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  7571/100000: episode: 310, duration: 0.022s, episode steps:  80, steps per second: 3655, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      "  7604/100000: episode: 311, duration: 0.009s, episode steps:  33, steps per second: 3614, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "  7638/100000: episode: 312, duration: 0.009s, episode steps:  34, steps per second: 3718, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  7661/100000: episode: 313, duration: 0.007s, episode steps:  23, steps per second: 3520, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      "  7717/100000: episode: 314, duration: 0.016s, episode steps:  56, steps per second: 3589, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  mean_best_reward: --\n",
      "  7746/100000: episode: 315, duration: 0.008s, episode steps:  29, steps per second: 3522, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      "  7757/100000: episode: 316, duration: 0.004s, episode steps:  11, steps per second: 2911, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  mean_best_reward: --\n",
      "  7787/100000: episode: 317, duration: 0.008s, episode steps:  30, steps per second: 3544, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  7820/100000: episode: 318, duration: 0.009s, episode steps:  33, steps per second: 3600, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      "  7851/100000: episode: 319, duration: 0.009s, episode steps:  31, steps per second: 3526, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      "  7882/100000: episode: 320, duration: 0.010s, episode steps:  31, steps per second: 3149, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      "  7917/100000: episode: 321, duration: 0.011s, episode steps:  35, steps per second: 3194, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      "  7936/100000: episode: 322, duration: 0.006s, episode steps:  19, steps per second: 3209, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  mean_best_reward: --\n",
      "  7950/100000: episode: 323, duration: 0.004s, episode steps:  14, steps per second: 3357, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  7990/100000: episode: 324, duration: 0.011s, episode steps:  40, steps per second: 3670, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      "  8040/100000: episode: 325, duration: 0.014s, episode steps:  50, steps per second: 3486, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      "  8061/100000: episode: 326, duration: 0.006s, episode steps:  21, steps per second: 3427, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  mean_best_reward: --\n",
      "  8115/100000: episode: 327, duration: 0.016s, episode steps:  54, steps per second: 3457, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      "  8171/100000: episode: 328, duration: 0.016s, episode steps:  56, steps per second: 3546, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      "  8212/100000: episode: 329, duration: 0.012s, episode steps:  41, steps per second: 3534, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      "  8361/100000: episode: 330, duration: 0.040s, episode steps: 149, steps per second: 3714, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  mean_best_reward: --\n",
      "  8379/100000: episode: 331, duration: 0.006s, episode steps:  18, steps per second: 3181, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      "  8406/100000: episode: 332, duration: 0.008s, episode steps:  27, steps per second: 3510, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      "  8476/100000: episode: 333, duration: 0.019s, episode steps:  70, steps per second: 3626, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  8497/100000: episode: 334, duration: 0.008s, episode steps:  21, steps per second: 2737, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      "  8554/100000: episode: 335, duration: 0.018s, episode steps:  57, steps per second: 3228, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  mean_best_reward: --\n",
      "  8597/100000: episode: 336, duration: 0.013s, episode steps:  43, steps per second: 3433, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      "  8675/100000: episode: 337, duration: 0.024s, episode steps:  78, steps per second: 3303, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  8735/100000: episode: 338, duration: 0.018s, episode steps:  60, steps per second: 3320, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  8792/100000: episode: 339, duration: 0.017s, episode steps:  57, steps per second: 3348, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      "  8842/100000: episode: 340, duration: 0.014s, episode steps:  50, steps per second: 3593, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      "  8901/100000: episode: 341, duration: 0.019s, episode steps:  59, steps per second: 3146, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      "  8969/100000: episode: 342, duration: 0.019s, episode steps:  68, steps per second: 3640, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9013/100000: episode: 343, duration: 0.012s, episode steps:  44, steps per second: 3534, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9029/100000: episode: 344, duration: 0.005s, episode steps:  16, steps per second: 3294, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      "  9063/100000: episode: 345, duration: 0.009s, episode steps:  34, steps per second: 3601, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9098/100000: episode: 346, duration: 0.011s, episode steps:  35, steps per second: 3199, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      "  9125/100000: episode: 347, duration: 0.008s, episode steps:  27, steps per second: 3225, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      "  9140/100000: episode: 348, duration: 0.005s, episode steps:  15, steps per second: 3115, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  9159/100000: episode: 349, duration: 0.006s, episode steps:  19, steps per second: 3388, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  9210/100000: episode: 350, duration: 0.014s, episode steps:  51, steps per second: 3708, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  9240/100000: episode: 351, duration: 0.009s, episode steps:  30, steps per second: 3302, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: 75.000000\n",
      "  9261/100000: episode: 352, duration: 0.006s, episode steps:  21, steps per second: 3293, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      "  9295/100000: episode: 353, duration: 0.009s, episode steps:  34, steps per second: 3592, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      "  9314/100000: episode: 354, duration: 0.005s, episode steps:  19, steps per second: 3458, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "  9356/100000: episode: 355, duration: 0.012s, episode steps:  42, steps per second: 3431, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      "  9420/100000: episode: 356, duration: 0.018s, episode steps:  64, steps per second: 3652, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      "  9450/100000: episode: 357, duration: 0.009s, episode steps:  30, steps per second: 3297, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  9475/100000: episode: 358, duration: 0.007s, episode steps:  25, steps per second: 3594, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      "  9512/100000: episode: 359, duration: 0.010s, episode steps:  37, steps per second: 3696, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      "  9548/100000: episode: 360, duration: 0.010s, episode steps:  36, steps per second: 3517, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      "  9588/100000: episode: 361, duration: 0.011s, episode steps:  40, steps per second: 3661, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      "  9618/100000: episode: 362, duration: 0.009s, episode steps:  30, steps per second: 3452, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  9727/100000: episode: 363, duration: 0.030s, episode steps: 109, steps per second: 3596, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      "  9859/100000: episode: 364, duration: 0.036s, episode steps: 132, steps per second: 3648, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      "  9904/100000: episode: 365, duration: 0.012s, episode steps:  45, steps per second: 3714, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  9999/100000: episode: 366, duration: 0.025s, episode steps:  95, steps per second: 3737, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  mean_best_reward: --\n",
      " 10077/100000: episode: 367, duration: 0.022s, episode steps:  78, steps per second: 3604, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 10108/100000: episode: 368, duration: 0.009s, episode steps:  31, steps per second: 3427, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 10151/100000: episode: 369, duration: 0.013s, episode steps:  43, steps per second: 3375, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 10185/100000: episode: 370, duration: 0.010s, episode steps:  34, steps per second: 3287, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 10222/100000: episode: 371, duration: 0.011s, episode steps:  37, steps per second: 3397, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 10333/100000: episode: 372, duration: 0.033s, episode steps: 111, steps per second: 3373, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 10431/100000: episode: 373, duration: 0.027s, episode steps:  98, steps per second: 3577, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 10479/100000: episode: 374, duration: 0.014s, episode steps:  48, steps per second: 3427, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 10560/100000: episode: 375, duration: 0.026s, episode steps:  81, steps per second: 3096, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 10586/100000: episode: 376, duration: 0.010s, episode steps:  26, steps per second: 2525, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 10623/100000: episode: 377, duration: 0.016s, episode steps:  37, steps per second: 2261, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 10676/100000: episode: 378, duration: 0.022s, episode steps:  53, steps per second: 2425, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 10727/100000: episode: 379, duration: 0.015s, episode steps:  51, steps per second: 3353, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 10776/100000: episode: 380, duration: 0.015s, episode steps:  49, steps per second: 3351, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 10826/100000: episode: 381, duration: 0.014s, episode steps:  50, steps per second: 3641, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 10855/100000: episode: 382, duration: 0.008s, episode steps:  29, steps per second: 3505, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 10918/100000: episode: 383, duration: 0.017s, episode steps:  63, steps per second: 3764, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 10941/100000: episode: 384, duration: 0.007s, episode steps:  23, steps per second: 3324, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 11017/100000: episode: 385, duration: 0.021s, episode steps:  76, steps per second: 3583, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 11051/100000: episode: 386, duration: 0.010s, episode steps:  34, steps per second: 3540, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 11099/100000: episode: 387, duration: 0.013s, episode steps:  48, steps per second: 3680, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 11118/100000: episode: 388, duration: 0.006s, episode steps:  19, steps per second: 3187, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 11197/100000: episode: 389, duration: 0.022s, episode steps:  79, steps per second: 3562, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 11235/100000: episode: 390, duration: 0.011s, episode steps:  38, steps per second: 3582, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 11269/100000: episode: 391, duration: 0.010s, episode steps:  34, steps per second: 3531, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 11336/100000: episode: 392, duration: 0.019s, episode steps:  67, steps per second: 3588, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 11399/100000: episode: 393, duration: 0.020s, episode steps:  63, steps per second: 3231, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 11418/100000: episode: 394, duration: 0.006s, episode steps:  19, steps per second: 3346, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 11507/100000: episode: 395, duration: 0.027s, episode steps:  89, steps per second: 3279, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 11602/100000: episode: 396, duration: 0.028s, episode steps:  95, steps per second: 3421, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  mean_best_reward: --\n",
      " 11682/100000: episode: 397, duration: 0.025s, episode steps:  80, steps per second: 3181, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 11779/100000: episode: 398, duration: 0.027s, episode steps:  97, steps per second: 3577, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 11812/100000: episode: 399, duration: 0.010s, episode steps:  33, steps per second: 3387, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 11827/100000: episode: 400, duration: 0.005s, episode steps:  15, steps per second: 2968, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 11900/100000: episode: 401, duration: 0.021s, episode steps:  73, steps per second: 3436, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: 83.500000\n",
      " 11919/100000: episode: 402, duration: 0.006s, episode steps:  19, steps per second: 3237, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 11962/100000: episode: 403, duration: 0.013s, episode steps:  43, steps per second: 3434, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 11979/100000: episode: 404, duration: 0.006s, episode steps:  17, steps per second: 2982, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 12002/100000: episode: 405, duration: 0.007s, episode steps:  23, steps per second: 3367, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 12050/100000: episode: 406, duration: 0.014s, episode steps:  48, steps per second: 3544, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 12162/100000: episode: 407, duration: 0.030s, episode steps: 112, steps per second: 3786, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 12178/100000: episode: 408, duration: 0.005s, episode steps:  16, steps per second: 3057, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 12206/100000: episode: 409, duration: 0.008s, episode steps:  28, steps per second: 3568, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 12231/100000: episode: 410, duration: 0.008s, episode steps:  25, steps per second: 3318, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 12271/100000: episode: 411, duration: 0.011s, episode steps:  40, steps per second: 3613, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 12348/100000: episode: 412, duration: 0.023s, episode steps:  77, steps per second: 3351, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 12407/100000: episode: 413, duration: 0.017s, episode steps:  59, steps per second: 3551, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 12469/100000: episode: 414, duration: 0.017s, episode steps:  62, steps per second: 3678, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 12492/100000: episode: 415, duration: 0.008s, episode steps:  23, steps per second: 2894, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 12522/100000: episode: 416, duration: 0.009s, episode steps:  30, steps per second: 3420, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  mean_best_reward: --\n",
      " 12554/100000: episode: 417, duration: 0.010s, episode steps:  32, steps per second: 3303, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 12625/100000: episode: 418, duration: 0.020s, episode steps:  71, steps per second: 3491, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 12687/100000: episode: 419, duration: 0.020s, episode steps:  62, steps per second: 3098, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 12730/100000: episode: 420, duration: 0.014s, episode steps:  43, steps per second: 3107, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 12755/100000: episode: 421, duration: 0.008s, episode steps:  25, steps per second: 3069, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 12794/100000: episode: 422, duration: 0.012s, episode steps:  39, steps per second: 3359, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 12815/100000: episode: 423, duration: 0.007s, episode steps:  21, steps per second: 2874, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 12867/100000: episode: 424, duration: 0.016s, episode steps:  52, steps per second: 3339, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 12960/100000: episode: 425, duration: 0.027s, episode steps:  93, steps per second: 3423, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 12993/100000: episode: 426, duration: 0.009s, episode steps:  33, steps per second: 3599, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 13056/100000: episode: 427, duration: 0.018s, episode steps:  63, steps per second: 3474, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 13085/100000: episode: 428, duration: 0.009s, episode steps:  29, steps per second: 3227, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 13110/100000: episode: 429, duration: 0.009s, episode steps:  25, steps per second: 2677, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 13143/100000: episode: 430, duration: 0.010s, episode steps:  33, steps per second: 3310, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 13198/100000: episode: 431, duration: 0.016s, episode steps:  55, steps per second: 3481, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.582 [0.000, 1.000],  mean_best_reward: --\n",
      " 13244/100000: episode: 432, duration: 0.014s, episode steps:  46, steps per second: 3369, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 13293/100000: episode: 433, duration: 0.015s, episode steps:  49, steps per second: 3315, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 13335/100000: episode: 434, duration: 0.013s, episode steps:  42, steps per second: 3112, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 13368/100000: episode: 435, duration: 0.009s, episode steps:  33, steps per second: 3477, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  mean_best_reward: --\n",
      " 13403/100000: episode: 436, duration: 0.010s, episode steps:  35, steps per second: 3515, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 13446/100000: episode: 437, duration: 0.012s, episode steps:  43, steps per second: 3531, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 13463/100000: episode: 438, duration: 0.005s, episode steps:  17, steps per second: 3273, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 13480/100000: episode: 439, duration: 0.005s, episode steps:  17, steps per second: 3428, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 13499/100000: episode: 440, duration: 0.007s, episode steps:  19, steps per second: 2831, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 13525/100000: episode: 441, duration: 0.007s, episode steps:  26, steps per second: 3544, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13549/100000: episode: 442, duration: 0.007s, episode steps:  24, steps per second: 3390, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13575/100000: episode: 443, duration: 0.007s, episode steps:  26, steps per second: 3511, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13588/100000: episode: 444, duration: 0.004s, episode steps:  13, steps per second: 3170, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      " 13641/100000: episode: 445, duration: 0.015s, episode steps:  53, steps per second: 3590, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 13715/100000: episode: 446, duration: 0.020s, episode steps:  74, steps per second: 3612, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  mean_best_reward: --\n",
      " 13761/100000: episode: 447, duration: 0.013s, episode steps:  46, steps per second: 3650, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13791/100000: episode: 448, duration: 0.008s, episode steps:  30, steps per second: 3610, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  mean_best_reward: --\n",
      " 13844/100000: episode: 449, duration: 0.015s, episode steps:  53, steps per second: 3449, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 13864/100000: episode: 450, duration: 0.006s, episode steps:  20, steps per second: 3474, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13898/100000: episode: 451, duration: 0.010s, episode steps:  34, steps per second: 3508, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: 111.000000\n",
      " 13926/100000: episode: 452, duration: 0.008s, episode steps:  28, steps per second: 3536, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 13986/100000: episode: 453, duration: 0.016s, episode steps:  60, steps per second: 3757, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 14003/100000: episode: 454, duration: 0.006s, episode steps:  17, steps per second: 2840, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 14038/100000: episode: 455, duration: 0.010s, episode steps:  35, steps per second: 3507, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 14091/100000: episode: 456, duration: 0.016s, episode steps:  53, steps per second: 3366, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 14134/100000: episode: 457, duration: 0.015s, episode steps:  43, steps per second: 2939, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 14231/100000: episode: 458, duration: 0.034s, episode steps:  97, steps per second: 2860, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 14252/100000: episode: 459, duration: 0.007s, episode steps:  21, steps per second: 2880, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 14277/100000: episode: 460, duration: 0.008s, episode steps:  25, steps per second: 3165, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 14308/100000: episode: 461, duration: 0.011s, episode steps:  31, steps per second: 2814, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 14346/100000: episode: 462, duration: 0.011s, episode steps:  38, steps per second: 3607, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 14412/100000: episode: 463, duration: 0.017s, episode steps:  66, steps per second: 3788, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  mean_best_reward: --\n",
      " 14510/100000: episode: 464, duration: 0.027s, episode steps:  98, steps per second: 3677, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 14528/100000: episode: 465, duration: 0.006s, episode steps:  18, steps per second: 2982, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 14559/100000: episode: 466, duration: 0.011s, episode steps:  31, steps per second: 2798, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 14647/100000: episode: 467, duration: 0.029s, episode steps:  88, steps per second: 2985, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 14674/100000: episode: 468, duration: 0.009s, episode steps:  27, steps per second: 3119, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 14778/100000: episode: 469, duration: 0.031s, episode steps: 104, steps per second: 3395, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 14809/100000: episode: 470, duration: 0.009s, episode steps:  31, steps per second: 3504, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 14834/100000: episode: 471, duration: 0.007s, episode steps:  25, steps per second: 3453, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 14879/100000: episode: 472, duration: 0.013s, episode steps:  45, steps per second: 3514, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 14919/100000: episode: 473, duration: 0.013s, episode steps:  40, steps per second: 2997, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 14957/100000: episode: 474, duration: 0.012s, episode steps:  38, steps per second: 3089, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 14975/100000: episode: 475, duration: 0.006s, episode steps:  18, steps per second: 3129, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 15035/100000: episode: 476, duration: 0.017s, episode steps:  60, steps per second: 3627, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 15077/100000: episode: 477, duration: 0.012s, episode steps:  42, steps per second: 3537, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 15115/100000: episode: 478, duration: 0.012s, episode steps:  38, steps per second: 3230, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 15140/100000: episode: 479, duration: 0.008s, episode steps:  25, steps per second: 3150, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 15171/100000: episode: 480, duration: 0.008s, episode steps:  31, steps per second: 3657, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 15205/100000: episode: 481, duration: 0.011s, episode steps:  34, steps per second: 3079, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 15225/100000: episode: 482, duration: 0.006s, episode steps:  20, steps per second: 3470, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  mean_best_reward: --\n",
      " 15279/100000: episode: 483, duration: 0.015s, episode steps:  54, steps per second: 3524, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 15328/100000: episode: 484, duration: 0.016s, episode steps:  49, steps per second: 3153, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 15360/100000: episode: 485, duration: 0.010s, episode steps:  32, steps per second: 3276, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 15399/100000: episode: 486, duration: 0.011s, episode steps:  39, steps per second: 3450, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 15409/100000: episode: 487, duration: 0.003s, episode steps:  10, steps per second: 3060, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      " 15461/100000: episode: 488, duration: 0.014s, episode steps:  52, steps per second: 3656, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 15498/100000: episode: 489, duration: 0.010s, episode steps:  37, steps per second: 3553, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 15531/100000: episode: 490, duration: 0.011s, episode steps:  33, steps per second: 2980, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 15584/100000: episode: 491, duration: 0.016s, episode steps:  53, steps per second: 3274, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 15623/100000: episode: 492, duration: 0.012s, episode steps:  39, steps per second: 3354, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 15642/100000: episode: 493, duration: 0.005s, episode steps:  19, steps per second: 3471, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 15668/100000: episode: 494, duration: 0.007s, episode steps:  26, steps per second: 3521, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 15736/100000: episode: 495, duration: 0.019s, episode steps:  68, steps per second: 3554, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 15787/100000: episode: 496, duration: 0.014s, episode steps:  51, steps per second: 3700, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 15811/100000: episode: 497, duration: 0.007s, episode steps:  24, steps per second: 3423, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 15856/100000: episode: 498, duration: 0.013s, episode steps:  45, steps per second: 3567, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 15892/100000: episode: 499, duration: 0.010s, episode steps:  36, steps per second: 3470, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 15928/100000: episode: 500, duration: 0.011s, episode steps:  36, steps per second: 3261, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 15986/100000: episode: 501, duration: 0.016s, episode steps:  58, steps per second: 3634, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 63.000000\n",
      " 16025/100000: episode: 502, duration: 0.012s, episode steps:  39, steps per second: 3229, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 16048/100000: episode: 503, duration: 0.007s, episode steps:  23, steps per second: 3488, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 16066/100000: episode: 504, duration: 0.006s, episode steps:  18, steps per second: 3090, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 16122/100000: episode: 505, duration: 0.015s, episode steps:  56, steps per second: 3713, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 16137/100000: episode: 506, duration: 0.004s, episode steps:  15, steps per second: 3353, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 16172/100000: episode: 507, duration: 0.010s, episode steps:  35, steps per second: 3491, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 16207/100000: episode: 508, duration: 0.010s, episode steps:  35, steps per second: 3455, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 16234/100000: episode: 509, duration: 0.008s, episode steps:  27, steps per second: 3443, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 16277/100000: episode: 510, duration: 0.012s, episode steps:  43, steps per second: 3519, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 16320/100000: episode: 511, duration: 0.014s, episode steps:  43, steps per second: 3183, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 16351/100000: episode: 512, duration: 0.009s, episode steps:  31, steps per second: 3351, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 16380/100000: episode: 513, duration: 0.009s, episode steps:  29, steps per second: 3405, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 16455/100000: episode: 514, duration: 0.021s, episode steps:  75, steps per second: 3571, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 16484/100000: episode: 515, duration: 0.008s, episode steps:  29, steps per second: 3550, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 16560/100000: episode: 516, duration: 0.024s, episode steps:  76, steps per second: 3168, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 16598/100000: episode: 517, duration: 0.011s, episode steps:  38, steps per second: 3618, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 16641/100000: episode: 518, duration: 0.012s, episode steps:  43, steps per second: 3696, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 16690/100000: episode: 519, duration: 0.015s, episode steps:  49, steps per second: 3308, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  mean_best_reward: --\n",
      " 16705/100000: episode: 520, duration: 0.005s, episode steps:  15, steps per second: 2761, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 16717/100000: episode: 521, duration: 0.004s, episode steps:  12, steps per second: 2902, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 16736/100000: episode: 522, duration: 0.006s, episode steps:  19, steps per second: 3419, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 16772/100000: episode: 523, duration: 0.011s, episode steps:  36, steps per second: 3399, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 16788/100000: episode: 524, duration: 0.005s, episode steps:  16, steps per second: 3228, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 16810/100000: episode: 525, duration: 0.007s, episode steps:  22, steps per second: 3155, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 16848/100000: episode: 526, duration: 0.012s, episode steps:  38, steps per second: 3149, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 16897/100000: episode: 527, duration: 0.014s, episode steps:  49, steps per second: 3381, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 16994/100000: episode: 528, duration: 0.026s, episode steps:  97, steps per second: 3755, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.454 [0.000, 1.000],  mean_best_reward: --\n",
      " 17014/100000: episode: 529, duration: 0.006s, episode steps:  20, steps per second: 3476, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 17042/100000: episode: 530, duration: 0.009s, episode steps:  28, steps per second: 3116, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 17092/100000: episode: 531, duration: 0.014s, episode steps:  50, steps per second: 3675, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 17113/100000: episode: 532, duration: 0.007s, episode steps:  21, steps per second: 2996, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 17140/100000: episode: 533, duration: 0.008s, episode steps:  27, steps per second: 3549, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 17165/100000: episode: 534, duration: 0.007s, episode steps:  25, steps per second: 3397, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 17212/100000: episode: 535, duration: 0.014s, episode steps:  47, steps per second: 3328, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 17267/100000: episode: 536, duration: 0.016s, episode steps:  55, steps per second: 3420, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 17383/100000: episode: 537, duration: 0.032s, episode steps: 116, steps per second: 3654, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  mean_best_reward: --\n",
      " 17412/100000: episode: 538, duration: 0.008s, episode steps:  29, steps per second: 3463, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 17432/100000: episode: 539, duration: 0.006s, episode steps:  20, steps per second: 3343, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 17504/100000: episode: 540, duration: 0.021s, episode steps:  72, steps per second: 3459, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 17535/100000: episode: 541, duration: 0.011s, episode steps:  31, steps per second: 2878, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 17589/100000: episode: 542, duration: 0.020s, episode steps:  54, steps per second: 2696, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 17670/100000: episode: 543, duration: 0.030s, episode steps:  81, steps per second: 2686, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 17700/100000: episode: 544, duration: 0.014s, episode steps:  30, steps per second: 2177, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 17758/100000: episode: 545, duration: 0.018s, episode steps:  58, steps per second: 3258, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 17799/100000: episode: 546, duration: 0.013s, episode steps:  41, steps per second: 3070, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 17830/100000: episode: 547, duration: 0.010s, episode steps:  31, steps per second: 3228, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 17845/100000: episode: 548, duration: 0.006s, episode steps:  15, steps per second: 2518, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 17929/100000: episode: 549, duration: 0.025s, episode steps:  84, steps per second: 3395, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 17983/100000: episode: 550, duration: 0.016s, episode steps:  54, steps per second: 3409, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 18000/100000: episode: 551, duration: 0.007s, episode steps:  17, steps per second: 2603, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: 98.500000\n",
      " 18027/100000: episode: 552, duration: 0.009s, episode steps:  27, steps per second: 3102, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 18049/100000: episode: 553, duration: 0.008s, episode steps:  22, steps per second: 2722, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18095/100000: episode: 554, duration: 0.013s, episode steps:  46, steps per second: 3513, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 18149/100000: episode: 555, duration: 0.015s, episode steps:  54, steps per second: 3530, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 18190/100000: episode: 556, duration: 0.011s, episode steps:  41, steps per second: 3635, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 18243/100000: episode: 557, duration: 0.015s, episode steps:  53, steps per second: 3593, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 18280/100000: episode: 558, duration: 0.012s, episode steps:  37, steps per second: 3160, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 18335/100000: episode: 559, duration: 0.015s, episode steps:  55, steps per second: 3602, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 18380/100000: episode: 560, duration: 0.013s, episode steps:  45, steps per second: 3503, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 18412/100000: episode: 561, duration: 0.010s, episode steps:  32, steps per second: 3223, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 18463/100000: episode: 562, duration: 0.015s, episode steps:  51, steps per second: 3373, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 18518/100000: episode: 563, duration: 0.015s, episode steps:  55, steps per second: 3655, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 18562/100000: episode: 564, duration: 0.012s, episode steps:  44, steps per second: 3681, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18595/100000: episode: 565, duration: 0.009s, episode steps:  33, steps per second: 3670, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 18636/100000: episode: 566, duration: 0.011s, episode steps:  41, steps per second: 3619, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 18658/100000: episode: 567, duration: 0.006s, episode steps:  22, steps per second: 3458, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 18678/100000: episode: 568, duration: 0.006s, episode steps:  20, steps per second: 3248, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 18722/100000: episode: 569, duration: 0.012s, episode steps:  44, steps per second: 3687, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  mean_best_reward: --\n",
      " 18766/100000: episode: 570, duration: 0.012s, episode steps:  44, steps per second: 3703, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18817/100000: episode: 571, duration: 0.014s, episode steps:  51, steps per second: 3727, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 18904/100000: episode: 572, duration: 0.024s, episode steps:  87, steps per second: 3651, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 18941/100000: episode: 573, duration: 0.011s, episode steps:  37, steps per second: 3508, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 18997/100000: episode: 574, duration: 0.015s, episode steps:  56, steps per second: 3682, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19009/100000: episode: 575, duration: 0.004s, episode steps:  12, steps per second: 2988, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 19048/100000: episode: 576, duration: 0.012s, episode steps:  39, steps per second: 3281, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 19118/100000: episode: 577, duration: 0.019s, episode steps:  70, steps per second: 3605, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 19196/100000: episode: 578, duration: 0.022s, episode steps:  78, steps per second: 3479, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19241/100000: episode: 579, duration: 0.012s, episode steps:  45, steps per second: 3600, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 19286/100000: episode: 580, duration: 0.012s, episode steps:  45, steps per second: 3648, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 19317/100000: episode: 581, duration: 0.008s, episode steps:  31, steps per second: 3680, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 19352/100000: episode: 582, duration: 0.010s, episode steps:  35, steps per second: 3590, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 19373/100000: episode: 583, duration: 0.006s, episode steps:  21, steps per second: 3257, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 19399/100000: episode: 584, duration: 0.007s, episode steps:  26, steps per second: 3521, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.346 [0.000, 1.000],  mean_best_reward: --\n",
      " 19422/100000: episode: 585, duration: 0.006s, episode steps:  23, steps per second: 3592, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 19481/100000: episode: 586, duration: 0.016s, episode steps:  59, steps per second: 3679, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 19575/100000: episode: 587, duration: 0.028s, episode steps:  94, steps per second: 3328, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 19625/100000: episode: 588, duration: 0.015s, episode steps:  50, steps per second: 3363, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 19658/100000: episode: 589, duration: 0.010s, episode steps:  33, steps per second: 3401, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 19718/100000: episode: 590, duration: 0.018s, episode steps:  60, steps per second: 3400, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 19795/100000: episode: 591, duration: 0.023s, episode steps:  77, steps per second: 3308, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 19852/100000: episode: 592, duration: 0.015s, episode steps:  57, steps per second: 3695, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 19938/100000: episode: 593, duration: 0.024s, episode steps:  86, steps per second: 3624, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 20016/100000: episode: 594, duration: 0.021s, episode steps:  78, steps per second: 3707, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 20031/100000: episode: 595, duration: 0.004s, episode steps:  15, steps per second: 3394, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 20082/100000: episode: 596, duration: 0.014s, episode steps:  51, steps per second: 3683, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 20136/100000: episode: 597, duration: 0.016s, episode steps:  54, steps per second: 3286, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 20255/100000: episode: 598, duration: 0.032s, episode steps: 119, steps per second: 3724, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 20277/100000: episode: 599, duration: 0.008s, episode steps:  22, steps per second: 2887, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 20315/100000: episode: 600, duration: 0.011s, episode steps:  38, steps per second: 3364, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 20362/100000: episode: 601, duration: 0.013s, episode steps:  47, steps per second: 3506, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: 122.500000\n",
      " 20400/100000: episode: 602, duration: 0.011s, episode steps:  38, steps per second: 3318, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 20423/100000: episode: 603, duration: 0.008s, episode steps:  23, steps per second: 3011, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 20454/100000: episode: 604, duration: 0.009s, episode steps:  31, steps per second: 3423, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 20494/100000: episode: 605, duration: 0.012s, episode steps:  40, steps per second: 3362, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 20556/100000: episode: 606, duration: 0.017s, episode steps:  62, steps per second: 3631, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 20615/100000: episode: 607, duration: 0.016s, episode steps:  59, steps per second: 3606, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 20635/100000: episode: 608, duration: 0.006s, episode steps:  20, steps per second: 3227, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 20696/100000: episode: 609, duration: 0.016s, episode steps:  61, steps per second: 3787, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 20750/100000: episode: 610, duration: 0.015s, episode steps:  54, steps per second: 3643, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 20764/100000: episode: 611, duration: 0.004s, episode steps:  14, steps per second: 3341, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 20864/100000: episode: 612, duration: 0.027s, episode steps: 100, steps per second: 3710, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  mean_best_reward: --\n",
      " 20902/100000: episode: 613, duration: 0.011s, episode steps:  38, steps per second: 3605, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 20955/100000: episode: 614, duration: 0.016s, episode steps:  53, steps per second: 3400, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  mean_best_reward: --\n",
      " 20989/100000: episode: 615, duration: 0.010s, episode steps:  34, steps per second: 3528, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21023/100000: episode: 616, duration: 0.009s, episode steps:  34, steps per second: 3627, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21047/100000: episode: 617, duration: 0.007s, episode steps:  24, steps per second: 3334, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 21084/100000: episode: 618, duration: 0.012s, episode steps:  37, steps per second: 2977, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 21163/100000: episode: 619, duration: 0.023s, episode steps:  79, steps per second: 3375, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 21189/100000: episode: 620, duration: 0.008s, episode steps:  26, steps per second: 3090, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21219/100000: episode: 621, duration: 0.014s, episode steps:  30, steps per second: 2220, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21284/100000: episode: 622, duration: 0.025s, episode steps:  65, steps per second: 2640, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 21326/100000: episode: 623, duration: 0.015s, episode steps:  42, steps per second: 2715, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 21367/100000: episode: 624, duration: 0.015s, episode steps:  41, steps per second: 2670, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 21381/100000: episode: 625, duration: 0.006s, episode steps:  14, steps per second: 2522, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 21426/100000: episode: 626, duration: 0.016s, episode steps:  45, steps per second: 2746, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 21470/100000: episode: 627, duration: 0.015s, episode steps:  44, steps per second: 2845, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 21528/100000: episode: 628, duration: 0.017s, episode steps:  58, steps per second: 3495, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 21577/100000: episode: 629, duration: 0.014s, episode steps:  49, steps per second: 3420, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 21605/100000: episode: 630, duration: 0.009s, episode steps:  28, steps per second: 2978, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 21695/100000: episode: 631, duration: 0.029s, episode steps:  90, steps per second: 3073, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 21740/100000: episode: 632, duration: 0.013s, episode steps:  45, steps per second: 3538, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 21785/100000: episode: 633, duration: 0.013s, episode steps:  45, steps per second: 3338, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 21816/100000: episode: 634, duration: 0.009s, episode steps:  31, steps per second: 3497, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 21834/100000: episode: 635, duration: 0.005s, episode steps:  18, steps per second: 3431, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21881/100000: episode: 636, duration: 0.014s, episode steps:  47, steps per second: 3407, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 21951/100000: episode: 637, duration: 0.019s, episode steps:  70, steps per second: 3629, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 21989/100000: episode: 638, duration: 0.010s, episode steps:  38, steps per second: 3626, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 22016/100000: episode: 639, duration: 0.008s, episode steps:  27, steps per second: 3436, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 22074/100000: episode: 640, duration: 0.017s, episode steps:  58, steps per second: 3490, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 22101/100000: episode: 641, duration: 0.009s, episode steps:  27, steps per second: 3166, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  mean_best_reward: --\n",
      " 22119/100000: episode: 642, duration: 0.005s, episode steps:  18, steps per second: 3378, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22149/100000: episode: 643, duration: 0.009s, episode steps:  30, steps per second: 3193, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 22251/100000: episode: 644, duration: 0.028s, episode steps: 102, steps per second: 3690, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 22303/100000: episode: 645, duration: 0.015s, episode steps:  52, steps per second: 3576, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 22334/100000: episode: 646, duration: 0.009s, episode steps:  31, steps per second: 3564, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 22371/100000: episode: 647, duration: 0.010s, episode steps:  37, steps per second: 3543, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 22394/100000: episode: 648, duration: 0.007s, episode steps:  23, steps per second: 3263, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 22411/100000: episode: 649, duration: 0.005s, episode steps:  17, steps per second: 3271, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 22512/100000: episode: 650, duration: 0.027s, episode steps: 101, steps per second: 3686, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 22529/100000: episode: 651, duration: 0.005s, episode steps:  17, steps per second: 3260, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: 100.000000\n",
      " 22585/100000: episode: 652, duration: 0.016s, episode steps:  56, steps per second: 3446, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 22623/100000: episode: 653, duration: 0.012s, episode steps:  38, steps per second: 3267, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22681/100000: episode: 654, duration: 0.020s, episode steps:  58, steps per second: 2958, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  mean_best_reward: --\n",
      " 22744/100000: episode: 655, duration: 0.020s, episode steps:  63, steps per second: 3194, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 22795/100000: episode: 656, duration: 0.015s, episode steps:  51, steps per second: 3388, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 22872/100000: episode: 657, duration: 0.021s, episode steps:  77, steps per second: 3692, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 22888/100000: episode: 658, duration: 0.005s, episode steps:  16, steps per second: 3366, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22936/100000: episode: 659, duration: 0.015s, episode steps:  48, steps per second: 3155, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22965/100000: episode: 660, duration: 0.009s, episode steps:  29, steps per second: 3321, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 23006/100000: episode: 661, duration: 0.012s, episode steps:  41, steps per second: 3453, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 23030/100000: episode: 662, duration: 0.007s, episode steps:  24, steps per second: 3387, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 23043/100000: episode: 663, duration: 0.004s, episode steps:  13, steps per second: 3324, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 23078/100000: episode: 664, duration: 0.011s, episode steps:  35, steps per second: 3303, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 23131/100000: episode: 665, duration: 0.015s, episode steps:  53, steps per second: 3509, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 23245/100000: episode: 666, duration: 0.030s, episode steps: 114, steps per second: 3791, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 23315/100000: episode: 667, duration: 0.020s, episode steps:  70, steps per second: 3423, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 23342/100000: episode: 668, duration: 0.009s, episode steps:  27, steps per second: 3173, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 23468/100000: episode: 669, duration: 0.036s, episode steps: 126, steps per second: 3471, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 23481/100000: episode: 670, duration: 0.004s, episode steps:  13, steps per second: 3220, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      " 23531/100000: episode: 671, duration: 0.014s, episode steps:  50, steps per second: 3594, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 23581/100000: episode: 672, duration: 0.014s, episode steps:  50, steps per second: 3506, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 23688/100000: episode: 673, duration: 0.033s, episode steps: 107, steps per second: 3246, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 23701/100000: episode: 674, duration: 0.004s, episode steps:  13, steps per second: 3055, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 23808/100000: episode: 675, duration: 0.031s, episode steps: 107, steps per second: 3401, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 23838/100000: episode: 676, duration: 0.008s, episode steps:  30, steps per second: 3560, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 23886/100000: episode: 677, duration: 0.015s, episode steps:  48, steps per second: 3131, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 23921/100000: episode: 678, duration: 0.011s, episode steps:  35, steps per second: 3120, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 23978/100000: episode: 679, duration: 0.018s, episode steps:  57, steps per second: 3162, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 24053/100000: episode: 680, duration: 0.022s, episode steps:  75, steps per second: 3407, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 24065/100000: episode: 681, duration: 0.004s, episode steps:  12, steps per second: 3046, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      " 24098/100000: episode: 682, duration: 0.009s, episode steps:  33, steps per second: 3490, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 24147/100000: episode: 683, duration: 0.014s, episode steps:  49, steps per second: 3393, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  mean_best_reward: --\n",
      " 24212/100000: episode: 684, duration: 0.019s, episode steps:  65, steps per second: 3425, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 24264/100000: episode: 685, duration: 0.014s, episode steps:  52, steps per second: 3629, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 24286/100000: episode: 686, duration: 0.007s, episode steps:  22, steps per second: 3380, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 24311/100000: episode: 687, duration: 0.007s, episode steps:  25, steps per second: 3546, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 24344/100000: episode: 688, duration: 0.010s, episode steps:  33, steps per second: 3424, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 24355/100000: episode: 689, duration: 0.004s, episode steps:  11, steps per second: 2938, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 24398/100000: episode: 690, duration: 0.012s, episode steps:  43, steps per second: 3540, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 24416/100000: episode: 691, duration: 0.005s, episode steps:  18, steps per second: 3359, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 24474/100000: episode: 692, duration: 0.017s, episode steps:  58, steps per second: 3424, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 24509/100000: episode: 693, duration: 0.011s, episode steps:  35, steps per second: 3187, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 24531/100000: episode: 694, duration: 0.007s, episode steps:  22, steps per second: 3276, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 24580/100000: episode: 695, duration: 0.015s, episode steps:  49, steps per second: 3341, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 24615/100000: episode: 696, duration: 0.011s, episode steps:  35, steps per second: 3278, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 24629/100000: episode: 697, duration: 0.006s, episode steps:  14, steps per second: 2430, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 24664/100000: episode: 698, duration: 0.010s, episode steps:  35, steps per second: 3423, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 24681/100000: episode: 699, duration: 0.005s, episode steps:  17, steps per second: 3181, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 24753/100000: episode: 700, duration: 0.024s, episode steps:  72, steps per second: 3001, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 24803/100000: episode: 701, duration: 0.017s, episode steps:  50, steps per second: 2891, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: 87.500000\n",
      " 24831/100000: episode: 702, duration: 0.011s, episode steps:  28, steps per second: 2488, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 24907/100000: episode: 703, duration: 0.027s, episode steps:  76, steps per second: 2786, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 24948/100000: episode: 704, duration: 0.013s, episode steps:  41, steps per second: 3075, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 24966/100000: episode: 705, duration: 0.006s, episode steps:  18, steps per second: 2973, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 24997/100000: episode: 706, duration: 0.010s, episode steps:  31, steps per second: 3149, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 25020/100000: episode: 707, duration: 0.007s, episode steps:  23, steps per second: 3246, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 25057/100000: episode: 708, duration: 0.011s, episode steps:  37, steps per second: 3462, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 25107/100000: episode: 709, duration: 0.016s, episode steps:  50, steps per second: 3194, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 25184/100000: episode: 710, duration: 0.021s, episode steps:  77, steps per second: 3638, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 25230/100000: episode: 711, duration: 0.013s, episode steps:  46, steps per second: 3551, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 25312/100000: episode: 712, duration: 0.024s, episode steps:  82, steps per second: 3354, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 25344/100000: episode: 713, duration: 0.009s, episode steps:  32, steps per second: 3504, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 25401/100000: episode: 714, duration: 0.016s, episode steps:  57, steps per second: 3557, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 25441/100000: episode: 715, duration: 0.012s, episode steps:  40, steps per second: 3364, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 25497/100000: episode: 716, duration: 0.015s, episode steps:  56, steps per second: 3700, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  mean_best_reward: --\n",
      " 25539/100000: episode: 717, duration: 0.012s, episode steps:  42, steps per second: 3496, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 25556/100000: episode: 718, duration: 0.005s, episode steps:  17, steps per second: 3377, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 25584/100000: episode: 719, duration: 0.008s, episode steps:  28, steps per second: 3467, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 25616/100000: episode: 720, duration: 0.009s, episode steps:  32, steps per second: 3391, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 25677/100000: episode: 721, duration: 0.016s, episode steps:  61, steps per second: 3728, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 25698/100000: episode: 722, duration: 0.007s, episode steps:  21, steps per second: 3068, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 25728/100000: episode: 723, duration: 0.009s, episode steps:  30, steps per second: 3404, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 25759/100000: episode: 724, duration: 0.009s, episode steps:  31, steps per second: 3581, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 25770/100000: episode: 725, duration: 0.003s, episode steps:  11, steps per second: 3170, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 25819/100000: episode: 726, duration: 0.015s, episode steps:  49, steps per second: 3325, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 25857/100000: episode: 727, duration: 0.011s, episode steps:  38, steps per second: 3401, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 25882/100000: episode: 728, duration: 0.007s, episode steps:  25, steps per second: 3499, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 25916/100000: episode: 729, duration: 0.011s, episode steps:  34, steps per second: 3159, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 25965/100000: episode: 730, duration: 0.014s, episode steps:  49, steps per second: 3629, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 26025/100000: episode: 731, duration: 0.017s, episode steps:  60, steps per second: 3462, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 26102/100000: episode: 732, duration: 0.023s, episode steps:  77, steps per second: 3289, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 26166/100000: episode: 733, duration: 0.019s, episode steps:  64, steps per second: 3419, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 26220/100000: episode: 734, duration: 0.016s, episode steps:  54, steps per second: 3313, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 26265/100000: episode: 735, duration: 0.014s, episode steps:  45, steps per second: 3132, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 26335/100000: episode: 736, duration: 0.021s, episode steps:  70, steps per second: 3361, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 26349/100000: episode: 737, duration: 0.006s, episode steps:  14, steps per second: 2180, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 26417/100000: episode: 738, duration: 0.020s, episode steps:  68, steps per second: 3371, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 26451/100000: episode: 739, duration: 0.010s, episode steps:  34, steps per second: 3436, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 26464/100000: episode: 740, duration: 0.004s, episode steps:  13, steps per second: 3121, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 26487/100000: episode: 741, duration: 0.007s, episode steps:  23, steps per second: 3495, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 26526/100000: episode: 742, duration: 0.011s, episode steps:  39, steps per second: 3497, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 26609/100000: episode: 743, duration: 0.024s, episode steps:  83, steps per second: 3496, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  mean_best_reward: --\n",
      " 26643/100000: episode: 744, duration: 0.011s, episode steps:  34, steps per second: 3135, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 26681/100000: episode: 745, duration: 0.011s, episode steps:  38, steps per second: 3377, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 26716/100000: episode: 746, duration: 0.010s, episode steps:  35, steps per second: 3541, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 26741/100000: episode: 747, duration: 0.007s, episode steps:  25, steps per second: 3585, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 26802/100000: episode: 748, duration: 0.016s, episode steps:  61, steps per second: 3739, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 26817/100000: episode: 749, duration: 0.004s, episode steps:  15, steps per second: 3334, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 26852/100000: episode: 750, duration: 0.011s, episode steps:  35, steps per second: 3265, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 26933/100000: episode: 751, duration: 0.023s, episode steps:  81, steps per second: 3527, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: 72.500000\n",
      " 26975/100000: episode: 752, duration: 0.012s, episode steps:  42, steps per second: 3385, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 26990/100000: episode: 753, duration: 0.005s, episode steps:  15, steps per second: 2853, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 27041/100000: episode: 754, duration: 0.016s, episode steps:  51, steps per second: 3221, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  mean_best_reward: --\n",
      " 27064/100000: episode: 755, duration: 0.007s, episode steps:  23, steps per second: 3467, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 27169/100000: episode: 756, duration: 0.032s, episode steps: 105, steps per second: 3317, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 27235/100000: episode: 757, duration: 0.019s, episode steps:  66, steps per second: 3488, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 27273/100000: episode: 758, duration: 0.011s, episode steps:  38, steps per second: 3501, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 27337/100000: episode: 759, duration: 0.020s, episode steps:  64, steps per second: 3209, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 27384/100000: episode: 760, duration: 0.015s, episode steps:  47, steps per second: 3155, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 27413/100000: episode: 761, duration: 0.009s, episode steps:  29, steps per second: 3347, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 27459/100000: episode: 762, duration: 0.013s, episode steps:  46, steps per second: 3524, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 27537/100000: episode: 763, duration: 0.023s, episode steps:  78, steps per second: 3369, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 27600/100000: episode: 764, duration: 0.022s, episode steps:  63, steps per second: 2820, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 27641/100000: episode: 765, duration: 0.012s, episode steps:  41, steps per second: 3293, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 27673/100000: episode: 766, duration: 0.010s, episode steps:  32, steps per second: 3326, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 27710/100000: episode: 767, duration: 0.010s, episode steps:  37, steps per second: 3620, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 27770/100000: episode: 768, duration: 0.019s, episode steps:  60, steps per second: 3179, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 27822/100000: episode: 769, duration: 0.014s, episode steps:  52, steps per second: 3605, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 27851/100000: episode: 770, duration: 0.008s, episode steps:  29, steps per second: 3637, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 27899/100000: episode: 771, duration: 0.013s, episode steps:  48, steps per second: 3668, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 27948/100000: episode: 772, duration: 0.015s, episode steps:  49, steps per second: 3236, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 27974/100000: episode: 773, duration: 0.007s, episode steps:  26, steps per second: 3582, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 28013/100000: episode: 774, duration: 0.011s, episode steps:  39, steps per second: 3668, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 28111/100000: episode: 775, duration: 0.027s, episode steps:  98, steps per second: 3569, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 28163/100000: episode: 776, duration: 0.014s, episode steps:  52, steps per second: 3720, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 28186/100000: episode: 777, duration: 0.009s, episode steps:  23, steps per second: 2571, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 28227/100000: episode: 778, duration: 0.013s, episode steps:  41, steps per second: 3208, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 28273/100000: episode: 779, duration: 0.014s, episode steps:  46, steps per second: 3199, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 28322/100000: episode: 780, duration: 0.017s, episode steps:  49, steps per second: 2966, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  mean_best_reward: --\n",
      " 28403/100000: episode: 781, duration: 0.028s, episode steps:  81, steps per second: 2876, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 28472/100000: episode: 782, duration: 0.021s, episode steps:  69, steps per second: 3244, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 28507/100000: episode: 783, duration: 0.012s, episode steps:  35, steps per second: 2873, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 28533/100000: episode: 784, duration: 0.008s, episode steps:  26, steps per second: 3106, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 28579/100000: episode: 785, duration: 0.013s, episode steps:  46, steps per second: 3557, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 28611/100000: episode: 786, duration: 0.009s, episode steps:  32, steps per second: 3616, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 28656/100000: episode: 787, duration: 0.012s, episode steps:  45, steps per second: 3680, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 28761/100000: episode: 788, duration: 0.029s, episode steps: 105, steps per second: 3669, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 28818/100000: episode: 789, duration: 0.016s, episode steps:  57, steps per second: 3488, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  mean_best_reward: --\n",
      " 28901/100000: episode: 790, duration: 0.022s, episode steps:  83, steps per second: 3740, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 28952/100000: episode: 791, duration: 0.016s, episode steps:  51, steps per second: 3180, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  mean_best_reward: --\n",
      " 29003/100000: episode: 792, duration: 0.015s, episode steps:  51, steps per second: 3385, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 29046/100000: episode: 793, duration: 0.012s, episode steps:  43, steps per second: 3506, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 29105/100000: episode: 794, duration: 0.017s, episode steps:  59, steps per second: 3442, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 29177/100000: episode: 795, duration: 0.019s, episode steps:  72, steps per second: 3750, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 29247/100000: episode: 796, duration: 0.020s, episode steps:  70, steps per second: 3515, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 29296/100000: episode: 797, duration: 0.013s, episode steps:  49, steps per second: 3693, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 29330/100000: episode: 798, duration: 0.010s, episode steps:  34, steps per second: 3517, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 29401/100000: episode: 799, duration: 0.021s, episode steps:  71, steps per second: 3393, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 29444/100000: episode: 800, duration: 0.012s, episode steps:  43, steps per second: 3565, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 29477/100000: episode: 801, duration: 0.010s, episode steps:  33, steps per second: 3463, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: 95.000000\n",
      " 29514/100000: episode: 802, duration: 0.010s, episode steps:  37, steps per second: 3566, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 29606/100000: episode: 803, duration: 0.027s, episode steps:  92, steps per second: 3386, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 29686/100000: episode: 804, duration: 0.022s, episode steps:  80, steps per second: 3612, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 29736/100000: episode: 805, duration: 0.015s, episode steps:  50, steps per second: 3395, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 29751/100000: episode: 806, duration: 0.004s, episode steps:  15, steps per second: 3350, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 29789/100000: episode: 807, duration: 0.011s, episode steps:  38, steps per second: 3498, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 29823/100000: episode: 808, duration: 0.010s, episode steps:  34, steps per second: 3472, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  mean_best_reward: --\n",
      " 29890/100000: episode: 809, duration: 0.019s, episode steps:  67, steps per second: 3596, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 29976/100000: episode: 810, duration: 0.023s, episode steps:  86, steps per second: 3777, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 30030/100000: episode: 811, duration: 0.017s, episode steps:  54, steps per second: 3200, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 30058/100000: episode: 812, duration: 0.008s, episode steps:  28, steps per second: 3583, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 30085/100000: episode: 813, duration: 0.008s, episode steps:  27, steps per second: 3360, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 30113/100000: episode: 814, duration: 0.008s, episode steps:  28, steps per second: 3353, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 30136/100000: episode: 815, duration: 0.007s, episode steps:  23, steps per second: 3225, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 30274/100000: episode: 816, duration: 0.039s, episode steps: 138, steps per second: 3579, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 30291/100000: episode: 817, duration: 0.006s, episode steps:  17, steps per second: 3083, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 30351/100000: episode: 818, duration: 0.018s, episode steps:  60, steps per second: 3421, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 30389/100000: episode: 819, duration: 0.011s, episode steps:  38, steps per second: 3421, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 30434/100000: episode: 820, duration: 0.013s, episode steps:  45, steps per second: 3571, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 30453/100000: episode: 821, duration: 0.006s, episode steps:  19, steps per second: 3397, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 30546/100000: episode: 822, duration: 0.024s, episode steps:  93, steps per second: 3817, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 30632/100000: episode: 823, duration: 0.024s, episode steps:  86, steps per second: 3632, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 30669/100000: episode: 824, duration: 0.011s, episode steps:  37, steps per second: 3351, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 30710/100000: episode: 825, duration: 0.013s, episode steps:  41, steps per second: 3218, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 30729/100000: episode: 826, duration: 0.006s, episode steps:  19, steps per second: 3213, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 30775/100000: episode: 827, duration: 0.013s, episode steps:  46, steps per second: 3644, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 30878/100000: episode: 828, duration: 0.033s, episode steps: 103, steps per second: 3157, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 30896/100000: episode: 829, duration: 0.006s, episode steps:  18, steps per second: 3248, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 30929/100000: episode: 830, duration: 0.010s, episode steps:  33, steps per second: 3257, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 30943/100000: episode: 831, duration: 0.005s, episode steps:  14, steps per second: 2801, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 30961/100000: episode: 832, duration: 0.005s, episode steps:  18, steps per second: 3344, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 30981/100000: episode: 833, duration: 0.006s, episode steps:  20, steps per second: 3272, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 31022/100000: episode: 834, duration: 0.012s, episode steps:  41, steps per second: 3406, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 31065/100000: episode: 835, duration: 0.014s, episode steps:  43, steps per second: 3026, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 31107/100000: episode: 836, duration: 0.012s, episode steps:  42, steps per second: 3618, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 31153/100000: episode: 837, duration: 0.013s, episode steps:  46, steps per second: 3490, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 31217/100000: episode: 838, duration: 0.019s, episode steps:  64, steps per second: 3382, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 31261/100000: episode: 839, duration: 0.013s, episode steps:  44, steps per second: 3373, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 31298/100000: episode: 840, duration: 0.010s, episode steps:  37, steps per second: 3674, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 31342/100000: episode: 841, duration: 0.013s, episode steps:  44, steps per second: 3503, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 31354/100000: episode: 842, duration: 0.004s, episode steps:  12, steps per second: 2980, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 31414/100000: episode: 843, duration: 0.019s, episode steps:  60, steps per second: 3177, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 31478/100000: episode: 844, duration: 0.018s, episode steps:  64, steps per second: 3577, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  mean_best_reward: --\n",
      " 31508/100000: episode: 845, duration: 0.009s, episode steps:  30, steps per second: 3446, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 31599/100000: episode: 846, duration: 0.028s, episode steps:  91, steps per second: 3230, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 31644/100000: episode: 847, duration: 0.013s, episode steps:  45, steps per second: 3544, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 31740/100000: episode: 848, duration: 0.029s, episode steps:  96, steps per second: 3285, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 31784/100000: episode: 849, duration: 0.014s, episode steps:  44, steps per second: 3153, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 31860/100000: episode: 850, duration: 0.028s, episode steps:  76, steps per second: 2690, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 31927/100000: episode: 851, duration: 0.032s, episode steps:  67, steps per second: 2102, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: 106.000000\n",
      " 31945/100000: episode: 852, duration: 0.007s, episode steps:  18, steps per second: 2432, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 31986/100000: episode: 853, duration: 0.014s, episode steps:  41, steps per second: 2861, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 32032/100000: episode: 854, duration: 0.013s, episode steps:  46, steps per second: 3422, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 32070/100000: episode: 855, duration: 0.012s, episode steps:  38, steps per second: 3049, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 32101/100000: episode: 856, duration: 0.009s, episode steps:  31, steps per second: 3381, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 32122/100000: episode: 857, duration: 0.006s, episode steps:  21, steps per second: 3419, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  mean_best_reward: --\n",
      " 32158/100000: episode: 858, duration: 0.010s, episode steps:  36, steps per second: 3529, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32181/100000: episode: 859, duration: 0.007s, episode steps:  23, steps per second: 3506, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 32225/100000: episode: 860, duration: 0.013s, episode steps:  44, steps per second: 3372, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32285/100000: episode: 861, duration: 0.016s, episode steps:  60, steps per second: 3731, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 32323/100000: episode: 862, duration: 0.010s, episode steps:  38, steps per second: 3684, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 32353/100000: episode: 863, duration: 0.009s, episode steps:  30, steps per second: 3441, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 32383/100000: episode: 864, duration: 0.010s, episode steps:  30, steps per second: 3076, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 32456/100000: episode: 865, duration: 0.019s, episode steps:  73, steps per second: 3758, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  mean_best_reward: --\n",
      " 32517/100000: episode: 866, duration: 0.017s, episode steps:  61, steps per second: 3636, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 32571/100000: episode: 867, duration: 0.015s, episode steps:  54, steps per second: 3541, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 32596/100000: episode: 868, duration: 0.008s, episode steps:  25, steps per second: 3025, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 32621/100000: episode: 869, duration: 0.007s, episode steps:  25, steps per second: 3469, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 32651/100000: episode: 870, duration: 0.009s, episode steps:  30, steps per second: 3504, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32666/100000: episode: 871, duration: 0.004s, episode steps:  15, steps per second: 3366, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 32759/100000: episode: 872, duration: 0.027s, episode steps:  93, steps per second: 3416, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 32844/100000: episode: 873, duration: 0.023s, episode steps:  85, steps per second: 3671, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 32891/100000: episode: 874, duration: 0.013s, episode steps:  47, steps per second: 3537, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 32913/100000: episode: 875, duration: 0.007s, episode steps:  22, steps per second: 3235, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32939/100000: episode: 876, duration: 0.007s, episode steps:  26, steps per second: 3505, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 33005/100000: episode: 877, duration: 0.017s, episode steps:  66, steps per second: 3781, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 33036/100000: episode: 878, duration: 0.009s, episode steps:  31, steps per second: 3588, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 33094/100000: episode: 879, duration: 0.016s, episode steps:  58, steps per second: 3714, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 33112/100000: episode: 880, duration: 0.006s, episode steps:  18, steps per second: 2931, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 33192/100000: episode: 881, duration: 0.023s, episode steps:  80, steps per second: 3495, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 33219/100000: episode: 882, duration: 0.008s, episode steps:  27, steps per second: 3471, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 33243/100000: episode: 883, duration: 0.007s, episode steps:  24, steps per second: 3542, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 33290/100000: episode: 884, duration: 0.013s, episode steps:  47, steps per second: 3617, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 33354/100000: episode: 885, duration: 0.018s, episode steps:  64, steps per second: 3498, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 33367/100000: episode: 886, duration: 0.004s, episode steps:  13, steps per second: 2940, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 33450/100000: episode: 887, duration: 0.023s, episode steps:  83, steps per second: 3673, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 33474/100000: episode: 888, duration: 0.007s, episode steps:  24, steps per second: 3514, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 33520/100000: episode: 889, duration: 0.013s, episode steps:  46, steps per second: 3526, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 33567/100000: episode: 890, duration: 0.013s, episode steps:  47, steps per second: 3684, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 33590/100000: episode: 891, duration: 0.006s, episode steps:  23, steps per second: 3551, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 33604/100000: episode: 892, duration: 0.005s, episode steps:  14, steps per second: 3089, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 33653/100000: episode: 893, duration: 0.013s, episode steps:  49, steps per second: 3678, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 33700/100000: episode: 894, duration: 0.013s, episode steps:  47, steps per second: 3721, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 33726/100000: episode: 895, duration: 0.008s, episode steps:  26, steps per second: 3205, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  mean_best_reward: --\n",
      " 33756/100000: episode: 896, duration: 0.010s, episode steps:  30, steps per second: 3020, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 33781/100000: episode: 897, duration: 0.007s, episode steps:  25, steps per second: 3376, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 33799/100000: episode: 898, duration: 0.005s, episode steps:  18, steps per second: 3466, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  mean_best_reward: --\n",
      " 33827/100000: episode: 899, duration: 0.008s, episode steps:  28, steps per second: 3394, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 33850/100000: episode: 900, duration: 0.007s, episode steps:  23, steps per second: 3318, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 33890/100000: episode: 901, duration: 0.012s, episode steps:  40, steps per second: 3467, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  mean_best_reward: 104.000000\n",
      " 33947/100000: episode: 902, duration: 0.016s, episode steps:  57, steps per second: 3480, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 33992/100000: episode: 903, duration: 0.013s, episode steps:  45, steps per second: 3522, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 34038/100000: episode: 904, duration: 0.013s, episode steps:  46, steps per second: 3515, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 34072/100000: episode: 905, duration: 0.010s, episode steps:  34, steps per second: 3527, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34110/100000: episode: 906, duration: 0.011s, episode steps:  38, steps per second: 3511, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 34143/100000: episode: 907, duration: 0.010s, episode steps:  33, steps per second: 3419, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 34225/100000: episode: 908, duration: 0.022s, episode steps:  82, steps per second: 3694, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 34250/100000: episode: 909, duration: 0.007s, episode steps:  25, steps per second: 3595, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 34315/100000: episode: 910, duration: 0.019s, episode steps:  65, steps per second: 3490, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 34354/100000: episode: 911, duration: 0.012s, episode steps:  39, steps per second: 3256, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 34366/100000: episode: 912, duration: 0.004s, episode steps:  12, steps per second: 3197, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 34433/100000: episode: 913, duration: 0.019s, episode steps:  67, steps per second: 3533, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 34502/100000: episode: 914, duration: 0.019s, episode steps:  69, steps per second: 3628, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 34533/100000: episode: 915, duration: 0.011s, episode steps:  31, steps per second: 2937, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 34543/100000: episode: 916, duration: 0.004s, episode steps:  10, steps per second: 2753, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  mean_best_reward: --\n",
      " 34557/100000: episode: 917, duration: 0.005s, episode steps:  14, steps per second: 2885, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 34606/100000: episode: 918, duration: 0.014s, episode steps:  49, steps per second: 3528, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 34624/100000: episode: 919, duration: 0.006s, episode steps:  18, steps per second: 3041, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  mean_best_reward: --\n",
      " 34665/100000: episode: 920, duration: 0.012s, episode steps:  41, steps per second: 3514, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 34769/100000: episode: 921, duration: 0.028s, episode steps: 104, steps per second: 3745, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 34880/100000: episode: 922, duration: 0.030s, episode steps: 111, steps per second: 3699, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 34930/100000: episode: 923, duration: 0.014s, episode steps:  50, steps per second: 3654, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34981/100000: episode: 924, duration: 0.014s, episode steps:  51, steps per second: 3535, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 35007/100000: episode: 925, duration: 0.009s, episode steps:  26, steps per second: 3002, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 35028/100000: episode: 926, duration: 0.006s, episode steps:  21, steps per second: 3416, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 35070/100000: episode: 927, duration: 0.013s, episode steps:  42, steps per second: 3350, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 35104/100000: episode: 928, duration: 0.010s, episode steps:  34, steps per second: 3439, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 35190/100000: episode: 929, duration: 0.026s, episode steps:  86, steps per second: 3353, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 35212/100000: episode: 930, duration: 0.007s, episode steps:  22, steps per second: 3311, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 35258/100000: episode: 931, duration: 0.013s, episode steps:  46, steps per second: 3528, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 35281/100000: episode: 932, duration: 0.007s, episode steps:  23, steps per second: 3330, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 35353/100000: episode: 933, duration: 0.023s, episode steps:  72, steps per second: 3170, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 35377/100000: episode: 934, duration: 0.007s, episode steps:  24, steps per second: 3273, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 35413/100000: episode: 935, duration: 0.014s, episode steps:  36, steps per second: 2501, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 35484/100000: episode: 936, duration: 0.025s, episode steps:  71, steps per second: 2851, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 35552/100000: episode: 937, duration: 0.027s, episode steps:  68, steps per second: 2487, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 35609/100000: episode: 938, duration: 0.019s, episode steps:  57, steps per second: 2946, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 35638/100000: episode: 939, duration: 0.009s, episode steps:  29, steps per second: 3372, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 35706/100000: episode: 940, duration: 0.020s, episode steps:  68, steps per second: 3445, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 35737/100000: episode: 941, duration: 0.010s, episode steps:  31, steps per second: 3188, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 35798/100000: episode: 942, duration: 0.018s, episode steps:  61, steps per second: 3404, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 35831/100000: episode: 943, duration: 0.009s, episode steps:  33, steps per second: 3506, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 35844/100000: episode: 944, duration: 0.004s, episode steps:  13, steps per second: 3234, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 35903/100000: episode: 945, duration: 0.016s, episode steps:  59, steps per second: 3626, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 35953/100000: episode: 946, duration: 0.014s, episode steps:  50, steps per second: 3501, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 35989/100000: episode: 947, duration: 0.010s, episode steps:  36, steps per second: 3513, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 36003/100000: episode: 948, duration: 0.004s, episode steps:  14, steps per second: 3322, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 36087/100000: episode: 949, duration: 0.023s, episode steps:  84, steps per second: 3645, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 36108/100000: episode: 950, duration: 0.006s, episode steps:  21, steps per second: 3513, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 36124/100000: episode: 951, duration: 0.005s, episode steps:  16, steps per second: 3009, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 112.500000\n",
      " 36167/100000: episode: 952, duration: 0.013s, episode steps:  43, steps per second: 3339, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 36205/100000: episode: 953, duration: 0.012s, episode steps:  38, steps per second: 3182, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 36219/100000: episode: 954, duration: 0.004s, episode steps:  14, steps per second: 3162, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 36256/100000: episode: 955, duration: 0.012s, episode steps:  37, steps per second: 3109, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 36286/100000: episode: 956, duration: 0.011s, episode steps:  30, steps per second: 2792, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 36328/100000: episode: 957, duration: 0.016s, episode steps:  42, steps per second: 2694, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 36423/100000: episode: 958, duration: 0.028s, episode steps:  95, steps per second: 3357, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 36466/100000: episode: 959, duration: 0.013s, episode steps:  43, steps per second: 3187, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  mean_best_reward: --\n",
      " 36540/100000: episode: 960, duration: 0.023s, episode steps:  74, steps per second: 3258, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 36561/100000: episode: 961, duration: 0.007s, episode steps:  21, steps per second: 2979, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 36628/100000: episode: 962, duration: 0.020s, episode steps:  67, steps per second: 3290, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 36648/100000: episode: 963, duration: 0.006s, episode steps:  20, steps per second: 3300, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 36682/100000: episode: 964, duration: 0.010s, episode steps:  34, steps per second: 3332, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 36714/100000: episode: 965, duration: 0.010s, episode steps:  32, steps per second: 3344, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 36750/100000: episode: 966, duration: 0.011s, episode steps:  36, steps per second: 3414, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 36838/100000: episode: 967, duration: 0.026s, episode steps:  88, steps per second: 3330, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 36849/100000: episode: 968, duration: 0.004s, episode steps:  11, steps per second: 3134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 36864/100000: episode: 969, duration: 0.005s, episode steps:  15, steps per second: 3043, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 36959/100000: episode: 970, duration: 0.026s, episode steps:  95, steps per second: 3643, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 37009/100000: episode: 971, duration: 0.014s, episode steps:  50, steps per second: 3645, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 37067/100000: episode: 972, duration: 0.018s, episode steps:  58, steps per second: 3252, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 37099/100000: episode: 973, duration: 0.009s, episode steps:  32, steps per second: 3493, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  mean_best_reward: --\n",
      " 37121/100000: episode: 974, duration: 0.006s, episode steps:  22, steps per second: 3430, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      " 37150/100000: episode: 975, duration: 0.008s, episode steps:  29, steps per second: 3486, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 37220/100000: episode: 976, duration: 0.019s, episode steps:  70, steps per second: 3749, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 37246/100000: episode: 977, duration: 0.008s, episode steps:  26, steps per second: 3282, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 37271/100000: episode: 978, duration: 0.007s, episode steps:  25, steps per second: 3344, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 37308/100000: episode: 979, duration: 0.010s, episode steps:  37, steps per second: 3551, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  mean_best_reward: --\n",
      " 37397/100000: episode: 980, duration: 0.027s, episode steps:  89, steps per second: 3313, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 37423/100000: episode: 981, duration: 0.010s, episode steps:  26, steps per second: 2710, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 37442/100000: episode: 982, duration: 0.006s, episode steps:  19, steps per second: 3371, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 37480/100000: episode: 983, duration: 0.012s, episode steps:  38, steps per second: 3155, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 37524/100000: episode: 984, duration: 0.012s, episode steps:  44, steps per second: 3636, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 37541/100000: episode: 985, duration: 0.005s, episode steps:  17, steps per second: 3258, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 37565/100000: episode: 986, duration: 0.007s, episode steps:  24, steps per second: 3391, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 37604/100000: episode: 987, duration: 0.011s, episode steps:  39, steps per second: 3554, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  mean_best_reward: --\n",
      " 37645/100000: episode: 988, duration: 0.012s, episode steps:  41, steps per second: 3411, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 37704/100000: episode: 989, duration: 0.019s, episode steps:  59, steps per second: 3145, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 37741/100000: episode: 990, duration: 0.011s, episode steps:  37, steps per second: 3454, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 37835/100000: episode: 991, duration: 0.026s, episode steps:  94, steps per second: 3557, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 37875/100000: episode: 992, duration: 0.012s, episode steps:  40, steps per second: 3366, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 37892/100000: episode: 993, duration: 0.006s, episode steps:  17, steps per second: 2966, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 38000/100000: episode: 994, duration: 0.031s, episode steps: 108, steps per second: 3501, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 38017/100000: episode: 995, duration: 0.006s, episode steps:  17, steps per second: 2668, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 38036/100000: episode: 996, duration: 0.008s, episode steps:  19, steps per second: 2427, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 38086/100000: episode: 997, duration: 0.016s, episode steps:  50, steps per second: 3107, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 38124/100000: episode: 998, duration: 0.013s, episode steps:  38, steps per second: 2855, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      " 38144/100000: episode: 999, duration: 0.006s, episode steps:  20, steps per second: 3373, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 38201/100000: episode: 1000, duration: 0.017s, episode steps:  57, steps per second: 3349, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 38264/100000: episode: 1001, duration: 0.019s, episode steps:  63, steps per second: 3331, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: 95.500000\n",
      " 38333/100000: episode: 1002, duration: 0.021s, episode steps:  69, steps per second: 3300, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 38360/100000: episode: 1003, duration: 0.008s, episode steps:  27, steps per second: 3369, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 38384/100000: episode: 1004, duration: 0.007s, episode steps:  24, steps per second: 3342, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 38409/100000: episode: 1005, duration: 0.007s, episode steps:  25, steps per second: 3422, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 38593/100000: episode: 1006, duration: 0.051s, episode steps: 184, steps per second: 3602, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 38671/100000: episode: 1007, duration: 0.022s, episode steps:  78, steps per second: 3511, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 38690/100000: episode: 1008, duration: 0.006s, episode steps:  19, steps per second: 3161, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 38749/100000: episode: 1009, duration: 0.016s, episode steps:  59, steps per second: 3599, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 38797/100000: episode: 1010, duration: 0.015s, episode steps:  48, steps per second: 3216, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 38824/100000: episode: 1011, duration: 0.008s, episode steps:  27, steps per second: 3327, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 38859/100000: episode: 1012, duration: 0.011s, episode steps:  35, steps per second: 3299, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 38945/100000: episode: 1013, duration: 0.029s, episode steps:  86, steps per second: 2988, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 38981/100000: episode: 1014, duration: 0.014s, episode steps:  36, steps per second: 2655, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 38997/100000: episode: 1015, duration: 0.006s, episode steps:  16, steps per second: 2511, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 39021/100000: episode: 1016, duration: 0.009s, episode steps:  24, steps per second: 2800, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 39053/100000: episode: 1017, duration: 0.011s, episode steps:  32, steps per second: 2846, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39231/100000: episode: 1018, duration: 0.053s, episode steps: 178, steps per second: 3341, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 39321/100000: episode: 1019, duration: 0.026s, episode steps:  90, steps per second: 3418, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 39352/100000: episode: 1020, duration: 0.009s, episode steps:  31, steps per second: 3402, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 39372/100000: episode: 1021, duration: 0.006s, episode steps:  20, steps per second: 3163, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39416/100000: episode: 1022, duration: 0.013s, episode steps:  44, steps per second: 3469, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 39477/100000: episode: 1023, duration: 0.017s, episode steps:  61, steps per second: 3653, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 39493/100000: episode: 1024, duration: 0.006s, episode steps:  16, steps per second: 2641, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      " 39567/100000: episode: 1025, duration: 0.022s, episode steps:  74, steps per second: 3394, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39596/100000: episode: 1026, duration: 0.009s, episode steps:  29, steps per second: 3333, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 39609/100000: episode: 1027, duration: 0.004s, episode steps:  13, steps per second: 3203, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 39667/100000: episode: 1028, duration: 0.016s, episode steps:  58, steps per second: 3735, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 39741/100000: episode: 1029, duration: 0.021s, episode steps:  74, steps per second: 3567, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 39804/100000: episode: 1030, duration: 0.017s, episode steps:  63, steps per second: 3704, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 39829/100000: episode: 1031, duration: 0.008s, episode steps:  25, steps per second: 3312, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 39924/100000: episode: 1032, duration: 0.026s, episode steps:  95, steps per second: 3615, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 39966/100000: episode: 1033, duration: 0.012s, episode steps:  42, steps per second: 3472, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 40060/100000: episode: 1034, duration: 0.027s, episode steps:  94, steps per second: 3435, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 40146/100000: episode: 1035, duration: 0.024s, episode steps:  86, steps per second: 3605, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 40191/100000: episode: 1036, duration: 0.013s, episode steps:  45, steps per second: 3553, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 40329/100000: episode: 1037, duration: 0.039s, episode steps: 138, steps per second: 3534, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 40377/100000: episode: 1038, duration: 0.014s, episode steps:  48, steps per second: 3502, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 40422/100000: episode: 1039, duration: 0.015s, episode steps:  45, steps per second: 3095, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 40451/100000: episode: 1040, duration: 0.009s, episode steps:  29, steps per second: 3300, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 40514/100000: episode: 1041, duration: 0.019s, episode steps:  63, steps per second: 3248, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 40622/100000: episode: 1042, duration: 0.031s, episode steps: 108, steps per second: 3505, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 40690/100000: episode: 1043, duration: 0.022s, episode steps:  68, steps per second: 3072, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 40717/100000: episode: 1044, duration: 0.008s, episode steps:  27, steps per second: 3483, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 40738/100000: episode: 1045, duration: 0.006s, episode steps:  21, steps per second: 3437, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 40775/100000: episode: 1046, duration: 0.011s, episode steps:  37, steps per second: 3422, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 40798/100000: episode: 1047, duration: 0.007s, episode steps:  23, steps per second: 3470, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 40878/100000: episode: 1048, duration: 0.023s, episode steps:  80, steps per second: 3481, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 40936/100000: episode: 1049, duration: 0.018s, episode steps:  58, steps per second: 3261, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 40964/100000: episode: 1050, duration: 0.008s, episode steps:  28, steps per second: 3500, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 41034/100000: episode: 1051, duration: 0.020s, episode steps:  70, steps per second: 3470, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: 139.000000\n",
      " 41068/100000: episode: 1052, duration: 0.009s, episode steps:  34, steps per second: 3597, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 41105/100000: episode: 1053, duration: 0.010s, episode steps:  37, steps per second: 3632, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 41144/100000: episode: 1054, duration: 0.011s, episode steps:  39, steps per second: 3652, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 41200/100000: episode: 1055, duration: 0.016s, episode steps:  56, steps per second: 3474, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 41229/100000: episode: 1056, duration: 0.008s, episode steps:  29, steps per second: 3414, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 41259/100000: episode: 1057, duration: 0.009s, episode steps:  30, steps per second: 3504, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 41275/100000: episode: 1058, duration: 0.005s, episode steps:  16, steps per second: 3370, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 41475/100000: episode: 1059, duration: 0.056s, episode steps: 200, steps per second: 3593, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 41489/100000: episode: 1060, duration: 0.005s, episode steps:  14, steps per second: 2912, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 41536/100000: episode: 1061, duration: 0.014s, episode steps:  47, steps per second: 3259, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 41556/100000: episode: 1062, duration: 0.006s, episode steps:  20, steps per second: 3275, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 41591/100000: episode: 1063, duration: 0.010s, episode steps:  35, steps per second: 3608, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 41657/100000: episode: 1064, duration: 0.018s, episode steps:  66, steps per second: 3741, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 41692/100000: episode: 1065, duration: 0.011s, episode steps:  35, steps per second: 3291, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 41752/100000: episode: 1066, duration: 0.016s, episode steps:  60, steps per second: 3701, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 41813/100000: episode: 1067, duration: 0.016s, episode steps:  61, steps per second: 3751, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 41876/100000: episode: 1068, duration: 0.018s, episode steps:  63, steps per second: 3570, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 41957/100000: episode: 1069, duration: 0.022s, episode steps:  81, steps per second: 3670, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 42011/100000: episode: 1070, duration: 0.016s, episode steps:  54, steps per second: 3422, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 42055/100000: episode: 1071, duration: 0.012s, episode steps:  44, steps per second: 3609, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 42100/100000: episode: 1072, duration: 0.014s, episode steps:  45, steps per second: 3326, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 42191/100000: episode: 1073, duration: 0.026s, episode steps:  91, steps per second: 3563, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 42202/100000: episode: 1074, duration: 0.003s, episode steps:  11, steps per second: 3199, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 42280/100000: episode: 1075, duration: 0.021s, episode steps:  78, steps per second: 3726, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 42313/100000: episode: 1076, duration: 0.011s, episode steps:  33, steps per second: 2873, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 42364/100000: episode: 1077, duration: 0.014s, episode steps:  51, steps per second: 3715, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 42407/100000: episode: 1078, duration: 0.012s, episode steps:  43, steps per second: 3533, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 42445/100000: episode: 1079, duration: 0.011s, episode steps:  38, steps per second: 3491, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 42523/100000: episode: 1080, duration: 0.024s, episode steps:  78, steps per second: 3290, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 42602/100000: episode: 1081, duration: 0.023s, episode steps:  79, steps per second: 3374, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 42637/100000: episode: 1082, duration: 0.012s, episode steps:  35, steps per second: 2917, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 42667/100000: episode: 1083, duration: 0.014s, episode steps:  30, steps per second: 2180, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 42720/100000: episode: 1084, duration: 0.016s, episode steps:  53, steps per second: 3331, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  mean_best_reward: --\n",
      " 42757/100000: episode: 1085, duration: 0.011s, episode steps:  37, steps per second: 3391, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 42794/100000: episode: 1086, duration: 0.011s, episode steps:  37, steps per second: 3457, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 42905/100000: episode: 1087, duration: 0.031s, episode steps: 111, steps per second: 3607, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 42947/100000: episode: 1088, duration: 0.012s, episode steps:  42, steps per second: 3509, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 42974/100000: episode: 1089, duration: 0.008s, episode steps:  27, steps per second: 3468, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 43016/100000: episode: 1090, duration: 0.011s, episode steps:  42, steps per second: 3656, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 43041/100000: episode: 1091, duration: 0.007s, episode steps:  25, steps per second: 3481, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 43053/100000: episode: 1092, duration: 0.004s, episode steps:  12, steps per second: 3204, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      " 43105/100000: episode: 1093, duration: 0.015s, episode steps:  52, steps per second: 3558, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  mean_best_reward: --\n",
      " 43187/100000: episode: 1094, duration: 0.023s, episode steps:  82, steps per second: 3576, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 43235/100000: episode: 1095, duration: 0.013s, episode steps:  48, steps per second: 3574, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 43256/100000: episode: 1096, duration: 0.006s, episode steps:  21, steps per second: 3490, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 43280/100000: episode: 1097, duration: 0.011s, episode steps:  24, steps per second: 2200, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 43339/100000: episode: 1098, duration: 0.016s, episode steps:  59, steps per second: 3660, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 43384/100000: episode: 1099, duration: 0.013s, episode steps:  45, steps per second: 3471, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 43405/100000: episode: 1100, duration: 0.007s, episode steps:  21, steps per second: 3227, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 43434/100000: episode: 1101, duration: 0.009s, episode steps:  29, steps per second: 3332, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: 82.000000\n",
      " 43469/100000: episode: 1102, duration: 0.010s, episode steps:  35, steps per second: 3509, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 43510/100000: episode: 1103, duration: 0.013s, episode steps:  41, steps per second: 3265, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 43571/100000: episode: 1104, duration: 0.017s, episode steps:  61, steps per second: 3684, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 43607/100000: episode: 1105, duration: 0.012s, episode steps:  36, steps per second: 2949, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 43619/100000: episode: 1106, duration: 0.004s, episode steps:  12, steps per second: 3158, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 43654/100000: episode: 1107, duration: 0.011s, episode steps:  35, steps per second: 3061, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 43693/100000: episode: 1108, duration: 0.013s, episode steps:  39, steps per second: 3084, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 43717/100000: episode: 1109, duration: 0.008s, episode steps:  24, steps per second: 3111, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 43759/100000: episode: 1110, duration: 0.012s, episode steps:  42, steps per second: 3397, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 43773/100000: episode: 1111, duration: 0.004s, episode steps:  14, steps per second: 3162, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 43834/100000: episode: 1112, duration: 0.017s, episode steps:  61, steps per second: 3606, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.443 [0.000, 1.000],  mean_best_reward: --\n",
      " 43894/100000: episode: 1113, duration: 0.018s, episode steps:  60, steps per second: 3277, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 43910/100000: episode: 1114, duration: 0.005s, episode steps:  16, steps per second: 3255, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      " 43953/100000: episode: 1115, duration: 0.012s, episode steps:  43, steps per second: 3576, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 43977/100000: episode: 1116, duration: 0.007s, episode steps:  24, steps per second: 3452, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44031/100000: episode: 1117, duration: 0.016s, episode steps:  54, steps per second: 3375, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44066/100000: episode: 1118, duration: 0.010s, episode steps:  35, steps per second: 3397, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 44139/100000: episode: 1119, duration: 0.020s, episode steps:  73, steps per second: 3609, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 44170/100000: episode: 1120, duration: 0.010s, episode steps:  31, steps per second: 3134, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 44252/100000: episode: 1121, duration: 0.025s, episode steps:  82, steps per second: 3290, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 44304/100000: episode: 1122, duration: 0.016s, episode steps:  52, steps per second: 3179, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44337/100000: episode: 1123, duration: 0.012s, episode steps:  33, steps per second: 2690, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 44400/100000: episode: 1124, duration: 0.018s, episode steps:  63, steps per second: 3412, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 44423/100000: episode: 1125, duration: 0.007s, episode steps:  23, steps per second: 3273, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 44454/100000: episode: 1126, duration: 0.009s, episode steps:  31, steps per second: 3562, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 44483/100000: episode: 1127, duration: 0.009s, episode steps:  29, steps per second: 3366, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 44559/100000: episode: 1128, duration: 0.023s, episode steps:  76, steps per second: 3264, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 44578/100000: episode: 1129, duration: 0.006s, episode steps:  19, steps per second: 3403, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 44630/100000: episode: 1130, duration: 0.015s, episode steps:  52, steps per second: 3398, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 44712/100000: episode: 1131, duration: 0.023s, episode steps:  82, steps per second: 3507, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 44761/100000: episode: 1132, duration: 0.015s, episode steps:  49, steps per second: 3325, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 44919/100000: episode: 1133, duration: 0.041s, episode steps: 158, steps per second: 3807, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 44944/100000: episode: 1134, duration: 0.009s, episode steps:  25, steps per second: 2804, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 45012/100000: episode: 1135, duration: 0.019s, episode steps:  68, steps per second: 3494, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 45057/100000: episode: 1136, duration: 0.012s, episode steps:  45, steps per second: 3649, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 45101/100000: episode: 1137, duration: 0.014s, episode steps:  44, steps per second: 3157, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 45163/100000: episode: 1138, duration: 0.017s, episode steps:  62, steps per second: 3601, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 45188/100000: episode: 1139, duration: 0.007s, episode steps:  25, steps per second: 3515, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 45241/100000: episode: 1140, duration: 0.015s, episode steps:  53, steps per second: 3493, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 45319/100000: episode: 1141, duration: 0.021s, episode steps:  78, steps per second: 3682, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 45366/100000: episode: 1142, duration: 0.014s, episode steps:  47, steps per second: 3378, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 45377/100000: episode: 1143, duration: 0.003s, episode steps:  11, steps per second: 3171, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 45444/100000: episode: 1144, duration: 0.018s, episode steps:  67, steps per second: 3729, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 45475/100000: episode: 1145, duration: 0.009s, episode steps:  31, steps per second: 3527, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 45506/100000: episode: 1146, duration: 0.009s, episode steps:  31, steps per second: 3575, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 45522/100000: episode: 1147, duration: 0.005s, episode steps:  16, steps per second: 3420, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 45540/100000: episode: 1148, duration: 0.005s, episode steps:  18, steps per second: 3447, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 45553/100000: episode: 1149, duration: 0.006s, episode steps:  13, steps per second: 2177, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 45601/100000: episode: 1150, duration: 0.017s, episode steps:  48, steps per second: 2896, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 45635/100000: episode: 1151, duration: 0.010s, episode steps:  34, steps per second: 3308, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: 108.500000\n",
      " 45667/100000: episode: 1152, duration: 0.010s, episode steps:  32, steps per second: 3270, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 45690/100000: episode: 1153, duration: 0.007s, episode steps:  23, steps per second: 3193, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 45713/100000: episode: 1154, duration: 0.007s, episode steps:  23, steps per second: 3275, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 45762/100000: episode: 1155, duration: 0.015s, episode steps:  49, steps per second: 3325, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 45789/100000: episode: 1156, duration: 0.009s, episode steps:  27, steps per second: 3151, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 45825/100000: episode: 1157, duration: 0.011s, episode steps:  36, steps per second: 3209, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 45839/100000: episode: 1158, duration: 0.005s, episode steps:  14, steps per second: 2641, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 45906/100000: episode: 1159, duration: 0.020s, episode steps:  67, steps per second: 3344, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 45966/100000: episode: 1160, duration: 0.023s, episode steps:  60, steps per second: 2625, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 46029/100000: episode: 1161, duration: 0.022s, episode steps:  63, steps per second: 2830, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 46073/100000: episode: 1162, duration: 0.017s, episode steps:  44, steps per second: 2572, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46135/100000: episode: 1163, duration: 0.024s, episode steps:  62, steps per second: 2535, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 46180/100000: episode: 1164, duration: 0.018s, episode steps:  45, steps per second: 2437, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 46206/100000: episode: 1165, duration: 0.008s, episode steps:  26, steps per second: 3092, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46270/100000: episode: 1166, duration: 0.018s, episode steps:  64, steps per second: 3576, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 46319/100000: episode: 1167, duration: 0.014s, episode steps:  49, steps per second: 3584, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 46362/100000: episode: 1168, duration: 0.015s, episode steps:  43, steps per second: 2887, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.605 [0.000, 1.000],  mean_best_reward: --\n",
      " 46443/100000: episode: 1169, duration: 0.024s, episode steps:  81, steps per second: 3445, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 46464/100000: episode: 1170, duration: 0.007s, episode steps:  21, steps per second: 3229, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 46491/100000: episode: 1171, duration: 0.008s, episode steps:  27, steps per second: 3529, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 46549/100000: episode: 1172, duration: 0.016s, episode steps:  58, steps per second: 3594, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 46647/100000: episode: 1173, duration: 0.026s, episode steps:  98, steps per second: 3767, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 46663/100000: episode: 1174, duration: 0.006s, episode steps:  16, steps per second: 2788, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 46706/100000: episode: 1175, duration: 0.013s, episode steps:  43, steps per second: 3233, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 46742/100000: episode: 1176, duration: 0.011s, episode steps:  36, steps per second: 3145, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46765/100000: episode: 1177, duration: 0.009s, episode steps:  23, steps per second: 2647, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 46789/100000: episode: 1178, duration: 0.007s, episode steps:  24, steps per second: 3337, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 46821/100000: episode: 1179, duration: 0.009s, episode steps:  32, steps per second: 3503, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 46879/100000: episode: 1180, duration: 0.018s, episode steps:  58, steps per second: 3151, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 46909/100000: episode: 1181, duration: 0.008s, episode steps:  30, steps per second: 3558, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 46922/100000: episode: 1182, duration: 0.004s, episode steps:  13, steps per second: 3167, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      " 46986/100000: episode: 1183, duration: 0.019s, episode steps:  64, steps per second: 3353, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  mean_best_reward: --\n",
      " 47057/100000: episode: 1184, duration: 0.020s, episode steps:  71, steps per second: 3629, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 47092/100000: episode: 1185, duration: 0.011s, episode steps:  35, steps per second: 3121, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 47123/100000: episode: 1186, duration: 0.009s, episode steps:  31, steps per second: 3534, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 47245/100000: episode: 1187, duration: 0.033s, episode steps: 122, steps per second: 3740, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 47358/100000: episode: 1188, duration: 0.032s, episode steps: 113, steps per second: 3514, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 47402/100000: episode: 1189, duration: 0.012s, episode steps:  44, steps per second: 3533, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 47445/100000: episode: 1190, duration: 0.013s, episode steps:  43, steps per second: 3229, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 47472/100000: episode: 1191, duration: 0.011s, episode steps:  27, steps per second: 2536, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 47499/100000: episode: 1192, duration: 0.009s, episode steps:  27, steps per second: 2978, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  mean_best_reward: --\n",
      " 47523/100000: episode: 1193, duration: 0.007s, episode steps:  24, steps per second: 3516, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 47591/100000: episode: 1194, duration: 0.019s, episode steps:  68, steps per second: 3530, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 47618/100000: episode: 1195, duration: 0.008s, episode steps:  27, steps per second: 3400, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 47635/100000: episode: 1196, duration: 0.005s, episode steps:  17, steps per second: 3464, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 47673/100000: episode: 1197, duration: 0.013s, episode steps:  38, steps per second: 2823, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 47700/100000: episode: 1198, duration: 0.008s, episode steps:  27, steps per second: 3178, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 47766/100000: episode: 1199, duration: 0.020s, episode steps:  66, steps per second: 3374, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 47797/100000: episode: 1200, duration: 0.009s, episode steps:  31, steps per second: 3646, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 47832/100000: episode: 1201, duration: 0.011s, episode steps:  35, steps per second: 3217, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: 83.000000\n",
      " 47884/100000: episode: 1202, duration: 0.015s, episode steps:  52, steps per second: 3481, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 47917/100000: episode: 1203, duration: 0.009s, episode steps:  33, steps per second: 3561, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 47993/100000: episode: 1204, duration: 0.021s, episode steps:  76, steps per second: 3568, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  mean_best_reward: --\n",
      " 48046/100000: episode: 1205, duration: 0.016s, episode steps:  53, steps per second: 3305, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  mean_best_reward: --\n",
      " 48113/100000: episode: 1206, duration: 0.021s, episode steps:  67, steps per second: 3222, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 48161/100000: episode: 1207, duration: 0.014s, episode steps:  48, steps per second: 3502, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 48241/100000: episode: 1208, duration: 0.022s, episode steps:  80, steps per second: 3584, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 48293/100000: episode: 1209, duration: 0.016s, episode steps:  52, steps per second: 3288, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 48355/100000: episode: 1210, duration: 0.017s, episode steps:  62, steps per second: 3569, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 48438/100000: episode: 1211, duration: 0.024s, episode steps:  83, steps per second: 3481, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 48462/100000: episode: 1212, duration: 0.008s, episode steps:  24, steps per second: 3103, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 48480/100000: episode: 1213, duration: 0.006s, episode steps:  18, steps per second: 2816, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 48507/100000: episode: 1214, duration: 0.008s, episode steps:  27, steps per second: 3238, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 48534/100000: episode: 1215, duration: 0.010s, episode steps:  27, steps per second: 2590, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 48588/100000: episode: 1216, duration: 0.016s, episode steps:  54, steps per second: 3408, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 48640/100000: episode: 1217, duration: 0.015s, episode steps:  52, steps per second: 3532, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 48699/100000: episode: 1218, duration: 0.018s, episode steps:  59, steps per second: 3310, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 48760/100000: episode: 1219, duration: 0.017s, episode steps:  61, steps per second: 3528, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 48825/100000: episode: 1220, duration: 0.019s, episode steps:  65, steps per second: 3350, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 48856/100000: episode: 1221, duration: 0.010s, episode steps:  31, steps per second: 3158, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 48917/100000: episode: 1222, duration: 0.017s, episode steps:  61, steps per second: 3503, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 49021/100000: episode: 1223, duration: 0.028s, episode steps: 104, steps per second: 3690, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 49057/100000: episode: 1224, duration: 0.010s, episode steps:  36, steps per second: 3468, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 49091/100000: episode: 1225, duration: 0.010s, episode steps:  34, steps per second: 3417, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 49135/100000: episode: 1226, duration: 0.012s, episode steps:  44, steps per second: 3665, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 49171/100000: episode: 1227, duration: 0.011s, episode steps:  36, steps per second: 3142, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 49186/100000: episode: 1228, duration: 0.005s, episode steps:  15, steps per second: 3207, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 49219/100000: episode: 1229, duration: 0.010s, episode steps:  33, steps per second: 3468, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 49353/100000: episode: 1230, duration: 0.039s, episode steps: 134, steps per second: 3439, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  mean_best_reward: --\n",
      " 49375/100000: episode: 1231, duration: 0.006s, episode steps:  22, steps per second: 3406, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      " 49417/100000: episode: 1232, duration: 0.012s, episode steps:  42, steps per second: 3419, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 49454/100000: episode: 1233, duration: 0.011s, episode steps:  37, steps per second: 3389, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 49519/100000: episode: 1234, duration: 0.024s, episode steps:  65, steps per second: 2710, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 49540/100000: episode: 1235, duration: 0.008s, episode steps:  21, steps per second: 2574, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 49559/100000: episode: 1236, duration: 0.007s, episode steps:  19, steps per second: 2794, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 49588/100000: episode: 1237, duration: 0.011s, episode steps:  29, steps per second: 2758, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  mean_best_reward: --\n",
      " 49690/100000: episode: 1238, duration: 0.033s, episode steps: 102, steps per second: 3077, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 49739/100000: episode: 1239, duration: 0.014s, episode steps:  49, steps per second: 3508, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 49770/100000: episode: 1240, duration: 0.010s, episode steps:  31, steps per second: 3228, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 49796/100000: episode: 1241, duration: 0.008s, episode steps:  26, steps per second: 3193, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 49858/100000: episode: 1242, duration: 0.020s, episode steps:  62, steps per second: 3083, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 49903/100000: episode: 1243, duration: 0.013s, episode steps:  45, steps per second: 3534, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 49939/100000: episode: 1244, duration: 0.011s, episode steps:  36, steps per second: 3418, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 49976/100000: episode: 1245, duration: 0.012s, episode steps:  37, steps per second: 3057, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 50083/100000: episode: 1246, duration: 0.032s, episode steps: 107, steps per second: 3304, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 50107/100000: episode: 1247, duration: 0.008s, episode steps:  24, steps per second: 3143, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 50142/100000: episode: 1248, duration: 0.010s, episode steps:  35, steps per second: 3500, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 50204/100000: episode: 1249, duration: 0.017s, episode steps:  62, steps per second: 3566, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 50246/100000: episode: 1250, duration: 0.013s, episode steps:  42, steps per second: 3247, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 50354/100000: episode: 1251, duration: 0.030s, episode steps: 108, steps per second: 3614, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: 109.500000\n",
      " 50374/100000: episode: 1252, duration: 0.007s, episode steps:  20, steps per second: 3024, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 50454/100000: episode: 1253, duration: 0.022s, episode steps:  80, steps per second: 3556, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 50505/100000: episode: 1254, duration: 0.015s, episode steps:  51, steps per second: 3501, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  mean_best_reward: --\n",
      " 50570/100000: episode: 1255, duration: 0.018s, episode steps:  65, steps per second: 3571, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 50627/100000: episode: 1256, duration: 0.016s, episode steps:  57, steps per second: 3455, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  mean_best_reward: --\n",
      " 50714/100000: episode: 1257, duration: 0.027s, episode steps:  87, steps per second: 3252, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 50736/100000: episode: 1258, duration: 0.007s, episode steps:  22, steps per second: 3077, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      " 50798/100000: episode: 1259, duration: 0.017s, episode steps:  62, steps per second: 3716, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 50823/100000: episode: 1260, duration: 0.007s, episode steps:  25, steps per second: 3538, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 50919/100000: episode: 1261, duration: 0.027s, episode steps:  96, steps per second: 3517, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 50977/100000: episode: 1262, duration: 0.016s, episode steps:  58, steps per second: 3633, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 51018/100000: episode: 1263, duration: 0.012s, episode steps:  41, steps per second: 3338, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 51062/100000: episode: 1264, duration: 0.013s, episode steps:  44, steps per second: 3487, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 51112/100000: episode: 1265, duration: 0.014s, episode steps:  50, steps per second: 3533, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 51138/100000: episode: 1266, duration: 0.007s, episode steps:  26, steps per second: 3585, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 51159/100000: episode: 1267, duration: 0.006s, episode steps:  21, steps per second: 3478, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 51198/100000: episode: 1268, duration: 0.011s, episode steps:  39, steps per second: 3546, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  mean_best_reward: --\n",
      " 51234/100000: episode: 1269, duration: 0.014s, episode steps:  36, steps per second: 2520, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 51262/100000: episode: 1270, duration: 0.011s, episode steps:  28, steps per second: 2452, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51279/100000: episode: 1271, duration: 0.005s, episode steps:  17, steps per second: 3355, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n",
      " 51343/100000: episode: 1272, duration: 0.018s, episode steps:  64, steps per second: 3515, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 51420/100000: episode: 1273, duration: 0.023s, episode steps:  77, steps per second: 3278, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 51469/100000: episode: 1274, duration: 0.015s, episode steps:  49, steps per second: 3347, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 51502/100000: episode: 1275, duration: 0.011s, episode steps:  33, steps per second: 3023, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 51563/100000: episode: 1276, duration: 0.019s, episode steps:  61, steps per second: 3234, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 51597/100000: episode: 1277, duration: 0.012s, episode steps:  34, steps per second: 2835, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 51620/100000: episode: 1278, duration: 0.007s, episode steps:  23, steps per second: 3202, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  mean_best_reward: --\n",
      " 51743/100000: episode: 1279, duration: 0.036s, episode steps: 123, steps per second: 3424, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 51862/100000: episode: 1280, duration: 0.035s, episode steps: 119, steps per second: 3406, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 51927/100000: episode: 1281, duration: 0.020s, episode steps:  65, steps per second: 3298, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 51969/100000: episode: 1282, duration: 0.013s, episode steps:  42, steps per second: 3344, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51982/100000: episode: 1283, duration: 0.004s, episode steps:  13, steps per second: 3242, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 52030/100000: episode: 1284, duration: 0.014s, episode steps:  48, steps per second: 3516, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 52076/100000: episode: 1285, duration: 0.013s, episode steps:  46, steps per second: 3501, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 52121/100000: episode: 1286, duration: 0.013s, episode steps:  45, steps per second: 3416, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 52216/100000: episode: 1287, duration: 0.026s, episode steps:  95, steps per second: 3664, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 52259/100000: episode: 1288, duration: 0.013s, episode steps:  43, steps per second: 3302, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 52283/100000: episode: 1289, duration: 0.007s, episode steps:  24, steps per second: 3263, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 52342/100000: episode: 1290, duration: 0.016s, episode steps:  59, steps per second: 3662, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 52377/100000: episode: 1291, duration: 0.010s, episode steps:  35, steps per second: 3531, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 52400/100000: episode: 1292, duration: 0.007s, episode steps:  23, steps per second: 3223, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 52453/100000: episode: 1293, duration: 0.016s, episode steps:  53, steps per second: 3249, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 52487/100000: episode: 1294, duration: 0.010s, episode steps:  34, steps per second: 3531, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 52524/100000: episode: 1295, duration: 0.011s, episode steps:  37, steps per second: 3378, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 52654/100000: episode: 1296, duration: 0.037s, episode steps: 130, steps per second: 3553, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 52694/100000: episode: 1297, duration: 0.011s, episode steps:  40, steps per second: 3616, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 52732/100000: episode: 1298, duration: 0.012s, episode steps:  38, steps per second: 3266, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 52759/100000: episode: 1299, duration: 0.009s, episode steps:  27, steps per second: 3047, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 52775/100000: episode: 1300, duration: 0.005s, episode steps:  16, steps per second: 2931, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 52846/100000: episode: 1301, duration: 0.023s, episode steps:  71, steps per second: 3032, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: 110.000000\n",
      " 52885/100000: episode: 1302, duration: 0.012s, episode steps:  39, steps per second: 3204, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 52918/100000: episode: 1303, duration: 0.009s, episode steps:  33, steps per second: 3558, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 52952/100000: episode: 1304, duration: 0.011s, episode steps:  34, steps per second: 2999, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 52987/100000: episode: 1305, duration: 0.010s, episode steps:  35, steps per second: 3495, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 53095/100000: episode: 1306, duration: 0.036s, episode steps: 108, steps per second: 2996, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 53106/100000: episode: 1307, duration: 0.004s, episode steps:  11, steps per second: 2643, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 53156/100000: episode: 1308, duration: 0.017s, episode steps:  50, steps per second: 2875, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 53253/100000: episode: 1309, duration: 0.034s, episode steps:  97, steps per second: 2879, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 53271/100000: episode: 1310, duration: 0.007s, episode steps:  18, steps per second: 2739, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53300/100000: episode: 1311, duration: 0.008s, episode steps:  29, steps per second: 3424, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 53326/100000: episode: 1312, duration: 0.008s, episode steps:  26, steps per second: 3419, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 53390/100000: episode: 1313, duration: 0.018s, episode steps:  64, steps per second: 3519, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 53422/100000: episode: 1314, duration: 0.011s, episode steps:  32, steps per second: 2829, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53458/100000: episode: 1315, duration: 0.010s, episode steps:  36, steps per second: 3484, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 53503/100000: episode: 1316, duration: 0.015s, episode steps:  45, steps per second: 2982, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 53595/100000: episode: 1317, duration: 0.026s, episode steps:  92, steps per second: 3593, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 53647/100000: episode: 1318, duration: 0.015s, episode steps:  52, steps per second: 3502, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53700/100000: episode: 1319, duration: 0.015s, episode steps:  53, steps per second: 3470, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 53723/100000: episode: 1320, duration: 0.009s, episode steps:  23, steps per second: 2462, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 53763/100000: episode: 1321, duration: 0.014s, episode steps:  40, steps per second: 2866, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53795/100000: episode: 1322, duration: 0.010s, episode steps:  32, steps per second: 3074, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 53844/100000: episode: 1323, duration: 0.015s, episode steps:  49, steps per second: 3282, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 53863/100000: episode: 1324, duration: 0.007s, episode steps:  19, steps per second: 2894, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 53885/100000: episode: 1325, duration: 0.007s, episode steps:  22, steps per second: 3209, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      " 53969/100000: episode: 1326, duration: 0.029s, episode steps:  84, steps per second: 2926, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 54013/100000: episode: 1327, duration: 0.016s, episode steps:  44, steps per second: 2720, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 54043/100000: episode: 1328, duration: 0.010s, episode steps:  30, steps per second: 2861, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 54095/100000: episode: 1329, duration: 0.015s, episode steps:  52, steps per second: 3558, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 54110/100000: episode: 1330, duration: 0.005s, episode steps:  15, steps per second: 3222, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 54140/100000: episode: 1331, duration: 0.010s, episode steps:  30, steps per second: 3050, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 54166/100000: episode: 1332, duration: 0.008s, episode steps:  26, steps per second: 3195, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 54195/100000: episode: 1333, duration: 0.008s, episode steps:  29, steps per second: 3527, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 54245/100000: episode: 1334, duration: 0.014s, episode steps:  50, steps per second: 3571, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 54308/100000: episode: 1335, duration: 0.018s, episode steps:  63, steps per second: 3409, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 54404/100000: episode: 1336, duration: 0.028s, episode steps:  96, steps per second: 3454, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 54427/100000: episode: 1337, duration: 0.007s, episode steps:  23, steps per second: 3444, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 54485/100000: episode: 1338, duration: 0.017s, episode steps:  58, steps per second: 3352, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 54532/100000: episode: 1339, duration: 0.014s, episode steps:  47, steps per second: 3275, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      " 54556/100000: episode: 1340, duration: 0.008s, episode steps:  24, steps per second: 3032, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 54579/100000: episode: 1341, duration: 0.009s, episode steps:  23, steps per second: 2519, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 54636/100000: episode: 1342, duration: 0.016s, episode steps:  57, steps per second: 3581, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 54670/100000: episode: 1343, duration: 0.010s, episode steps:  34, steps per second: 3549, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 54701/100000: episode: 1344, duration: 0.009s, episode steps:  31, steps per second: 3587, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 54729/100000: episode: 1345, duration: 0.010s, episode steps:  28, steps per second: 2747, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 54780/100000: episode: 1346, duration: 0.015s, episode steps:  51, steps per second: 3370, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 54852/100000: episode: 1347, duration: 0.020s, episode steps:  72, steps per second: 3565, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 54867/100000: episode: 1348, duration: 0.004s, episode steps:  15, steps per second: 3366, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 54898/100000: episode: 1349, duration: 0.011s, episode steps:  31, steps per second: 2753, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 54918/100000: episode: 1350, duration: 0.006s, episode steps:  20, steps per second: 3275, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 54958/100000: episode: 1351, duration: 0.013s, episode steps:  40, steps per second: 3194, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: 121.000000\n",
      " 55056/100000: episode: 1352, duration: 0.027s, episode steps:  98, steps per second: 3587, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 55110/100000: episode: 1353, duration: 0.015s, episode steps:  54, steps per second: 3531, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 55126/100000: episode: 1354, duration: 0.005s, episode steps:  16, steps per second: 3383, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 55156/100000: episode: 1355, duration: 0.009s, episode steps:  30, steps per second: 3299, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 55213/100000: episode: 1356, duration: 0.016s, episode steps:  57, steps per second: 3461, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 55248/100000: episode: 1357, duration: 0.011s, episode steps:  35, steps per second: 3156, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 55331/100000: episode: 1358, duration: 0.023s, episode steps:  83, steps per second: 3558, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 55384/100000: episode: 1359, duration: 0.015s, episode steps:  53, steps per second: 3464, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 55411/100000: episode: 1360, duration: 0.008s, episode steps:  27, steps per second: 3316, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 55499/100000: episode: 1361, duration: 0.024s, episode steps:  88, steps per second: 3615, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 55647/100000: episode: 1362, duration: 0.040s, episode steps: 148, steps per second: 3657, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 55740/100000: episode: 1363, duration: 0.025s, episode steps:  93, steps per second: 3718, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 55828/100000: episode: 1364, duration: 0.026s, episode steps:  88, steps per second: 3445, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  mean_best_reward: --\n",
      " 55848/100000: episode: 1365, duration: 0.006s, episode steps:  20, steps per second: 3143, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 55869/100000: episode: 1366, duration: 0.007s, episode steps:  21, steps per second: 2926, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 55893/100000: episode: 1367, duration: 0.007s, episode steps:  24, steps per second: 3406, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 55952/100000: episode: 1368, duration: 0.017s, episode steps:  59, steps per second: 3456, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 56052/100000: episode: 1369, duration: 0.028s, episode steps: 100, steps per second: 3567, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56112/100000: episode: 1370, duration: 0.016s, episode steps:  60, steps per second: 3722, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56129/100000: episode: 1371, duration: 0.008s, episode steps:  17, steps per second: 2111, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  mean_best_reward: --\n",
      " 56217/100000: episode: 1372, duration: 0.026s, episode steps:  88, steps per second: 3423, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56240/100000: episode: 1373, duration: 0.008s, episode steps:  23, steps per second: 2761, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 56265/100000: episode: 1374, duration: 0.007s, episode steps:  25, steps per second: 3393, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 56311/100000: episode: 1375, duration: 0.013s, episode steps:  46, steps per second: 3630, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56327/100000: episode: 1376, duration: 0.005s, episode steps:  16, steps per second: 2982, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      " 56367/100000: episode: 1377, duration: 0.011s, episode steps:  40, steps per second: 3608, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 56388/100000: episode: 1378, duration: 0.008s, episode steps:  21, steps per second: 2651, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  mean_best_reward: --\n",
      " 56482/100000: episode: 1379, duration: 0.026s, episode steps:  94, steps per second: 3624, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 56504/100000: episode: 1380, duration: 0.008s, episode steps:  22, steps per second: 2866, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56614/100000: episode: 1381, duration: 0.042s, episode steps: 110, steps per second: 2615, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 56714/100000: episode: 1382, duration: 0.034s, episode steps: 100, steps per second: 2905, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 56746/100000: episode: 1383, duration: 0.010s, episode steps:  32, steps per second: 3329, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 56941/100000: episode: 1384, duration: 0.055s, episode steps: 195, steps per second: 3573, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 56966/100000: episode: 1385, duration: 0.008s, episode steps:  25, steps per second: 3324, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 57033/100000: episode: 1386, duration: 0.020s, episode steps:  67, steps per second: 3435, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 57046/100000: episode: 1387, duration: 0.004s, episode steps:  13, steps per second: 3173, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 57132/100000: episode: 1388, duration: 0.023s, episode steps:  86, steps per second: 3755, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 57197/100000: episode: 1389, duration: 0.021s, episode steps:  65, steps per second: 3143, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 57313/100000: episode: 1390, duration: 0.033s, episode steps: 116, steps per second: 3548, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 57362/100000: episode: 1391, duration: 0.015s, episode steps:  49, steps per second: 3244, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 57397/100000: episode: 1392, duration: 0.011s, episode steps:  35, steps per second: 3242, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 57466/100000: episode: 1393, duration: 0.021s, episode steps:  69, steps per second: 3279, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 57490/100000: episode: 1394, duration: 0.007s, episode steps:  24, steps per second: 3325, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 57574/100000: episode: 1395, duration: 0.024s, episode steps:  84, steps per second: 3476, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 57610/100000: episode: 1396, duration: 0.011s, episode steps:  36, steps per second: 3297, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 57679/100000: episode: 1397, duration: 0.019s, episode steps:  69, steps per second: 3646, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  mean_best_reward: --\n",
      " 57698/100000: episode: 1398, duration: 0.006s, episode steps:  19, steps per second: 3338, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 57721/100000: episode: 1399, duration: 0.007s, episode steps:  23, steps per second: 3388, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 57771/100000: episode: 1400, duration: 0.014s, episode steps:  50, steps per second: 3561, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 57807/100000: episode: 1401, duration: 0.012s, episode steps:  36, steps per second: 2915, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 94.000000\n",
      " 57862/100000: episode: 1402, duration: 0.015s, episode steps:  55, steps per second: 3653, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 57950/100000: episode: 1403, duration: 0.028s, episode steps:  88, steps per second: 3132, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 57997/100000: episode: 1404, duration: 0.014s, episode steps:  47, steps per second: 3388, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 58092/100000: episode: 1405, duration: 0.028s, episode steps:  95, steps per second: 3349, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 58136/100000: episode: 1406, duration: 0.013s, episode steps:  44, steps per second: 3355, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  mean_best_reward: --\n",
      " 58168/100000: episode: 1407, duration: 0.009s, episode steps:  32, steps per second: 3475, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 58182/100000: episode: 1408, duration: 0.004s, episode steps:  14, steps per second: 3281, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 58201/100000: episode: 1409, duration: 0.006s, episode steps:  19, steps per second: 3097, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 58226/100000: episode: 1410, duration: 0.008s, episode steps:  25, steps per second: 2992, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 58287/100000: episode: 1411, duration: 0.017s, episode steps:  61, steps per second: 3600, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 58305/100000: episode: 1412, duration: 0.005s, episode steps:  18, steps per second: 3286, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 58332/100000: episode: 1413, duration: 0.008s, episode steps:  27, steps per second: 3446, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 58366/100000: episode: 1414, duration: 0.010s, episode steps:  34, steps per second: 3431, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 58387/100000: episode: 1415, duration: 0.006s, episode steps:  21, steps per second: 3428, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 58490/100000: episode: 1416, duration: 0.029s, episode steps: 103, steps per second: 3568, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 58548/100000: episode: 1417, duration: 0.016s, episode steps:  58, steps per second: 3634, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 58592/100000: episode: 1418, duration: 0.012s, episode steps:  44, steps per second: 3632, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 58652/100000: episode: 1419, duration: 0.019s, episode steps:  60, steps per second: 3236, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 58714/100000: episode: 1420, duration: 0.020s, episode steps:  62, steps per second: 3156, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 58727/100000: episode: 1421, duration: 0.004s, episode steps:  13, steps per second: 2949, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 58773/100000: episode: 1422, duration: 0.014s, episode steps:  46, steps per second: 3391, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 58820/100000: episode: 1423, duration: 0.016s, episode steps:  47, steps per second: 2899, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 58876/100000: episode: 1424, duration: 0.015s, episode steps:  56, steps per second: 3638, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 58914/100000: episode: 1425, duration: 0.011s, episode steps:  38, steps per second: 3559, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 58925/100000: episode: 1426, duration: 0.003s, episode steps:  11, steps per second: 3168, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      " 58936/100000: episode: 1427, duration: 0.004s, episode steps:  11, steps per second: 3104, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      " 59032/100000: episode: 1428, duration: 0.029s, episode steps:  96, steps per second: 3350, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 59068/100000: episode: 1429, duration: 0.010s, episode steps:  36, steps per second: 3528, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 59124/100000: episode: 1430, duration: 0.016s, episode steps:  56, steps per second: 3593, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 59187/100000: episode: 1431, duration: 0.019s, episode steps:  63, steps per second: 3277, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 59251/100000: episode: 1432, duration: 0.020s, episode steps:  64, steps per second: 3237, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59325/100000: episode: 1433, duration: 0.021s, episode steps:  74, steps per second: 3551, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 59375/100000: episode: 1434, duration: 0.016s, episode steps:  50, steps per second: 3071, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59432/100000: episode: 1435, duration: 0.017s, episode steps:  57, steps per second: 3325, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  mean_best_reward: --\n",
      " 59468/100000: episode: 1436, duration: 0.010s, episode steps:  36, steps per second: 3610, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 59500/100000: episode: 1437, duration: 0.010s, episode steps:  32, steps per second: 3326, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 59527/100000: episode: 1438, duration: 0.008s, episode steps:  27, steps per second: 3358, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 59547/100000: episode: 1439, duration: 0.006s, episode steps:  20, steps per second: 3417, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59583/100000: episode: 1440, duration: 0.010s, episode steps:  36, steps per second: 3453, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59595/100000: episode: 1441, duration: 0.004s, episode steps:  12, steps per second: 2817, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      " 59666/100000: episode: 1442, duration: 0.022s, episode steps:  71, steps per second: 3205, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 59712/100000: episode: 1443, duration: 0.013s, episode steps:  46, steps per second: 3646, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59799/100000: episode: 1444, duration: 0.025s, episode steps:  87, steps per second: 3439, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 59841/100000: episode: 1445, duration: 0.012s, episode steps:  42, steps per second: 3418, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59886/100000: episode: 1446, duration: 0.013s, episode steps:  45, steps per second: 3373, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 59965/100000: episode: 1447, duration: 0.026s, episode steps:  79, steps per second: 3086, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  mean_best_reward: --\n",
      " 60088/100000: episode: 1448, duration: 0.042s, episode steps: 123, steps per second: 2938, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  mean_best_reward: --\n",
      " 60161/100000: episode: 1449, duration: 0.029s, episode steps:  73, steps per second: 2493, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 60199/100000: episode: 1450, duration: 0.015s, episode steps:  38, steps per second: 2609, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 60229/100000: episode: 1451, duration: 0.010s, episode steps:  30, steps per second: 2913, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  mean_best_reward: 113.000000\n",
      " 60268/100000: episode: 1452, duration: 0.016s, episode steps:  39, steps per second: 2404, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 60329/100000: episode: 1453, duration: 0.018s, episode steps:  61, steps per second: 3397, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 60381/100000: episode: 1454, duration: 0.015s, episode steps:  52, steps per second: 3394, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 60495/100000: episode: 1455, duration: 0.037s, episode steps: 114, steps per second: 3045, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 60514/100000: episode: 1456, duration: 0.006s, episode steps:  19, steps per second: 3280, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 60540/100000: episode: 1457, duration: 0.010s, episode steps:  26, steps per second: 2530, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 60594/100000: episode: 1458, duration: 0.015s, episode steps:  54, steps per second: 3587, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 60638/100000: episode: 1459, duration: 0.013s, episode steps:  44, steps per second: 3365, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 60751/100000: episode: 1460, duration: 0.030s, episode steps: 113, steps per second: 3783, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  mean_best_reward: --\n",
      " 60766/100000: episode: 1461, duration: 0.005s, episode steps:  15, steps per second: 3028, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 60815/100000: episode: 1462, duration: 0.014s, episode steps:  49, steps per second: 3511, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 60825/100000: episode: 1463, duration: 0.004s, episode steps:  10, steps per second: 2709, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  mean_best_reward: --\n",
      " 60899/100000: episode: 1464, duration: 0.020s, episode steps:  74, steps per second: 3695, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 60946/100000: episode: 1465, duration: 0.013s, episode steps:  47, steps per second: 3595, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 60995/100000: episode: 1466, duration: 0.013s, episode steps:  49, steps per second: 3677, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 61024/100000: episode: 1467, duration: 0.009s, episode steps:  29, steps per second: 3222, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 61070/100000: episode: 1468, duration: 0.014s, episode steps:  46, steps per second: 3365, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 61086/100000: episode: 1469, duration: 0.006s, episode steps:  16, steps per second: 2887, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 61170/100000: episode: 1470, duration: 0.023s, episode steps:  84, steps per second: 3591, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 61238/100000: episode: 1471, duration: 0.019s, episode steps:  68, steps per second: 3618, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 61318/100000: episode: 1472, duration: 0.024s, episode steps:  80, steps per second: 3369, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 61378/100000: episode: 1473, duration: 0.018s, episode steps:  60, steps per second: 3321, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 61477/100000: episode: 1474, duration: 0.029s, episode steps:  99, steps per second: 3374, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 61566/100000: episode: 1475, duration: 0.026s, episode steps:  89, steps per second: 3438, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 61598/100000: episode: 1476, duration: 0.010s, episode steps:  32, steps per second: 3085, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 61648/100000: episode: 1477, duration: 0.014s, episode steps:  50, steps per second: 3609, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 61691/100000: episode: 1478, duration: 0.012s, episode steps:  43, steps per second: 3556, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 61741/100000: episode: 1479, duration: 0.014s, episode steps:  50, steps per second: 3620, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 61776/100000: episode: 1480, duration: 0.017s, episode steps:  35, steps per second: 2056, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 61811/100000: episode: 1481, duration: 0.012s, episode steps:  35, steps per second: 3002, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 61881/100000: episode: 1482, duration: 0.021s, episode steps:  70, steps per second: 3279, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 61936/100000: episode: 1483, duration: 0.016s, episode steps:  55, steps per second: 3465, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 62059/100000: episode: 1484, duration: 0.033s, episode steps: 123, steps per second: 3703, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 62080/100000: episode: 1485, duration: 0.006s, episode steps:  21, steps per second: 3417, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 62110/100000: episode: 1486, duration: 0.009s, episode steps:  30, steps per second: 3484, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  mean_best_reward: --\n",
      " 62130/100000: episode: 1487, duration: 0.007s, episode steps:  20, steps per second: 2906, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 62179/100000: episode: 1488, duration: 0.015s, episode steps:  49, steps per second: 3198, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 62203/100000: episode: 1489, duration: 0.007s, episode steps:  24, steps per second: 3434, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 62320/100000: episode: 1490, duration: 0.032s, episode steps: 117, steps per second: 3671, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 62399/100000: episode: 1491, duration: 0.022s, episode steps:  79, steps per second: 3611, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 62451/100000: episode: 1492, duration: 0.014s, episode steps:  52, steps per second: 3625, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  mean_best_reward: --\n",
      " 62486/100000: episode: 1493, duration: 0.011s, episode steps:  35, steps per second: 3143, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 62520/100000: episode: 1494, duration: 0.010s, episode steps:  34, steps per second: 3506, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 62574/100000: episode: 1495, duration: 0.015s, episode steps:  54, steps per second: 3508, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 62607/100000: episode: 1496, duration: 0.010s, episode steps:  33, steps per second: 3473, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 62639/100000: episode: 1497, duration: 0.010s, episode steps:  32, steps per second: 3275, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 62699/100000: episode: 1498, duration: 0.017s, episode steps:  60, steps per second: 3451, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 62733/100000: episode: 1499, duration: 0.010s, episode steps:  34, steps per second: 3460, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  mean_best_reward: --\n",
      " 62768/100000: episode: 1500, duration: 0.010s, episode steps:  35, steps per second: 3552, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 62824/100000: episode: 1501, duration: 0.019s, episode steps:  56, steps per second: 2913, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: 124.500000\n",
      " 62917/100000: episode: 1502, duration: 0.026s, episode steps:  93, steps per second: 3575, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 62955/100000: episode: 1503, duration: 0.012s, episode steps:  38, steps per second: 3184, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      " 62970/100000: episode: 1504, duration: 0.005s, episode steps:  15, steps per second: 3008, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 63068/100000: episode: 1505, duration: 0.029s, episode steps:  98, steps per second: 3391, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 63123/100000: episode: 1506, duration: 0.016s, episode steps:  55, steps per second: 3496, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 63164/100000: episode: 1507, duration: 0.012s, episode steps:  41, steps per second: 3420, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 63243/100000: episode: 1508, duration: 0.024s, episode steps:  79, steps per second: 3306, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 63316/100000: episode: 1509, duration: 0.021s, episode steps:  73, steps per second: 3504, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 63357/100000: episode: 1510, duration: 0.014s, episode steps:  41, steps per second: 2997, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 63415/100000: episode: 1511, duration: 0.016s, episode steps:  58, steps per second: 3566, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 63446/100000: episode: 1512, duration: 0.009s, episode steps:  31, steps per second: 3366, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 63537/100000: episode: 1513, duration: 0.026s, episode steps:  91, steps per second: 3461, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 63621/100000: episode: 1514, duration: 0.029s, episode steps:  84, steps per second: 2866, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63661/100000: episode: 1515, duration: 0.013s, episode steps:  40, steps per second: 3033, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 63676/100000: episode: 1516, duration: 0.006s, episode steps:  15, steps per second: 2630, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 63709/100000: episode: 1517, duration: 0.012s, episode steps:  33, steps per second: 2651, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 63763/100000: episode: 1518, duration: 0.017s, episode steps:  54, steps per second: 3145, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63792/100000: episode: 1519, duration: 0.009s, episode steps:  29, steps per second: 3152, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 63864/100000: episode: 1520, duration: 0.020s, episode steps:  72, steps per second: 3603, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 63888/100000: episode: 1521, duration: 0.007s, episode steps:  24, steps per second: 3391, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 63922/100000: episode: 1522, duration: 0.011s, episode steps:  34, steps per second: 3014, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 64003/100000: episode: 1523, duration: 0.023s, episode steps:  81, steps per second: 3585, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 64122/100000: episode: 1524, duration: 0.034s, episode steps: 119, steps per second: 3544, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 64167/100000: episode: 1525, duration: 0.013s, episode steps:  45, steps per second: 3576, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 64250/100000: episode: 1526, duration: 0.024s, episode steps:  83, steps per second: 3475, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 64282/100000: episode: 1527, duration: 0.010s, episode steps:  32, steps per second: 3109, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 64339/100000: episode: 1528, duration: 0.017s, episode steps:  57, steps per second: 3444, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 64395/100000: episode: 1529, duration: 0.017s, episode steps:  56, steps per second: 3246, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 64407/100000: episode: 1530, duration: 0.004s, episode steps:  12, steps per second: 2754, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 64421/100000: episode: 1531, duration: 0.004s, episode steps:  14, steps per second: 3133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 64450/100000: episode: 1532, duration: 0.010s, episode steps:  29, steps per second: 2928, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 64517/100000: episode: 1533, duration: 0.020s, episode steps:  67, steps per second: 3388, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 64575/100000: episode: 1534, duration: 0.018s, episode steps:  58, steps per second: 3251, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 64627/100000: episode: 1535, duration: 0.015s, episode steps:  52, steps per second: 3445, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 64658/100000: episode: 1536, duration: 0.009s, episode steps:  31, steps per second: 3321, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 64707/100000: episode: 1537, duration: 0.014s, episode steps:  49, steps per second: 3450, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  mean_best_reward: --\n",
      " 64731/100000: episode: 1538, duration: 0.007s, episode steps:  24, steps per second: 3373, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 64775/100000: episode: 1539, duration: 0.012s, episode steps:  44, steps per second: 3549, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 64840/100000: episode: 1540, duration: 0.019s, episode steps:  65, steps per second: 3424, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 64924/100000: episode: 1541, duration: 0.023s, episode steps:  84, steps per second: 3596, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 64957/100000: episode: 1542, duration: 0.010s, episode steps:  33, steps per second: 3261, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 64973/100000: episode: 1543, duration: 0.005s, episode steps:  16, steps per second: 3134, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 65049/100000: episode: 1544, duration: 0.023s, episode steps:  76, steps per second: 3237, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 65075/100000: episode: 1545, duration: 0.007s, episode steps:  26, steps per second: 3508, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 65182/100000: episode: 1546, duration: 0.029s, episode steps: 107, steps per second: 3669, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 65216/100000: episode: 1547, duration: 0.011s, episode steps:  34, steps per second: 3222, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 65304/100000: episode: 1548, duration: 0.024s, episode steps:  88, steps per second: 3631, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  mean_best_reward: --\n",
      " 65364/100000: episode: 1549, duration: 0.017s, episode steps:  60, steps per second: 3474, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 65407/100000: episode: 1550, duration: 0.012s, episode steps:  43, steps per second: 3556, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 65434/100000: episode: 1551, duration: 0.009s, episode steps:  27, steps per second: 2942, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: 109.500000\n",
      " 65493/100000: episode: 1552, duration: 0.017s, episode steps:  59, steps per second: 3409, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 65632/100000: episode: 1553, duration: 0.039s, episode steps: 139, steps per second: 3563, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 65701/100000: episode: 1554, duration: 0.019s, episode steps:  69, steps per second: 3563, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 65798/100000: episode: 1555, duration: 0.027s, episode steps:  97, steps per second: 3642, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 65831/100000: episode: 1556, duration: 0.010s, episode steps:  33, steps per second: 3222, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 65936/100000: episode: 1557, duration: 0.029s, episode steps: 105, steps per second: 3679, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 66040/100000: episode: 1558, duration: 0.030s, episode steps: 104, steps per second: 3488, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 66080/100000: episode: 1559, duration: 0.012s, episode steps:  40, steps per second: 3407, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 66092/100000: episode: 1560, duration: 0.004s, episode steps:  12, steps per second: 2798, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 66130/100000: episode: 1561, duration: 0.012s, episode steps:  38, steps per second: 3279, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 66157/100000: episode: 1562, duration: 0.008s, episode steps:  27, steps per second: 3277, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 66219/100000: episode: 1563, duration: 0.018s, episode steps:  62, steps per second: 3481, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 66232/100000: episode: 1564, duration: 0.004s, episode steps:  13, steps per second: 3051, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 66339/100000: episode: 1565, duration: 0.029s, episode steps: 107, steps per second: 3701, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 66369/100000: episode: 1566, duration: 0.008s, episode steps:  30, steps per second: 3594, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 66381/100000: episode: 1567, duration: 0.004s, episode steps:  12, steps per second: 3251, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 66409/100000: episode: 1568, duration: 0.009s, episode steps:  28, steps per second: 3141, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 66443/100000: episode: 1569, duration: 0.010s, episode steps:  34, steps per second: 3481, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 66527/100000: episode: 1570, duration: 0.024s, episode steps:  84, steps per second: 3465, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 66646/100000: episode: 1571, duration: 0.032s, episode steps: 119, steps per second: 3669, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 66685/100000: episode: 1572, duration: 0.011s, episode steps:  39, steps per second: 3587, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 66728/100000: episode: 1573, duration: 0.012s, episode steps:  43, steps per second: 3478, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 66759/100000: episode: 1574, duration: 0.010s, episode steps:  31, steps per second: 3245, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 66792/100000: episode: 1575, duration: 0.010s, episode steps:  33, steps per second: 3227, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 66834/100000: episode: 1576, duration: 0.013s, episode steps:  42, steps per second: 3235, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 66954/100000: episode: 1577, duration: 0.033s, episode steps: 120, steps per second: 3606, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 67043/100000: episode: 1578, duration: 0.024s, episode steps:  89, steps per second: 3758, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 67087/100000: episode: 1579, duration: 0.013s, episode steps:  44, steps per second: 3412, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67136/100000: episode: 1580, duration: 0.019s, episode steps:  49, steps per second: 2533, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 67174/100000: episode: 1581, duration: 0.014s, episode steps:  38, steps per second: 2791, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 67188/100000: episode: 1582, duration: 0.005s, episode steps:  14, steps per second: 2726, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 67214/100000: episode: 1583, duration: 0.010s, episode steps:  26, steps per second: 2617, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 67265/100000: episode: 1584, duration: 0.018s, episode steps:  51, steps per second: 2844, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.392 [0.000, 1.000],  mean_best_reward: --\n",
      " 67287/100000: episode: 1585, duration: 0.009s, episode steps:  22, steps per second: 2567, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      " 67319/100000: episode: 1586, duration: 0.010s, episode steps:  32, steps per second: 3062, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 67382/100000: episode: 1587, duration: 0.019s, episode steps:  63, steps per second: 3236, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 67444/100000: episode: 1588, duration: 0.017s, episode steps:  62, steps per second: 3594, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 67533/100000: episode: 1589, duration: 0.025s, episode steps:  89, steps per second: 3612, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 67591/100000: episode: 1590, duration: 0.018s, episode steps:  58, steps per second: 3196, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  mean_best_reward: --\n",
      " 67653/100000: episode: 1591, duration: 0.017s, episode steps:  62, steps per second: 3630, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 67721/100000: episode: 1592, duration: 0.022s, episode steps:  68, steps per second: 3111, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 67788/100000: episode: 1593, duration: 0.020s, episode steps:  67, steps per second: 3387, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 67818/100000: episode: 1594, duration: 0.008s, episode steps:  30, steps per second: 3557, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 67883/100000: episode: 1595, duration: 0.019s, episode steps:  65, steps per second: 3356, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 67960/100000: episode: 1596, duration: 0.021s, episode steps:  77, steps per second: 3684, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 67974/100000: episode: 1597, duration: 0.004s, episode steps:  14, steps per second: 3308, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 68030/100000: episode: 1598, duration: 0.016s, episode steps:  56, steps per second: 3480, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 68102/100000: episode: 1599, duration: 0.020s, episode steps:  72, steps per second: 3591, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 68152/100000: episode: 1600, duration: 0.014s, episode steps:  50, steps per second: 3476, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 68235/100000: episode: 1601, duration: 0.025s, episode steps:  83, steps per second: 3278, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: 169.000000\n",
      " 68315/100000: episode: 1602, duration: 0.023s, episode steps:  80, steps per second: 3487, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 68350/100000: episode: 1603, duration: 0.012s, episode steps:  35, steps per second: 2946, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 68427/100000: episode: 1604, duration: 0.021s, episode steps:  77, steps per second: 3598, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 68531/100000: episode: 1605, duration: 0.029s, episode steps: 104, steps per second: 3603, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 68610/100000: episode: 1606, duration: 0.023s, episode steps:  79, steps per second: 3510, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 68683/100000: episode: 1607, duration: 0.022s, episode steps:  73, steps per second: 3343, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  mean_best_reward: --\n",
      " 68736/100000: episode: 1608, duration: 0.015s, episode steps:  53, steps per second: 3435, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 68781/100000: episode: 1609, duration: 0.016s, episode steps:  45, steps per second: 2869, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 68809/100000: episode: 1610, duration: 0.013s, episode steps:  28, steps per second: 2211, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 68887/100000: episode: 1611, duration: 0.030s, episode steps:  78, steps per second: 2620, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  mean_best_reward: --\n",
      " 68938/100000: episode: 1612, duration: 0.020s, episode steps:  51, steps per second: 2556, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 69036/100000: episode: 1613, duration: 0.036s, episode steps:  98, steps per second: 2715, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 69071/100000: episode: 1614, duration: 0.010s, episode steps:  35, steps per second: 3412, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 69105/100000: episode: 1615, duration: 0.010s, episode steps:  34, steps per second: 3369, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 69139/100000: episode: 1616, duration: 0.010s, episode steps:  34, steps per second: 3421, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 69155/100000: episode: 1617, duration: 0.005s, episode steps:  16, steps per second: 3110, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 69244/100000: episode: 1618, duration: 0.032s, episode steps:  89, steps per second: 2762, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 69326/100000: episode: 1619, duration: 0.024s, episode steps:  82, steps per second: 3451, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 69346/100000: episode: 1620, duration: 0.006s, episode steps:  20, steps per second: 3120, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 69378/100000: episode: 1621, duration: 0.009s, episode steps:  32, steps per second: 3564, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 69430/100000: episode: 1622, duration: 0.014s, episode steps:  52, steps per second: 3782, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 69449/100000: episode: 1623, duration: 0.007s, episode steps:  19, steps per second: 2561, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 69477/100000: episode: 1624, duration: 0.009s, episode steps:  28, steps per second: 3150, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 69501/100000: episode: 1625, duration: 0.007s, episode steps:  24, steps per second: 3380, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 69596/100000: episode: 1626, duration: 0.026s, episode steps:  95, steps per second: 3604, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 69630/100000: episode: 1627, duration: 0.011s, episode steps:  34, steps per second: 3139, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 69690/100000: episode: 1628, duration: 0.018s, episode steps:  60, steps per second: 3338, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 69777/100000: episode: 1629, duration: 0.026s, episode steps:  87, steps per second: 3406, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 69799/100000: episode: 1630, duration: 0.012s, episode steps:  22, steps per second: 1788, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      " 69869/100000: episode: 1631, duration: 0.024s, episode steps:  70, steps per second: 2938, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 69938/100000: episode: 1632, duration: 0.019s, episode steps:  69, steps per second: 3607, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 70009/100000: episode: 1633, duration: 0.019s, episode steps:  71, steps per second: 3732, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  mean_best_reward: --\n",
      " 70063/100000: episode: 1634, duration: 0.016s, episode steps:  54, steps per second: 3431, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 70126/100000: episode: 1635, duration: 0.018s, episode steps:  63, steps per second: 3513, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 70154/100000: episode: 1636, duration: 0.008s, episode steps:  28, steps per second: 3496, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  mean_best_reward: --\n",
      " 70181/100000: episode: 1637, duration: 0.008s, episode steps:  27, steps per second: 3394, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 70213/100000: episode: 1638, duration: 0.010s, episode steps:  32, steps per second: 3111, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 70291/100000: episode: 1639, duration: 0.023s, episode steps:  78, steps per second: 3384, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 70352/100000: episode: 1640, duration: 0.018s, episode steps:  61, steps per second: 3390, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 70374/100000: episode: 1641, duration: 0.010s, episode steps:  22, steps per second: 2303, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      " 70419/100000: episode: 1642, duration: 0.013s, episode steps:  45, steps per second: 3336, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 70481/100000: episode: 1643, duration: 0.019s, episode steps:  62, steps per second: 3311, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 70537/100000: episode: 1644, duration: 0.020s, episode steps:  56, steps per second: 2769, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 70580/100000: episode: 1645, duration: 0.021s, episode steps:  43, steps per second: 2076, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 70634/100000: episode: 1646, duration: 0.019s, episode steps:  54, steps per second: 2898, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 70646/100000: episode: 1647, duration: 0.004s, episode steps:  12, steps per second: 2939, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 70730/100000: episode: 1648, duration: 0.027s, episode steps:  84, steps per second: 3149, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 70756/100000: episode: 1649, duration: 0.009s, episode steps:  26, steps per second: 3053, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 70776/100000: episode: 1650, duration: 0.008s, episode steps:  20, steps per second: 2659, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 70814/100000: episode: 1651, duration: 0.012s, episode steps:  38, steps per second: 3136, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: 131.000000\n",
      " 70844/100000: episode: 1652, duration: 0.009s, episode steps:  30, steps per second: 3216, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 70873/100000: episode: 1653, duration: 0.009s, episode steps:  29, steps per second: 3331, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 70903/100000: episode: 1654, duration: 0.008s, episode steps:  30, steps per second: 3543, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 70934/100000: episode: 1655, duration: 0.010s, episode steps:  31, steps per second: 3247, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 70965/100000: episode: 1656, duration: 0.010s, episode steps:  31, steps per second: 3248, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 70999/100000: episode: 1657, duration: 0.010s, episode steps:  34, steps per second: 3449, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 71019/100000: episode: 1658, duration: 0.006s, episode steps:  20, steps per second: 3363, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 71045/100000: episode: 1659, duration: 0.007s, episode steps:  26, steps per second: 3480, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 71117/100000: episode: 1660, duration: 0.021s, episode steps:  72, steps per second: 3389, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 71140/100000: episode: 1661, duration: 0.007s, episode steps:  23, steps per second: 3375, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 71195/100000: episode: 1662, duration: 0.015s, episode steps:  55, steps per second: 3620, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 71250/100000: episode: 1663, duration: 0.015s, episode steps:  55, steps per second: 3646, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 71314/100000: episode: 1664, duration: 0.018s, episode steps:  64, steps per second: 3540, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 71327/100000: episode: 1665, duration: 0.004s, episode steps:  13, steps per second: 2920, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 71386/100000: episode: 1666, duration: 0.017s, episode steps:  59, steps per second: 3488, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 71463/100000: episode: 1667, duration: 0.021s, episode steps:  77, steps per second: 3696, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 71549/100000: episode: 1668, duration: 0.025s, episode steps:  86, steps per second: 3450, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 71566/100000: episode: 1669, duration: 0.005s, episode steps:  17, steps per second: 3249, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 71587/100000: episode: 1670, duration: 0.007s, episode steps:  21, steps per second: 3182, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 71626/100000: episode: 1671, duration: 0.011s, episode steps:  39, steps per second: 3481, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 71663/100000: episode: 1672, duration: 0.010s, episode steps:  37, steps per second: 3617, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 71709/100000: episode: 1673, duration: 0.014s, episode steps:  46, steps per second: 3382, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 71732/100000: episode: 1674, duration: 0.007s, episode steps:  23, steps per second: 3526, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 71747/100000: episode: 1675, duration: 0.005s, episode steps:  15, steps per second: 3215, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 71808/100000: episode: 1676, duration: 0.017s, episode steps:  61, steps per second: 3679, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 71931/100000: episode: 1677, duration: 0.034s, episode steps: 123, steps per second: 3668, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  mean_best_reward: --\n",
      " 71971/100000: episode: 1678, duration: 0.012s, episode steps:  40, steps per second: 3283, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 71989/100000: episode: 1679, duration: 0.005s, episode steps:  18, steps per second: 3388, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  mean_best_reward: --\n",
      " 72037/100000: episode: 1680, duration: 0.014s, episode steps:  48, steps per second: 3531, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 72056/100000: episode: 1681, duration: 0.005s, episode steps:  19, steps per second: 3459, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 72071/100000: episode: 1682, duration: 0.005s, episode steps:  15, steps per second: 3178, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 72110/100000: episode: 1683, duration: 0.011s, episode steps:  39, steps per second: 3643, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 72150/100000: episode: 1684, duration: 0.012s, episode steps:  40, steps per second: 3383, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 72203/100000: episode: 1685, duration: 0.016s, episode steps:  53, steps per second: 3407, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 72229/100000: episode: 1686, duration: 0.008s, episode steps:  26, steps per second: 3435, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 72295/100000: episode: 1687, duration: 0.019s, episode steps:  66, steps per second: 3535, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 72366/100000: episode: 1688, duration: 0.021s, episode steps:  71, steps per second: 3333, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 72393/100000: episode: 1689, duration: 0.008s, episode steps:  27, steps per second: 3452, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 72431/100000: episode: 1690, duration: 0.011s, episode steps:  38, steps per second: 3487, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 72457/100000: episode: 1691, duration: 0.008s, episode steps:  26, steps per second: 3344, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 72486/100000: episode: 1692, duration: 0.008s, episode steps:  29, steps per second: 3429, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 72523/100000: episode: 1693, duration: 0.011s, episode steps:  37, steps per second: 3375, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 72580/100000: episode: 1694, duration: 0.016s, episode steps:  57, steps per second: 3548, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  mean_best_reward: --\n",
      " 72685/100000: episode: 1695, duration: 0.029s, episode steps: 105, steps per second: 3584, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 72756/100000: episode: 1696, duration: 0.021s, episode steps:  71, steps per second: 3336, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 72782/100000: episode: 1697, duration: 0.008s, episode steps:  26, steps per second: 3304, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 72802/100000: episode: 1698, duration: 0.006s, episode steps:  20, steps per second: 3240, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 72913/100000: episode: 1699, duration: 0.030s, episode steps: 111, steps per second: 3714, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 72939/100000: episode: 1700, duration: 0.008s, episode steps:  26, steps per second: 3307, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 72957/100000: episode: 1701, duration: 0.005s, episode steps:  18, steps per second: 3297, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  mean_best_reward: 127.500000\n",
      " 73004/100000: episode: 1702, duration: 0.014s, episode steps:  47, steps per second: 3383, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      " 73031/100000: episode: 1703, duration: 0.007s, episode steps:  27, steps per second: 3603, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 73083/100000: episode: 1704, duration: 0.014s, episode steps:  52, steps per second: 3685, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 73122/100000: episode: 1705, duration: 0.011s, episode steps:  39, steps per second: 3472, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 73146/100000: episode: 1706, duration: 0.008s, episode steps:  24, steps per second: 3160, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73193/100000: episode: 1707, duration: 0.013s, episode steps:  47, steps per second: 3561, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 73227/100000: episode: 1708, duration: 0.010s, episode steps:  34, steps per second: 3536, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 73252/100000: episode: 1709, duration: 0.008s, episode steps:  25, steps per second: 3273, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 73308/100000: episode: 1710, duration: 0.016s, episode steps:  56, steps per second: 3417, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73378/100000: episode: 1711, duration: 0.019s, episode steps:  70, steps per second: 3715, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 73489/100000: episode: 1712, duration: 0.030s, episode steps: 111, steps per second: 3674, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 73572/100000: episode: 1713, duration: 0.023s, episode steps:  83, steps per second: 3685, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 73602/100000: episode: 1714, duration: 0.009s, episode steps:  30, steps per second: 3273, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 73633/100000: episode: 1715, duration: 0.009s, episode steps:  31, steps per second: 3439, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 73687/100000: episode: 1716, duration: 0.015s, episode steps:  54, steps per second: 3627, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73721/100000: episode: 1717, duration: 0.010s, episode steps:  34, steps per second: 3427, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73757/100000: episode: 1718, duration: 0.011s, episode steps:  36, steps per second: 3423, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 73776/100000: episode: 1719, duration: 0.006s, episode steps:  19, steps per second: 3044, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 73803/100000: episode: 1720, duration: 0.008s, episode steps:  27, steps per second: 3321, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 73884/100000: episode: 1721, duration: 0.023s, episode steps:  81, steps per second: 3563, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 73932/100000: episode: 1722, duration: 0.014s, episode steps:  48, steps per second: 3445, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 73972/100000: episode: 1723, duration: 0.012s, episode steps:  40, steps per second: 3406, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74037/100000: episode: 1724, duration: 0.021s, episode steps:  65, steps per second: 3030, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  mean_best_reward: --\n",
      " 74093/100000: episode: 1725, duration: 0.017s, episode steps:  56, steps per second: 3267, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 74163/100000: episode: 1726, duration: 0.023s, episode steps:  70, steps per second: 3007, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 74258/100000: episode: 1727, duration: 0.029s, episode steps:  95, steps per second: 3235, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 74284/100000: episode: 1728, duration: 0.008s, episode steps:  26, steps per second: 3363, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 74322/100000: episode: 1729, duration: 0.013s, episode steps:  38, steps per second: 2959, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74388/100000: episode: 1730, duration: 0.019s, episode steps:  66, steps per second: 3491, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 74442/100000: episode: 1731, duration: 0.015s, episode steps:  54, steps per second: 3595, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 74484/100000: episode: 1732, duration: 0.013s, episode steps:  42, steps per second: 3237, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74597/100000: episode: 1733, duration: 0.035s, episode steps: 113, steps per second: 3216, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 74619/100000: episode: 1734, duration: 0.007s, episode steps:  22, steps per second: 3043, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 74634/100000: episode: 1735, duration: 0.005s, episode steps:  15, steps per second: 3161, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 74660/100000: episode: 1736, duration: 0.008s, episode steps:  26, steps per second: 3280, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 74685/100000: episode: 1737, duration: 0.007s, episode steps:  25, steps per second: 3390, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 74717/100000: episode: 1738, duration: 0.011s, episode steps:  32, steps per second: 2992, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74791/100000: episode: 1739, duration: 0.025s, episode steps:  74, steps per second: 2954, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 74809/100000: episode: 1740, duration: 0.006s, episode steps:  18, steps per second: 2774, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74839/100000: episode: 1741, duration: 0.009s, episode steps:  30, steps per second: 3250, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74944/100000: episode: 1742, duration: 0.031s, episode steps: 105, steps per second: 3429, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 74999/100000: episode: 1743, duration: 0.017s, episode steps:  55, steps per second: 3239, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 75069/100000: episode: 1744, duration: 0.023s, episode steps:  70, steps per second: 3089, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 75115/100000: episode: 1745, duration: 0.014s, episode steps:  46, steps per second: 3261, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 75236/100000: episode: 1746, duration: 0.034s, episode steps: 121, steps per second: 3549, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  mean_best_reward: --\n",
      " 75306/100000: episode: 1747, duration: 0.020s, episode steps:  70, steps per second: 3432, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 75369/100000: episode: 1748, duration: 0.018s, episode steps:  63, steps per second: 3581, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 75403/100000: episode: 1749, duration: 0.011s, episode steps:  34, steps per second: 3184, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 75478/100000: episode: 1750, duration: 0.023s, episode steps:  75, steps per second: 3220, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 75495/100000: episode: 1751, duration: 0.006s, episode steps:  17, steps per second: 3067, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: 129.000000\n",
      " 75545/100000: episode: 1752, duration: 0.014s, episode steps:  50, steps per second: 3499, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 75598/100000: episode: 1753, duration: 0.015s, episode steps:  53, steps per second: 3555, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 75667/100000: episode: 1754, duration: 0.023s, episode steps:  69, steps per second: 3065, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 75701/100000: episode: 1755, duration: 0.010s, episode steps:  34, steps per second: 3380, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 75745/100000: episode: 1756, duration: 0.013s, episode steps:  44, steps per second: 3505, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 75761/100000: episode: 1757, duration: 0.005s, episode steps:  16, steps per second: 3298, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 75812/100000: episode: 1758, duration: 0.014s, episode steps:  51, steps per second: 3672, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 75879/100000: episode: 1759, duration: 0.019s, episode steps:  67, steps per second: 3482, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 75903/100000: episode: 1760, duration: 0.007s, episode steps:  24, steps per second: 3416, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 75943/100000: episode: 1761, duration: 0.013s, episode steps:  40, steps per second: 3174, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 75961/100000: episode: 1762, duration: 0.006s, episode steps:  18, steps per second: 3220, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 76005/100000: episode: 1763, duration: 0.014s, episode steps:  44, steps per second: 3194, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 76029/100000: episode: 1764, duration: 0.007s, episode steps:  24, steps per second: 3448, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 76049/100000: episode: 1765, duration: 0.006s, episode steps:  20, steps per second: 3244, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 76109/100000: episode: 1766, duration: 0.017s, episode steps:  60, steps per second: 3549, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 76166/100000: episode: 1767, duration: 0.016s, episode steps:  57, steps per second: 3574, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 76285/100000: episode: 1768, duration: 0.033s, episode steps: 119, steps per second: 3630, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  mean_best_reward: --\n",
      " 76333/100000: episode: 1769, duration: 0.014s, episode steps:  48, steps per second: 3359, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 76357/100000: episode: 1770, duration: 0.008s, episode steps:  24, steps per second: 3149, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 76399/100000: episode: 1771, duration: 0.013s, episode steps:  42, steps per second: 3336, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 76436/100000: episode: 1772, duration: 0.011s, episode steps:  37, steps per second: 3379, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 76471/100000: episode: 1773, duration: 0.010s, episode steps:  35, steps per second: 3455, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 76485/100000: episode: 1774, duration: 0.005s, episode steps:  14, steps per second: 2925, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      " 76531/100000: episode: 1775, duration: 0.013s, episode steps:  46, steps per second: 3446, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 76626/100000: episode: 1776, duration: 0.028s, episode steps:  95, steps per second: 3342, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 76652/100000: episode: 1777, duration: 0.008s, episode steps:  26, steps per second: 3127, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 76678/100000: episode: 1778, duration: 0.007s, episode steps:  26, steps per second: 3532, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 76694/100000: episode: 1779, duration: 0.005s, episode steps:  16, steps per second: 3138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 76708/100000: episode: 1780, duration: 0.005s, episode steps:  14, steps per second: 3001, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 76721/100000: episode: 1781, duration: 0.004s, episode steps:  13, steps per second: 3309, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 76737/100000: episode: 1782, duration: 0.005s, episode steps:  16, steps per second: 3352, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 76769/100000: episode: 1783, duration: 0.010s, episode steps:  32, steps per second: 3201, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 76800/100000: episode: 1784, duration: 0.009s, episode steps:  31, steps per second: 3521, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 76836/100000: episode: 1785, duration: 0.011s, episode steps:  36, steps per second: 3177, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 76872/100000: episode: 1786, duration: 0.010s, episode steps:  36, steps per second: 3556, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 76905/100000: episode: 1787, duration: 0.010s, episode steps:  33, steps per second: 3314, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 76936/100000: episode: 1788, duration: 0.009s, episode steps:  31, steps per second: 3500, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 76985/100000: episode: 1789, duration: 0.015s, episode steps:  49, steps per second: 3362, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 77052/100000: episode: 1790, duration: 0.019s, episode steps:  67, steps per second: 3584, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 77151/100000: episode: 1791, duration: 0.027s, episode steps:  99, steps per second: 3608, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 77201/100000: episode: 1792, duration: 0.015s, episode steps:  50, steps per second: 3363, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77278/100000: episode: 1793, duration: 0.022s, episode steps:  77, steps per second: 3471, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 77328/100000: episode: 1794, duration: 0.014s, episode steps:  50, steps per second: 3454, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 77370/100000: episode: 1795, duration: 0.013s, episode steps:  42, steps per second: 3262, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77403/100000: episode: 1796, duration: 0.010s, episode steps:  33, steps per second: 3218, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 77424/100000: episode: 1797, duration: 0.008s, episode steps:  21, steps per second: 2770, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 77506/100000: episode: 1798, duration: 0.023s, episode steps:  82, steps per second: 3497, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 77571/100000: episode: 1799, duration: 0.019s, episode steps:  65, steps per second: 3335, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 77617/100000: episode: 1800, duration: 0.017s, episode steps:  46, steps per second: 2769, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 77693/100000: episode: 1801, duration: 0.028s, episode steps:  76, steps per second: 2693, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: 112.000000\n",
      " 77778/100000: episode: 1802, duration: 0.026s, episode steps:  85, steps per second: 3328, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 77821/100000: episode: 1803, duration: 0.014s, episode steps:  43, steps per second: 3008, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 77891/100000: episode: 1804, duration: 0.021s, episode steps:  70, steps per second: 3290, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 77903/100000: episode: 1805, duration: 0.004s, episode steps:  12, steps per second: 2683, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      " 77929/100000: episode: 1806, duration: 0.008s, episode steps:  26, steps per second: 3405, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 77948/100000: episode: 1807, duration: 0.006s, episode steps:  19, steps per second: 3194, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 78013/100000: episode: 1808, duration: 0.021s, episode steps:  65, steps per second: 3094, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 78047/100000: episode: 1809, duration: 0.013s, episode steps:  34, steps per second: 2612, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  mean_best_reward: --\n",
      " 78087/100000: episode: 1810, duration: 0.012s, episode steps:  40, steps per second: 3256, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 78139/100000: episode: 1811, duration: 0.018s, episode steps:  52, steps per second: 2959, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 78181/100000: episode: 1812, duration: 0.016s, episode steps:  42, steps per second: 2642, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 78313/100000: episode: 1813, duration: 0.044s, episode steps: 132, steps per second: 2998, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 78358/100000: episode: 1814, duration: 0.015s, episode steps:  45, steps per second: 3096, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 78455/100000: episode: 1815, duration: 0.028s, episode steps:  97, steps per second: 3453, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 78474/100000: episode: 1816, duration: 0.006s, episode steps:  19, steps per second: 3319, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 78483/100000: episode: 1817, duration: 0.003s, episode steps:   9, steps per second: 2600, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  mean_best_reward: --\n",
      " 78510/100000: episode: 1818, duration: 0.010s, episode steps:  27, steps per second: 2795, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 78608/100000: episode: 1819, duration: 0.029s, episode steps:  98, steps per second: 3387, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 78690/100000: episode: 1820, duration: 0.023s, episode steps:  82, steps per second: 3532, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 78741/100000: episode: 1821, duration: 0.014s, episode steps:  51, steps per second: 3542, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 78762/100000: episode: 1822, duration: 0.006s, episode steps:  21, steps per second: 3290, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  mean_best_reward: --\n",
      " 78816/100000: episode: 1823, duration: 0.016s, episode steps:  54, steps per second: 3299, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 78865/100000: episode: 1824, duration: 0.016s, episode steps:  49, steps per second: 3159, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 78990/100000: episode: 1825, duration: 0.035s, episode steps: 125, steps per second: 3589, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  mean_best_reward: --\n",
      " 79037/100000: episode: 1826, duration: 0.014s, episode steps:  47, steps per second: 3472, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 79092/100000: episode: 1827, duration: 0.015s, episode steps:  55, steps per second: 3572, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 79131/100000: episode: 1828, duration: 0.013s, episode steps:  39, steps per second: 2991, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  mean_best_reward: --\n",
      " 79141/100000: episode: 1829, duration: 0.004s, episode steps:  10, steps per second: 2818, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  mean_best_reward: --\n",
      " 79168/100000: episode: 1830, duration: 0.009s, episode steps:  27, steps per second: 3036, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 79220/100000: episode: 1831, duration: 0.016s, episode steps:  52, steps per second: 3231, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 79316/100000: episode: 1832, duration: 0.030s, episode steps:  96, steps per second: 3242, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 79373/100000: episode: 1833, duration: 0.019s, episode steps:  57, steps per second: 2993, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 79446/100000: episode: 1834, duration: 0.021s, episode steps:  73, steps per second: 3561, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  mean_best_reward: --\n",
      " 79474/100000: episode: 1835, duration: 0.008s, episode steps:  28, steps per second: 3312, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 79593/100000: episode: 1836, duration: 0.035s, episode steps: 119, steps per second: 3417, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 79684/100000: episode: 1837, duration: 0.025s, episode steps:  91, steps per second: 3638, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 79721/100000: episode: 1838, duration: 0.011s, episode steps:  37, steps per second: 3339, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  mean_best_reward: --\n",
      " 79849/100000: episode: 1839, duration: 0.037s, episode steps: 128, steps per second: 3419, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 79907/100000: episode: 1840, duration: 0.018s, episode steps:  58, steps per second: 3299, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 79950/100000: episode: 1841, duration: 0.016s, episode steps:  43, steps per second: 2649, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 79983/100000: episode: 1842, duration: 0.010s, episode steps:  33, steps per second: 3299, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 80067/100000: episode: 1843, duration: 0.024s, episode steps:  84, steps per second: 3475, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 80088/100000: episode: 1844, duration: 0.006s, episode steps:  21, steps per second: 3341, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 80168/100000: episode: 1845, duration: 0.025s, episode steps:  80, steps per second: 3200, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 80189/100000: episode: 1846, duration: 0.006s, episode steps:  21, steps per second: 3372, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 80210/100000: episode: 1847, duration: 0.007s, episode steps:  21, steps per second: 3047, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 80291/100000: episode: 1848, duration: 0.023s, episode steps:  81, steps per second: 3464, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 80332/100000: episode: 1849, duration: 0.013s, episode steps:  41, steps per second: 3085, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 80385/100000: episode: 1850, duration: 0.016s, episode steps:  53, steps per second: 3291, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 80425/100000: episode: 1851, duration: 0.011s, episode steps:  40, steps per second: 3496, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: 116.000000\n",
      " 80469/100000: episode: 1852, duration: 0.012s, episode steps:  44, steps per second: 3544, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 80532/100000: episode: 1853, duration: 0.019s, episode steps:  63, steps per second: 3360, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 80573/100000: episode: 1854, duration: 0.012s, episode steps:  41, steps per second: 3492, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 80592/100000: episode: 1855, duration: 0.006s, episode steps:  19, steps per second: 3160, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 80653/100000: episode: 1856, duration: 0.017s, episode steps:  61, steps per second: 3533, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 80715/100000: episode: 1857, duration: 0.019s, episode steps:  62, steps per second: 3323, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 80786/100000: episode: 1858, duration: 0.021s, episode steps:  71, steps per second: 3304, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 80805/100000: episode: 1859, duration: 0.006s, episode steps:  19, steps per second: 3119, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      " 80834/100000: episode: 1860, duration: 0.008s, episode steps:  29, steps per second: 3437, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 80888/100000: episode: 1861, duration: 0.016s, episode steps:  54, steps per second: 3341, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 80922/100000: episode: 1862, duration: 0.010s, episode steps:  34, steps per second: 3468, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 80998/100000: episode: 1863, duration: 0.025s, episode steps:  76, steps per second: 3023, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 81066/100000: episode: 1864, duration: 0.022s, episode steps:  68, steps per second: 3047, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 81091/100000: episode: 1865, duration: 0.010s, episode steps:  25, steps per second: 2489, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 81131/100000: episode: 1866, duration: 0.017s, episode steps:  40, steps per second: 2392, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 81144/100000: episode: 1867, duration: 0.006s, episode steps:  13, steps per second: 2353, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  mean_best_reward: --\n",
      " 81239/100000: episode: 1868, duration: 0.029s, episode steps:  95, steps per second: 3229, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 81289/100000: episode: 1869, duration: 0.015s, episode steps:  50, steps per second: 3417, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 81334/100000: episode: 1870, duration: 0.013s, episode steps:  45, steps per second: 3367, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 81354/100000: episode: 1871, duration: 0.007s, episode steps:  20, steps per second: 2949, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 81409/100000: episode: 1872, duration: 0.016s, episode steps:  55, steps per second: 3338, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 81439/100000: episode: 1873, duration: 0.009s, episode steps:  30, steps per second: 3350, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 81451/100000: episode: 1874, duration: 0.004s, episode steps:  12, steps per second: 3061, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 81478/100000: episode: 1875, duration: 0.010s, episode steps:  27, steps per second: 2807, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  mean_best_reward: --\n",
      " 81511/100000: episode: 1876, duration: 0.010s, episode steps:  33, steps per second: 3407, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 81609/100000: episode: 1877, duration: 0.027s, episode steps:  98, steps per second: 3602, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 81647/100000: episode: 1878, duration: 0.011s, episode steps:  38, steps per second: 3346, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 81774/100000: episode: 1879, duration: 0.040s, episode steps: 127, steps per second: 3176, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 81867/100000: episode: 1880, duration: 0.027s, episode steps:  93, steps per second: 3401, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 81886/100000: episode: 1881, duration: 0.006s, episode steps:  19, steps per second: 3135, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 81979/100000: episode: 1882, duration: 0.029s, episode steps:  93, steps per second: 3184, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 82034/100000: episode: 1883, duration: 0.017s, episode steps:  55, steps per second: 3263, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 82154/100000: episode: 1884, duration: 0.038s, episode steps: 120, steps per second: 3186, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 82174/100000: episode: 1885, duration: 0.007s, episode steps:  20, steps per second: 2949, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 82257/100000: episode: 1886, duration: 0.025s, episode steps:  83, steps per second: 3354, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 82282/100000: episode: 1887, duration: 0.007s, episode steps:  25, steps per second: 3469, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 82389/100000: episode: 1888, duration: 0.030s, episode steps: 107, steps per second: 3566, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 82439/100000: episode: 1889, duration: 0.015s, episode steps:  50, steps per second: 3323, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 82453/100000: episode: 1890, duration: 0.005s, episode steps:  14, steps per second: 3031, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 82485/100000: episode: 1891, duration: 0.010s, episode steps:  32, steps per second: 3330, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 82558/100000: episode: 1892, duration: 0.024s, episode steps:  73, steps per second: 3101, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 82601/100000: episode: 1893, duration: 0.013s, episode steps:  43, steps per second: 3274, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 82654/100000: episode: 1894, duration: 0.017s, episode steps:  53, steps per second: 3134, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 82681/100000: episode: 1895, duration: 0.008s, episode steps:  27, steps per second: 3373, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 82750/100000: episode: 1896, duration: 0.019s, episode steps:  69, steps per second: 3618, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 82781/100000: episode: 1897, duration: 0.010s, episode steps:  31, steps per second: 3235, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 82907/100000: episode: 1898, duration: 0.035s, episode steps: 126, steps per second: 3581, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 82959/100000: episode: 1899, duration: 0.015s, episode steps:  52, steps per second: 3400, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 82999/100000: episode: 1900, duration: 0.011s, episode steps:  40, steps per second: 3545, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 83053/100000: episode: 1901, duration: 0.016s, episode steps:  54, steps per second: 3292, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: 119.500000\n",
      " 83074/100000: episode: 1902, duration: 0.006s, episode steps:  21, steps per second: 3341, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 83159/100000: episode: 1903, duration: 0.023s, episode steps:  85, steps per second: 3628, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 83208/100000: episode: 1904, duration: 0.016s, episode steps:  49, steps per second: 3112, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 83316/100000: episode: 1905, duration: 0.031s, episode steps: 108, steps per second: 3432, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 83348/100000: episode: 1906, duration: 0.011s, episode steps:  32, steps per second: 2926, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 83376/100000: episode: 1907, duration: 0.008s, episode steps:  28, steps per second: 3350, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 83390/100000: episode: 1908, duration: 0.007s, episode steps:  14, steps per second: 2015, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 83527/100000: episode: 1909, duration: 0.040s, episode steps: 137, steps per second: 3395, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 83558/100000: episode: 1910, duration: 0.009s, episode steps:  31, steps per second: 3516, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 83579/100000: episode: 1911, duration: 0.006s, episode steps:  21, steps per second: 3317, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 83634/100000: episode: 1912, duration: 0.019s, episode steps:  55, steps per second: 2952, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 83686/100000: episode: 1913, duration: 0.015s, episode steps:  52, steps per second: 3429, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 83760/100000: episode: 1914, duration: 0.022s, episode steps:  74, steps per second: 3407, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 83777/100000: episode: 1915, duration: 0.007s, episode steps:  17, steps per second: 2549, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 83822/100000: episode: 1916, duration: 0.013s, episode steps:  45, steps per second: 3346, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 83878/100000: episode: 1917, duration: 0.017s, episode steps:  56, steps per second: 3289, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  mean_best_reward: --\n",
      " 83900/100000: episode: 1918, duration: 0.007s, episode steps:  22, steps per second: 3120, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 83968/100000: episode: 1919, duration: 0.021s, episode steps:  68, steps per second: 3211, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 83987/100000: episode: 1920, duration: 0.006s, episode steps:  19, steps per second: 3022, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 84021/100000: episode: 1921, duration: 0.013s, episode steps:  34, steps per second: 2640, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 84094/100000: episode: 1922, duration: 0.020s, episode steps:  73, steps per second: 3639, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  mean_best_reward: --\n",
      " 84128/100000: episode: 1923, duration: 0.012s, episode steps:  34, steps per second: 2913, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 84237/100000: episode: 1924, duration: 0.031s, episode steps: 109, steps per second: 3462, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 84262/100000: episode: 1925, duration: 0.008s, episode steps:  25, steps per second: 3150, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 84276/100000: episode: 1926, duration: 0.006s, episode steps:  14, steps per second: 2464, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 84321/100000: episode: 1927, duration: 0.013s, episode steps:  45, steps per second: 3502, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 84394/100000: episode: 1928, duration: 0.021s, episode steps:  73, steps per second: 3438, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 84464/100000: episode: 1929, duration: 0.027s, episode steps:  70, steps per second: 2582, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 84539/100000: episode: 1930, duration: 0.028s, episode steps:  75, steps per second: 2667, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  mean_best_reward: --\n",
      " 84654/100000: episode: 1931, duration: 0.045s, episode steps: 115, steps per second: 2569, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 84711/100000: episode: 1932, duration: 0.018s, episode steps:  57, steps per second: 3117, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 84743/100000: episode: 1933, duration: 0.010s, episode steps:  32, steps per second: 3183, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 84827/100000: episode: 1934, duration: 0.025s, episode steps:  84, steps per second: 3360, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 84858/100000: episode: 1935, duration: 0.010s, episode steps:  31, steps per second: 2986, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 84943/100000: episode: 1936, duration: 0.027s, episode steps:  85, steps per second: 3176, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 84989/100000: episode: 1937, duration: 0.013s, episode steps:  46, steps per second: 3423, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 85070/100000: episode: 1938, duration: 0.026s, episode steps:  81, steps per second: 3111, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 85122/100000: episode: 1939, duration: 0.017s, episode steps:  52, steps per second: 3013, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 85204/100000: episode: 1940, duration: 0.025s, episode steps:  82, steps per second: 3233, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 85275/100000: episode: 1941, duration: 0.021s, episode steps:  71, steps per second: 3421, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 85340/100000: episode: 1942, duration: 0.019s, episode steps:  65, steps per second: 3343, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 85388/100000: episode: 1943, duration: 0.018s, episode steps:  48, steps per second: 2608, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 85490/100000: episode: 1944, duration: 0.030s, episode steps: 102, steps per second: 3345, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 85501/100000: episode: 1945, duration: 0.004s, episode steps:  11, steps per second: 2722, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      " 85536/100000: episode: 1946, duration: 0.010s, episode steps:  35, steps per second: 3378, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 85564/100000: episode: 1947, duration: 0.009s, episode steps:  28, steps per second: 3264, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 85611/100000: episode: 1948, duration: 0.014s, episode steps:  47, steps per second: 3441, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 85629/100000: episode: 1949, duration: 0.006s, episode steps:  18, steps per second: 2960, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 85723/100000: episode: 1950, duration: 0.025s, episode steps:  94, steps per second: 3747, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 85873/100000: episode: 1951, duration: 0.043s, episode steps: 150, steps per second: 3459, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: 108.500000\n",
      " 85915/100000: episode: 1952, duration: 0.012s, episode steps:  42, steps per second: 3453, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 85947/100000: episode: 1953, duration: 0.013s, episode steps:  32, steps per second: 2458, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 85981/100000: episode: 1954, duration: 0.010s, episode steps:  34, steps per second: 3425, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 86013/100000: episode: 1955, duration: 0.010s, episode steps:  32, steps per second: 3324, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 86029/100000: episode: 1956, duration: 0.005s, episode steps:  16, steps per second: 3076, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 86151/100000: episode: 1957, duration: 0.038s, episode steps: 122, steps per second: 3219, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 86262/100000: episode: 1958, duration: 0.044s, episode steps: 111, steps per second: 2503, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 86281/100000: episode: 1959, duration: 0.006s, episode steps:  19, steps per second: 2955, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 86303/100000: episode: 1960, duration: 0.007s, episode steps:  22, steps per second: 3142, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      " 86330/100000: episode: 1961, duration: 0.008s, episode steps:  27, steps per second: 3200, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 86388/100000: episode: 1962, duration: 0.017s, episode steps:  58, steps per second: 3428, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  mean_best_reward: --\n",
      " 86424/100000: episode: 1963, duration: 0.010s, episode steps:  36, steps per second: 3522, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 86470/100000: episode: 1964, duration: 0.014s, episode steps:  46, steps per second: 3391, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 86491/100000: episode: 1965, duration: 0.006s, episode steps:  21, steps per second: 3254, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 86532/100000: episode: 1966, duration: 0.012s, episode steps:  41, steps per second: 3425, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 86577/100000: episode: 1967, duration: 0.014s, episode steps:  45, steps per second: 3239, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 86636/100000: episode: 1968, duration: 0.016s, episode steps:  59, steps per second: 3654, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 86652/100000: episode: 1969, duration: 0.005s, episode steps:  16, steps per second: 3240, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  mean_best_reward: --\n",
      " 86745/100000: episode: 1970, duration: 0.025s, episode steps:  93, steps per second: 3732, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 86765/100000: episode: 1971, duration: 0.007s, episode steps:  20, steps per second: 2991, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 86825/100000: episode: 1972, duration: 0.017s, episode steps:  60, steps per second: 3515, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 86862/100000: episode: 1973, duration: 0.010s, episode steps:  37, steps per second: 3553, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 86904/100000: episode: 1974, duration: 0.012s, episode steps:  42, steps per second: 3368, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 86953/100000: episode: 1975, duration: 0.015s, episode steps:  49, steps per second: 3244, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 86992/100000: episode: 1976, duration: 0.012s, episode steps:  39, steps per second: 3353, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 87012/100000: episode: 1977, duration: 0.006s, episode steps:  20, steps per second: 3152, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 87050/100000: episode: 1978, duration: 0.011s, episode steps:  38, steps per second: 3392, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 87085/100000: episode: 1979, duration: 0.010s, episode steps:  35, steps per second: 3481, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 87133/100000: episode: 1980, duration: 0.015s, episode steps:  48, steps per second: 3270, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 87158/100000: episode: 1981, duration: 0.008s, episode steps:  25, steps per second: 3190, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 87181/100000: episode: 1982, duration: 0.007s, episode steps:  23, steps per second: 3470, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 87204/100000: episode: 1983, duration: 0.007s, episode steps:  23, steps per second: 3261, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 87284/100000: episode: 1984, duration: 0.023s, episode steps:  80, steps per second: 3491, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 87319/100000: episode: 1985, duration: 0.011s, episode steps:  35, steps per second: 3189, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 87407/100000: episode: 1986, duration: 0.027s, episode steps:  88, steps per second: 3230, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  mean_best_reward: --\n",
      " 87492/100000: episode: 1987, duration: 0.024s, episode steps:  85, steps per second: 3565, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 87530/100000: episode: 1988, duration: 0.012s, episode steps:  38, steps per second: 3275, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 87591/100000: episode: 1989, duration: 0.019s, episode steps:  61, steps per second: 3247, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 87609/100000: episode: 1990, duration: 0.005s, episode steps:  18, steps per second: 3425, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 87626/100000: episode: 1991, duration: 0.006s, episode steps:  17, steps per second: 3036, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 87668/100000: episode: 1992, duration: 0.012s, episode steps:  42, steps per second: 3518, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 87703/100000: episode: 1993, duration: 0.013s, episode steps:  35, steps per second: 2688, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 87803/100000: episode: 1994, duration: 0.030s, episode steps: 100, steps per second: 3314, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 87830/100000: episode: 1995, duration: 0.008s, episode steps:  27, steps per second: 3264, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 87883/100000: episode: 1996, duration: 0.019s, episode steps:  53, steps per second: 2764, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 87914/100000: episode: 1997, duration: 0.010s, episode steps:  31, steps per second: 3041, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 87932/100000: episode: 1998, duration: 0.010s, episode steps:  18, steps per second: 1829, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  mean_best_reward: --\n",
      " 87941/100000: episode: 1999, duration: 0.004s, episode steps:   9, steps per second: 2184, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  mean_best_reward: --\n",
      " 88006/100000: episode: 2000, duration: 0.021s, episode steps:  65, steps per second: 3153, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 88035/100000: episode: 2001, duration: 0.011s, episode steps:  29, steps per second: 2746, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  mean_best_reward: 121.500000\n",
      " 88060/100000: episode: 2002, duration: 0.009s, episode steps:  25, steps per second: 2886, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 88081/100000: episode: 2003, duration: 0.006s, episode steps:  21, steps per second: 3270, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 88109/100000: episode: 2004, duration: 0.008s, episode steps:  28, steps per second: 3375, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 88134/100000: episode: 2005, duration: 0.009s, episode steps:  25, steps per second: 2684, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 88164/100000: episode: 2006, duration: 0.010s, episode steps:  30, steps per second: 3121, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 88256/100000: episode: 2007, duration: 0.028s, episode steps:  92, steps per second: 3341, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 88286/100000: episode: 2008, duration: 0.009s, episode steps:  30, steps per second: 3363, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  mean_best_reward: --\n",
      " 88305/100000: episode: 2009, duration: 0.006s, episode steps:  19, steps per second: 3118, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 88357/100000: episode: 2010, duration: 0.016s, episode steps:  52, steps per second: 3204, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 88399/100000: episode: 2011, duration: 0.013s, episode steps:  42, steps per second: 3181, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 88421/100000: episode: 2012, duration: 0.007s, episode steps:  22, steps per second: 3132, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 88438/100000: episode: 2013, duration: 0.005s, episode steps:  17, steps per second: 3187, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 88474/100000: episode: 2014, duration: 0.012s, episode steps:  36, steps per second: 3095, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 88487/100000: episode: 2015, duration: 0.004s, episode steps:  13, steps per second: 3167, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 88601/100000: episode: 2016, duration: 0.036s, episode steps: 114, steps per second: 3191, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 88669/100000: episode: 2017, duration: 0.020s, episode steps:  68, steps per second: 3344, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 88715/100000: episode: 2018, duration: 0.013s, episode steps:  46, steps per second: 3543, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 88809/100000: episode: 2019, duration: 0.028s, episode steps:  94, steps per second: 3406, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 88843/100000: episode: 2020, duration: 0.011s, episode steps:  34, steps per second: 3208, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 88890/100000: episode: 2021, duration: 0.014s, episode steps:  47, steps per second: 3252, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 89030/100000: episode: 2022, duration: 0.040s, episode steps: 140, steps per second: 3530, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 89039/100000: episode: 2023, duration: 0.003s, episode steps:   9, steps per second: 2924, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  mean_best_reward: --\n",
      " 89075/100000: episode: 2024, duration: 0.013s, episode steps:  36, steps per second: 2799, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 89108/100000: episode: 2025, duration: 0.011s, episode steps:  33, steps per second: 3115, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 89153/100000: episode: 2026, duration: 0.013s, episode steps:  45, steps per second: 3503, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 89203/100000: episode: 2027, duration: 0.014s, episode steps:  50, steps per second: 3494, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 89315/100000: episode: 2028, duration: 0.032s, episode steps: 112, steps per second: 3461, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 89362/100000: episode: 2029, duration: 0.015s, episode steps:  47, steps per second: 3213, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 89422/100000: episode: 2030, duration: 0.019s, episode steps:  60, steps per second: 3078, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 89442/100000: episode: 2031, duration: 0.006s, episode steps:  20, steps per second: 3222, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 89457/100000: episode: 2032, duration: 0.005s, episode steps:  15, steps per second: 3081, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 89470/100000: episode: 2033, duration: 0.004s, episode steps:  13, steps per second: 2936, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 89509/100000: episode: 2034, duration: 0.012s, episode steps:  39, steps per second: 3369, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 89571/100000: episode: 2035, duration: 0.023s, episode steps:  62, steps per second: 2742, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 89601/100000: episode: 2036, duration: 0.009s, episode steps:  30, steps per second: 3251, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 89696/100000: episode: 2037, duration: 0.028s, episode steps:  95, steps per second: 3444, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 89733/100000: episode: 2038, duration: 0.012s, episode steps:  37, steps per second: 3213, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 89807/100000: episode: 2039, duration: 0.023s, episode steps:  74, steps per second: 3264, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 89833/100000: episode: 2040, duration: 0.009s, episode steps:  26, steps per second: 2862, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 89850/100000: episode: 2041, duration: 0.005s, episode steps:  17, steps per second: 3303, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 89903/100000: episode: 2042, duration: 0.015s, episode steps:  53, steps per second: 3422, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 90000/100000: episode: 2043, duration: 0.028s, episode steps:  97, steps per second: 3505, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 90022/100000: episode: 2044, duration: 0.007s, episode steps:  22, steps per second: 3003, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 90056/100000: episode: 2045, duration: 0.010s, episode steps:  34, steps per second: 3305, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 90129/100000: episode: 2046, duration: 0.020s, episode steps:  73, steps per second: 3565, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 90272/100000: episode: 2047, duration: 0.043s, episode steps: 143, steps per second: 3359, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  mean_best_reward: --\n",
      " 90310/100000: episode: 2048, duration: 0.012s, episode steps:  38, steps per second: 3227, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 90352/100000: episode: 2049, duration: 0.016s, episode steps:  42, steps per second: 2637, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 90374/100000: episode: 2050, duration: 0.007s, episode steps:  22, steps per second: 3091, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 90392/100000: episode: 2051, duration: 0.006s, episode steps:  18, steps per second: 2996, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 119.500000\n",
      " 90440/100000: episode: 2052, duration: 0.015s, episode steps:  48, steps per second: 3215, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 90599/100000: episode: 2053, duration: 0.047s, episode steps: 159, steps per second: 3363, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 90628/100000: episode: 2054, duration: 0.009s, episode steps:  29, steps per second: 3290, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 90665/100000: episode: 2055, duration: 0.011s, episode steps:  37, steps per second: 3496, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 90761/100000: episode: 2056, duration: 0.028s, episode steps:  96, steps per second: 3486, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 90826/100000: episode: 2057, duration: 0.019s, episode steps:  65, steps per second: 3484, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  mean_best_reward: --\n",
      " 90875/100000: episode: 2058, duration: 0.014s, episode steps:  49, steps per second: 3426, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 90942/100000: episode: 2059, duration: 0.020s, episode steps:  67, steps per second: 3382, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 90958/100000: episode: 2060, duration: 0.005s, episode steps:  16, steps per second: 3296, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 91036/100000: episode: 2061, duration: 0.023s, episode steps:  78, steps per second: 3417, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 91069/100000: episode: 2062, duration: 0.010s, episode steps:  33, steps per second: 3287, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 91147/100000: episode: 2063, duration: 0.024s, episode steps:  78, steps per second: 3253, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 91206/100000: episode: 2064, duration: 0.021s, episode steps:  59, steps per second: 2858, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 91346/100000: episode: 2065, duration: 0.051s, episode steps: 140, steps per second: 2752, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 91407/100000: episode: 2066, duration: 0.022s, episode steps:  61, steps per second: 2791, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 91421/100000: episode: 2067, duration: 0.005s, episode steps:  14, steps per second: 3011, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 91441/100000: episode: 2068, duration: 0.007s, episode steps:  20, steps per second: 2734, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 91534/100000: episode: 2069, duration: 0.033s, episode steps:  93, steps per second: 2860, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 91601/100000: episode: 2070, duration: 0.021s, episode steps:  67, steps per second: 3197, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 91626/100000: episode: 2071, duration: 0.007s, episode steps:  25, steps per second: 3523, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 91704/100000: episode: 2072, duration: 0.026s, episode steps:  78, steps per second: 2991, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  mean_best_reward: --\n",
      " 91749/100000: episode: 2073, duration: 0.014s, episode steps:  45, steps per second: 3286, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 91777/100000: episode: 2074, duration: 0.009s, episode steps:  28, steps per second: 3210, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  mean_best_reward: --\n",
      " 91818/100000: episode: 2075, duration: 0.012s, episode steps:  41, steps per second: 3294, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 91834/100000: episode: 2076, duration: 0.005s, episode steps:  16, steps per second: 3120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 91888/100000: episode: 2077, duration: 0.019s, episode steps:  54, steps per second: 2805, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 91951/100000: episode: 2078, duration: 0.019s, episode steps:  63, steps per second: 3311, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 92012/100000: episode: 2079, duration: 0.020s, episode steps:  61, steps per second: 3049, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 92036/100000: episode: 2080, duration: 0.011s, episode steps:  24, steps per second: 2210, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 92074/100000: episode: 2081, duration: 0.012s, episode steps:  38, steps per second: 3173, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 92103/100000: episode: 2082, duration: 0.009s, episode steps:  29, steps per second: 3084, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 92152/100000: episode: 2083, duration: 0.015s, episode steps:  49, steps per second: 3190, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  mean_best_reward: --\n",
      " 92221/100000: episode: 2084, duration: 0.020s, episode steps:  69, steps per second: 3480, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 92253/100000: episode: 2085, duration: 0.009s, episode steps:  32, steps per second: 3391, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 92271/100000: episode: 2086, duration: 0.006s, episode steps:  18, steps per second: 3163, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 92387/100000: episode: 2087, duration: 0.031s, episode steps: 116, steps per second: 3695, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 92412/100000: episode: 2088, duration: 0.008s, episode steps:  25, steps per second: 3255, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 92459/100000: episode: 2089, duration: 0.014s, episode steps:  47, steps per second: 3404, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 92487/100000: episode: 2090, duration: 0.008s, episode steps:  28, steps per second: 3568, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 92579/100000: episode: 2091, duration: 0.025s, episode steps:  92, steps per second: 3648, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 92650/100000: episode: 2092, duration: 0.021s, episode steps:  71, steps per second: 3398, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 92663/100000: episode: 2093, duration: 0.004s, episode steps:  13, steps per second: 3162, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 92689/100000: episode: 2094, duration: 0.008s, episode steps:  26, steps per second: 3254, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 92746/100000: episode: 2095, duration: 0.016s, episode steps:  57, steps per second: 3572, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 92785/100000: episode: 2096, duration: 0.012s, episode steps:  39, steps per second: 3201, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 92813/100000: episode: 2097, duration: 0.009s, episode steps:  28, steps per second: 3077, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 92877/100000: episode: 2098, duration: 0.019s, episode steps:  64, steps per second: 3446, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 92912/100000: episode: 2099, duration: 0.013s, episode steps:  35, steps per second: 2746, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 92998/100000: episode: 2100, duration: 0.026s, episode steps:  86, steps per second: 3322, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 93012/100000: episode: 2101, duration: 0.005s, episode steps:  14, steps per second: 2808, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: 141.000000\n",
      " 93084/100000: episode: 2102, duration: 0.020s, episode steps:  72, steps per second: 3644, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93132/100000: episode: 2103, duration: 0.014s, episode steps:  48, steps per second: 3448, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 93160/100000: episode: 2104, duration: 0.009s, episode steps:  28, steps per second: 3100, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 93202/100000: episode: 2105, duration: 0.012s, episode steps:  42, steps per second: 3633, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 93296/100000: episode: 2106, duration: 0.026s, episode steps:  94, steps per second: 3572, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 93324/100000: episode: 2107, duration: 0.009s, episode steps:  28, steps per second: 3279, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 93365/100000: episode: 2108, duration: 0.013s, episode steps:  41, steps per second: 3214, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 93398/100000: episode: 2109, duration: 0.010s, episode steps:  33, steps per second: 3198, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 93420/100000: episode: 2110, duration: 0.006s, episode steps:  22, steps per second: 3470, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 93463/100000: episode: 2111, duration: 0.014s, episode steps:  43, steps per second: 3112, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 93501/100000: episode: 2112, duration: 0.011s, episode steps:  38, steps per second: 3336, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93555/100000: episode: 2113, duration: 0.015s, episode steps:  54, steps per second: 3485, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93611/100000: episode: 2114, duration: 0.017s, episode steps:  56, steps per second: 3239, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 93646/100000: episode: 2115, duration: 0.012s, episode steps:  35, steps per second: 2973, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 93661/100000: episode: 2116, duration: 0.005s, episode steps:  15, steps per second: 3145, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 93678/100000: episode: 2117, duration: 0.006s, episode steps:  17, steps per second: 3070, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  mean_best_reward: --\n",
      " 93696/100000: episode: 2118, duration: 0.006s, episode steps:  18, steps per second: 3046, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 93733/100000: episode: 2119, duration: 0.011s, episode steps:  37, steps per second: 3338, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 93766/100000: episode: 2120, duration: 0.010s, episode steps:  33, steps per second: 3334, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 93803/100000: episode: 2121, duration: 0.013s, episode steps:  37, steps per second: 2928, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 93826/100000: episode: 2122, duration: 0.010s, episode steps:  23, steps per second: 2343, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 93880/100000: episode: 2123, duration: 0.016s, episode steps:  54, steps per second: 3430, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93964/100000: episode: 2124, duration: 0.025s, episode steps:  84, steps per second: 3404, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 94006/100000: episode: 2125, duration: 0.012s, episode steps:  42, steps per second: 3431, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 94027/100000: episode: 2126, duration: 0.007s, episode steps:  21, steps per second: 2861, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 94040/100000: episode: 2127, duration: 0.004s, episode steps:  13, steps per second: 3040, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 94140/100000: episode: 2128, duration: 0.027s, episode steps: 100, steps per second: 3699, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 94172/100000: episode: 2129, duration: 0.010s, episode steps:  32, steps per second: 3340, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 94202/100000: episode: 2130, duration: 0.009s, episode steps:  30, steps per second: 3305, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 94229/100000: episode: 2131, duration: 0.008s, episode steps:  27, steps per second: 3336, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 94315/100000: episode: 2132, duration: 0.024s, episode steps:  86, steps per second: 3586, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 94377/100000: episode: 2133, duration: 0.017s, episode steps:  62, steps per second: 3576, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 94414/100000: episode: 2134, duration: 0.011s, episode steps:  37, steps per second: 3336, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 94449/100000: episode: 2135, duration: 0.012s, episode steps:  35, steps per second: 2837, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 94498/100000: episode: 2136, duration: 0.014s, episode steps:  49, steps per second: 3495, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 94521/100000: episode: 2137, duration: 0.010s, episode steps:  23, steps per second: 2381, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 94640/100000: episode: 2138, duration: 0.042s, episode steps: 119, steps per second: 2846, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 94728/100000: episode: 2139, duration: 0.031s, episode steps:  88, steps per second: 2796, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 94786/100000: episode: 2140, duration: 0.020s, episode steps:  58, steps per second: 2846, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 94831/100000: episode: 2141, duration: 0.014s, episode steps:  45, steps per second: 3199, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 94875/100000: episode: 2142, duration: 0.014s, episode steps:  44, steps per second: 3167, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 94920/100000: episode: 2143, duration: 0.014s, episode steps:  45, steps per second: 3138, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 95036/100000: episode: 2144, duration: 0.035s, episode steps: 116, steps per second: 3340, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 95109/100000: episode: 2145, duration: 0.021s, episode steps:  73, steps per second: 3552, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 95142/100000: episode: 2146, duration: 0.011s, episode steps:  33, steps per second: 2992, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 95180/100000: episode: 2147, duration: 0.011s, episode steps:  38, steps per second: 3562, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 95217/100000: episode: 2148, duration: 0.012s, episode steps:  37, steps per second: 3105, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 95265/100000: episode: 2149, duration: 0.014s, episode steps:  48, steps per second: 3372, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 95290/100000: episode: 2150, duration: 0.007s, episode steps:  25, steps per second: 3475, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 95449/100000: episode: 2151, duration: 0.046s, episode steps: 159, steps per second: 3458, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  mean_best_reward: 113.000000\n",
      " 95497/100000: episode: 2152, duration: 0.014s, episode steps:  48, steps per second: 3432, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 95530/100000: episode: 2153, duration: 0.009s, episode steps:  33, steps per second: 3548, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 95580/100000: episode: 2154, duration: 0.015s, episode steps:  50, steps per second: 3392, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 95657/100000: episode: 2155, duration: 0.024s, episode steps:  77, steps per second: 3181, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 95673/100000: episode: 2156, duration: 0.006s, episode steps:  16, steps per second: 2584, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 95754/100000: episode: 2157, duration: 0.022s, episode steps:  81, steps per second: 3612, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 95802/100000: episode: 2158, duration: 0.015s, episode steps:  48, steps per second: 3134, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 95826/100000: episode: 2159, duration: 0.007s, episode steps:  24, steps per second: 3209, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 95850/100000: episode: 2160, duration: 0.008s, episode steps:  24, steps per second: 3053, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 95887/100000: episode: 2161, duration: 0.011s, episode steps:  37, steps per second: 3389, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 95959/100000: episode: 2162, duration: 0.021s, episode steps:  72, steps per second: 3498, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 95975/100000: episode: 2163, duration: 0.005s, episode steps:  16, steps per second: 2940, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 96018/100000: episode: 2164, duration: 0.012s, episode steps:  43, steps per second: 3476, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 96057/100000: episode: 2165, duration: 0.011s, episode steps:  39, steps per second: 3542, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 96121/100000: episode: 2166, duration: 0.018s, episode steps:  64, steps per second: 3547, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96161/100000: episode: 2167, duration: 0.011s, episode steps:  40, steps per second: 3483, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96263/100000: episode: 2168, duration: 0.029s, episode steps: 102, steps per second: 3502, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  mean_best_reward: --\n",
      " 96334/100000: episode: 2169, duration: 0.021s, episode steps:  71, steps per second: 3373, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 96365/100000: episode: 2170, duration: 0.009s, episode steps:  31, steps per second: 3368, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 96402/100000: episode: 2171, duration: 0.012s, episode steps:  37, steps per second: 3011, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 96456/100000: episode: 2172, duration: 0.016s, episode steps:  54, steps per second: 3368, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 96477/100000: episode: 2173, duration: 0.007s, episode steps:  21, steps per second: 3062, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 96505/100000: episode: 2174, duration: 0.008s, episode steps:  28, steps per second: 3379, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96539/100000: episode: 2175, duration: 0.010s, episode steps:  34, steps per second: 3530, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 96553/100000: episode: 2176, duration: 0.004s, episode steps:  14, steps per second: 3235, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96579/100000: episode: 2177, duration: 0.008s, episode steps:  26, steps per second: 3409, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 96643/100000: episode: 2178, duration: 0.021s, episode steps:  64, steps per second: 3100, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 96676/100000: episode: 2179, duration: 0.011s, episode steps:  33, steps per second: 3072, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 96709/100000: episode: 2180, duration: 0.013s, episode steps:  33, steps per second: 2476, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 96854/100000: episode: 2181, duration: 0.040s, episode steps: 145, steps per second: 3588, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 96895/100000: episode: 2182, duration: 0.012s, episode steps:  41, steps per second: 3484, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 96915/100000: episode: 2183, duration: 0.007s, episode steps:  20, steps per second: 2836, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 96954/100000: episode: 2184, duration: 0.012s, episode steps:  39, steps per second: 3174, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 97016/100000: episode: 2185, duration: 0.019s, episode steps:  62, steps per second: 3183, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 97065/100000: episode: 2186, duration: 0.016s, episode steps:  49, steps per second: 3092, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 97252/100000: episode: 2187, duration: 0.052s, episode steps: 187, steps per second: 3590, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  mean_best_reward: --\n",
      " 97301/100000: episode: 2188, duration: 0.014s, episode steps:  49, steps per second: 3387, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 97401/100000: episode: 2189, duration: 0.028s, episode steps: 100, steps per second: 3557, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  mean_best_reward: --\n",
      " 97434/100000: episode: 2190, duration: 0.011s, episode steps:  33, steps per second: 3022, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 97472/100000: episode: 2191, duration: 0.013s, episode steps:  38, steps per second: 2951, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 97509/100000: episode: 2192, duration: 0.011s, episode steps:  37, steps per second: 3423, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 97546/100000: episode: 2193, duration: 0.011s, episode steps:  37, steps per second: 3406, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 97572/100000: episode: 2194, duration: 0.008s, episode steps:  26, steps per second: 3292, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 97632/100000: episode: 2195, duration: 0.018s, episode steps:  60, steps per second: 3276, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 97678/100000: episode: 2196, duration: 0.014s, episode steps:  46, steps per second: 3233, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 97728/100000: episode: 2197, duration: 0.016s, episode steps:  50, steps per second: 3055, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 97748/100000: episode: 2198, duration: 0.006s, episode steps:  20, steps per second: 3315, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 97878/100000: episode: 2199, duration: 0.038s, episode steps: 130, steps per second: 3437, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 97913/100000: episode: 2200, duration: 0.010s, episode steps:  35, steps per second: 3495, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 98002/100000: episode: 2201, duration: 0.029s, episode steps:  89, steps per second: 3108, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  mean_best_reward: 155.500000\n",
      " 98060/100000: episode: 2202, duration: 0.018s, episode steps:  58, steps per second: 3309, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 98087/100000: episode: 2203, duration: 0.010s, episode steps:  27, steps per second: 2767, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 98147/100000: episode: 2204, duration: 0.021s, episode steps:  60, steps per second: 2906, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 98177/100000: episode: 2205, duration: 0.013s, episode steps:  30, steps per second: 2350, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 98240/100000: episode: 2206, duration: 0.021s, episode steps:  63, steps per second: 2979, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 98274/100000: episode: 2207, duration: 0.010s, episode steps:  34, steps per second: 3242, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 98303/100000: episode: 2208, duration: 0.009s, episode steps:  29, steps per second: 3381, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 98333/100000: episode: 2209, duration: 0.011s, episode steps:  30, steps per second: 2643, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 98388/100000: episode: 2210, duration: 0.016s, episode steps:  55, steps per second: 3408, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 98516/100000: episode: 2211, duration: 0.034s, episode steps: 128, steps per second: 3724, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 98600/100000: episode: 2212, duration: 0.027s, episode steps:  84, steps per second: 3136, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 98625/100000: episode: 2213, duration: 0.008s, episode steps:  25, steps per second: 3127, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 98644/100000: episode: 2214, duration: 0.006s, episode steps:  19, steps per second: 3043, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 98743/100000: episode: 2215, duration: 0.027s, episode steps:  99, steps per second: 3610, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 98787/100000: episode: 2216, duration: 0.014s, episode steps:  44, steps per second: 3125, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 98833/100000: episode: 2217, duration: 0.016s, episode steps:  46, steps per second: 2925, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 98889/100000: episode: 2218, duration: 0.017s, episode steps:  56, steps per second: 3369, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 98975/100000: episode: 2219, duration: 0.026s, episode steps:  86, steps per second: 3323, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 99004/100000: episode: 2220, duration: 0.009s, episode steps:  29, steps per second: 3070, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 99046/100000: episode: 2221, duration: 0.012s, episode steps:  42, steps per second: 3396, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 99082/100000: episode: 2222, duration: 0.011s, episode steps:  36, steps per second: 3210, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 99148/100000: episode: 2223, duration: 0.022s, episode steps:  66, steps per second: 2974, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 99163/100000: episode: 2224, duration: 0.005s, episode steps:  15, steps per second: 2785, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 99216/100000: episode: 2225, duration: 0.015s, episode steps:  53, steps per second: 3441, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 99234/100000: episode: 2226, duration: 0.006s, episode steps:  18, steps per second: 3268, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  mean_best_reward: --\n",
      " 99282/100000: episode: 2227, duration: 0.015s, episode steps:  48, steps per second: 3301, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 99358/100000: episode: 2228, duration: 0.023s, episode steps:  76, steps per second: 3377, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 99403/100000: episode: 2229, duration: 0.014s, episode steps:  45, steps per second: 3263, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 99418/100000: episode: 2230, duration: 0.006s, episode steps:  15, steps per second: 2668, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  mean_best_reward: --\n",
      " 99460/100000: episode: 2231, duration: 0.014s, episode steps:  42, steps per second: 3010, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 99487/100000: episode: 2232, duration: 0.008s, episode steps:  27, steps per second: 3419, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 99510/100000: episode: 2233, duration: 0.007s, episode steps:  23, steps per second: 3295, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 99555/100000: episode: 2234, duration: 0.014s, episode steps:  45, steps per second: 3196, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 99579/100000: episode: 2235, duration: 0.008s, episode steps:  24, steps per second: 3167, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 99604/100000: episode: 2236, duration: 0.008s, episode steps:  25, steps per second: 3323, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 99689/100000: episode: 2237, duration: 0.028s, episode steps:  85, steps per second: 3073, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 99729/100000: episode: 2238, duration: 0.016s, episode steps:  40, steps per second: 2562, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 99763/100000: episode: 2239, duration: 0.011s, episode steps:  34, steps per second: 3051, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 99778/100000: episode: 2240, duration: 0.005s, episode steps:  15, steps per second: 3172, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 99838/100000: episode: 2241, duration: 0.017s, episode steps:  60, steps per second: 3468, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 99939/100000: episode: 2242, duration: 0.028s, episode steps: 101, steps per second: 3559, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 99992/100000: episode: 2243, duration: 0.017s, episode steps:  53, steps per second: 3037, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      "done, took 31.406 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 70.000, steps: 70\n",
      "Episode 3: reward: 119.000, steps: 119\n",
      "Episode 4: reward: 145.000, steps: 145\n",
      "Episode 5: reward: 82.000, steps: 82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d0d0486c48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Simple Neural Network\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Compile the Agent\n",
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "# Visualize\n",
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "# Save best weights\n",
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)\n",
    "# Evaluate our algorithm for 5 episodes.\n",
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full explanation of this example go to: [wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari Paper (Playing Atari with Deep Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence DeepMind used a CNN trained on a variant of Q-learning to create a Deep RL model which palyed seven (Beam Rider, Breakout, Enduro, Pong, Qbert, Seaquest, and Space Invaders) Atari 2600 games (with no architecture adjustments) and outpreformed human experts in three of the seven games (Breakout, Enduro, and Pong).\n",
    "\n",
    "The agent, a CNN, directly takes the image (pixels) from an Atari game which then given the action/state move it assigns values to each of the 18 possible joystick actions. The model outputs the agent expectation of the future reward it will get if it preforms a given action given a state. Then, the RL model can pick the action with the highest reward based on the enviorment (the Atari emulator)\n",
    "\n",
    "A big challenge here, is that in Deep Leearning often the input samples are assumed to be unrelated to each other. This is not the case for RL, a simple example would be chess where it might take several steps to knock an opponent's king, and each the early moves do not return an immediate reward as the final move does, even when one of those moves might be crucial to win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Atari](Atari.jpg \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome these issues DeepMind kept a record of all experiences, then used randomly distributed batches of the saved experiences to train. This makes the training data samples more random en pseudo-uncorrelated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backgound "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple frames that will be almost exactly the same while playing the game (the player might remain in the same position for seconds) meaning that the same action is continued into different frames -- these make a sequence of several frames with same action between them with an individual state. This gives rise to a large but finite MDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD Gammon was the first architecture that was used to play Atari in 19 it used an on-policy appoach where it trained a perceptron using direct samples of experiences drawn from the algorithm's interaction with the enviroment. This algorithm can often get stuck in  local minimums given biased samples. Instead of using this approach this papers explores the use of Experience Replay, where the experiences are stored for each time step, pooled over many *memories*, and via Q-learning updates drawn at random. After performing the Experience Replay algorith the agent selects and executes an action according to a greedy policy. Experience Replay reduces the bias on the samples by taking an average over many of its previous states -- making the algorithm an off-policy algorithm because the current parameters are different to those to generate the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Deep-Q-Networks (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Atari](architecture.jpg \"Ar\")\n",
    "![Atari](layers.jpg \"L\")\n",
    "\n",
    "The output corresponds to the predicted Q-values of the individual action fot the input state, the main advatange of this architecture is the ability to compute Q-values for all possible actions in a given state with a single forward pass through the network.\n",
    "\n",
    "Network:\n",
    "1. Input 84x84x4 image\n",
    "2. Hidden Layer: Convolution 16 8x8 filters with stride of 4 followed by ReLu\n",
    "3. Second Hidden Layer: Convolution 32 4x4 filters with stride of 2 followed by ReLu\n",
    "4. Final Hidden Layer: Fully-connected 256 rectifer units\n",
    "5. Output: Fully-connected linear layer with a single output action varying between 4 and 18 depending on the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Tutorial:https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26 (uses theano)\n",
    "def atari_model(n_actions):\n",
    "    ATARI_SHAPE = (4, 105, 80)\n",
    "    frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = keras.layers.Input((n_actions,), name='mask')\n",
    "\n",
    "    normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
    "    \n",
    "    conv_1 = keras.layers.convolutional.Convolution2D(16, 8, 8, subsample=(4, 4), activation='relu')(normalized)\n",
    "    conv_2 = keras.layers.convolutional.Convolution2D(32, 4, 4, subsample=(2, 2), activation='relu')(conv_1)\n",
    "    conv_flattened = keras.layers.core.Flatten()(conv_2)\n",
    "    hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    output = keras.layers.Dense(n_actions)(hidden)\n",
    "    filtered_output = keras.layers.merge([output, actions_input], mode='mul')\n",
    "\n",
    "    self.model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
    "    optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "    self.model.compile(optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: rewards are fixed to all positive rewards to 1, all negative rewards to -1, and zero rewards remained the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training there is no thing such as *validation accuracy* to compare with. In Deep RL it is often used the total reward as an evaluation metric, however given that this model takes averages the total score might be noisy. So, DeepMind used the Q-value itself as the evaluation metric (no idea how they realized this would work).\n",
    "\n",
    "![R](reward.jpg \"R\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![R](results.jpg \"R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ONLY raw pixels as an input this paper demonstrated an ability to master Atari Games using DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari Game Example\n",
    "Note, that this code does not work on Windows because OpenAI Gym does not support BreakoutDererministic-v4 in Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Keras Libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "# Keras-rl Libraries\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, Callback\n",
    "\n",
    "# Visualization Libraries\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "\n",
    "# Window Size\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "# Random Seed\n",
    "np.random.seed(123)\n",
    "env.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Atari Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part that DOES NOT work on Windows Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "nb_actions = env.action_space.n\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filename = 'dqn_{}_weights.h5f'.format(ENV_NAME)\n",
    "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(ENV_NAME)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100)]\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
    "\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "dqn.test(env, nb_episodes=1, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To visualize on Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Render(Callback):\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        plt.clf()\n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "#weights_filename = 'dqn_{}_weights.h5f'.format(ENV_NAME)\n",
    "weights_filename = 'dqn_{}_weights_1750000.h5f'.format(ENV_NAME) # @check point\n",
    "dqn.load_weights(weights_filename)\n",
    "\n",
    "callbacks = Render()\n",
    "plt.figure(figsize=(6,8))\n",
    "dqn.test(env, nb_episodes=1, visualize=False, callbacks=[callbacks])\n",
    "\n",
    "env.close()\n",
    "print('END')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = []\n",
    "\n",
    "class Render(Callback):\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        im = plt.imshow(env.render(mode='rgb_array'))\n",
    "        ims.append([im])\n",
    "\n",
    "weights_filename = 'dqn_{}_weights_1750000.h5f'.format(ENV_NAME)\n",
    "dqn.load_weights(weights_filename)\n",
    "\n",
    "callbacks = Render()\n",
    "fig = plt.figure(figsize=(4,5))\n",
    "plt.axis('off')\n",
    "dqn.test(env, nb_episodes=1, visualize=False, callbacks=[callbacks])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig=fig, artists=ims, interval=10)\n",
    "# ani.save(\"anim.gif\", writer = \"imagemagick\")  # imagemagick for Ubuntu\n",
    "plt.close()\n",
    "\n",
    "HTML(ani.to_jshtml())         # JavascriptHTML output\n",
    "#HTML(ani.to_html5_video())   # HTML5 Video output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References/Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://medium.com/free-code-camp/explained-simply-how-deepmind-taught-ai-to-play-video-games-9eb5f38c89ee\n",
    "2. https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
    "3. https://github.com/mirrornerror/Keras-RL/blob/master/dqn-atari-jupyter.ipynb (Keras RL)\n",
    "4. https://ai.stackexchange.com/questions/4660/how-to-combine-backpropagation-in-neural-nets-and-reinforcement-learning\n",
    "5. https://stats.stackexchange.com/questions/340651/how-does-backpropagation-work-in-the-case-of-reinforcement-learning-for-games\n",
    "6. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/\n",
    "7. http://incompleteideas.net/book/RLbook2018.pdf (introduction to RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
